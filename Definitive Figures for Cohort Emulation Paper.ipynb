{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b1e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import GPE_ensemble as GPE\n",
    "\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from GPErks.gp.data.dataset import Dataset\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "#from GPErks.gp.experiment import GPExperiment\n",
    "#from GPErks.train.emulator import GPEmulator\n",
    "#from GPErks.perks.inference import Inference\n",
    "#from GPErks.train.early_stop import NoEarlyStoppingCriterion\n",
    "#from GPErks.train.early_stop import (\n",
    "#    GLEarlyStoppingCriterion,\n",
    "#    PQEarlyStoppingCriterion,\n",
    "#    UPEarlyStoppingCriterion,\n",
    "#)\n",
    "#from GPErks.train.early_stop import PkEarlyStoppingCriterion\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set logger and enforce reproducibility\n",
    "#from GPErks.log.logger import get_logger\n",
    "#from GPErks.utils.random import set_seed\n",
    "#log = get_logger()\n",
    "seed = 7\n",
    "#set_seed(seed)\n",
    "from time import process_time \n",
    "import scipy\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf45825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "406.703\n",
      "02\n",
      "317.407\n",
      "03\n",
      "332.914\n",
      "04\n",
      "309.14\n",
      "05\n",
      "277.849\n",
      "06\n",
      "296.377\n",
      "07\n",
      "355.546\n",
      "08\n",
      "283.103\n",
      "09\n",
      "391.145\n",
      "10\n",
      "439.316\n",
      "11\n",
      "348.01\n",
      "12\n",
      "292.465\n",
      "13\n",
      "301.222\n",
      "14\n",
      "325.678\n",
      "15\n",
      "320.459\n",
      "16\n",
      "297.968\n",
      "17\n",
      "317.709\n",
      "18\n",
      "297.346\n",
      "19\n",
      "312.492\n"
     ]
    }
   ],
   "source": [
    "mode_weights = pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/modes_weights.csv',index_col=0,delim_whitespace=False,header=0)\n",
    "\n",
    "mode_weights\n",
    "\n",
    "#mode_weights=mode_weights.drop(15,axis=0)\n",
    "#mode_weights=mode_weights.drop(14,axis=0)\n",
    "\n",
    "meshes=['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19']\n",
    "\n",
    "x_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/input/xlabels_EP.txt',delim_whitespace=True,header=None)\n",
    "x_labels=x_labels.values.flatten().tolist()+mode_weights.columns.tolist()\n",
    "\n",
    "y_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/output/ylabels.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "\n",
    "\n",
    "all_input = []\n",
    "all_output=[]\n",
    "all_x=[]\n",
    "for i in range(len(meshes)):\n",
    "    val=meshes[i]\n",
    "    \n",
    "    inputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/\"+val+\"/X_EP.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    outputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/\"+val+\"/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    modeweights = np.tile(mode_weights.iloc[i,:].values, (inputData.shape[0],1))\n",
    "    input_modes = np.concatenate((inputData,modeweights),axis=1)\n",
    "    all_x.append(torch.tensor(inputData))\n",
    "    all_input.append(torch.tensor(input_modes))\n",
    "    all_output.append(torch.tensor(outputData))\n",
    "    print(val)\n",
    "    print(np.max(outputData))\n",
    "#all_input=pd.concat(all_input)\n",
    "#all_output=pd.concat(all_output\n",
    "#all_input.columns=x_labels\n",
    "#all_output.columns=y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1493530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=[]\n",
    "test_input = []\n",
    "train_output=[]\n",
    "test_output = []\n",
    "\n",
    "train_input_modes=[]\n",
    "test_input_modes = []\n",
    "train_output_modes=[]\n",
    "test_output_modes = []\n",
    "\n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=all_x[i]\n",
    "    y=all_output[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input.append(X_train)\n",
    "    test_input.append(X_test)\n",
    "    train_output.append(y_train)\n",
    "    test_output.append(y_test)\n",
    "    \n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=all_input[i]\n",
    "    y=all_output[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input_modes.append(X_train)\n",
    "    test_input_modes.append(X_test)\n",
    "    train_output_modes.append(y_train)\n",
    "    test_output_modes.append(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a8dda",
   "metadata": {},
   "source": [
    "# Emulator per mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff58820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "emulators=[]\n",
    "for i in range(len(meshes)):\n",
    "    emulators.append(GPE.ensemble(train_input[i],train_output[i],mean_func=\"linear\",training_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3056f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dda116d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:208: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3618.)\n",
      "  prediction=torch.stack(prediction).T\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    }
   ],
   "source": [
    "R2 = torch.zeros(len(meshes),2)\n",
    "R2_std = torch.zeros(len(meshes),2)\n",
    "for i in range(len(meshes)):\n",
    "    meanR, stdR = emulators[i].R2_sample(test_input[i],test_output[i],n=1000)\n",
    "    R2[i,:]=meanR\n",
    "    R2_std[i,:] = stdR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2beb20e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9983, 0.9977, 0.9984, 0.9986, 0.9992, 0.9982, 0.9988, 0.9975, 0.9982,\n",
       "        0.9949, 0.9978, 0.9970, 0.9987, 0.9984, 0.9968, 0.9966, 0.9980, 0.9975,\n",
       "        0.9987])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3aec3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0003, 0.0006],\n",
       "        [0.0002, 0.0011],\n",
       "        [0.0002, 0.0005],\n",
       "        [0.0002, 0.0004],\n",
       "        [0.0003, 0.0002],\n",
       "        [0.0002, 0.0005],\n",
       "        [0.0002, 0.0004],\n",
       "        [0.0004, 0.0008],\n",
       "        [0.0005, 0.0004],\n",
       "        [0.0002, 0.0017],\n",
       "        [0.0003, 0.0012],\n",
       "        [0.0007, 0.0008],\n",
       "        [0.0003, 0.0004],\n",
       "        [0.0001, 0.0008],\n",
       "        [0.0003, 0.0019],\n",
       "        [0.0007, 0.0015],\n",
       "        [0.0004, 0.0008],\n",
       "        [0.0010, 0.0004],\n",
       "        [0.0004, 0.0006]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3826e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "fontS=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b4da4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.scatter(meshes,R2[:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2[:,0].detach().numpy(),fmt='o',yerr=R2_std[:,0].detach().numpy())\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "ax.scatter(meshes,R2[:,1].detach().numpy())\n",
    "plt.errorbar(meshes,R2[:,1].detach().numpy(),fmt='o',yerr=R2_std[:,1].detach().numpy())\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTIndivEm.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d48880",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=[20,40,60,80,100,120,140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1cf2ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "R2 = torch.zeros(len(meshes),len(nn),2)\n",
    "R2_std = torch.zeros(len(meshes),len(nn),2)\n",
    "reps=5\n",
    "for i in range(len(meshes)):\n",
    "    for j in range(len(nn)):\n",
    "        for k in range(reps):\n",
    "            X=train_input[i]\n",
    "            y=train_output[i]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=nn[j],\n",
    "                random_state=k\n",
    "            )\n",
    "            emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "            meanR, stdR = emulator.R2_sample(test_input[i],test_output[i],n=1000)\n",
    "            R2[i,j,:]+=meanR/reps\n",
    "            R2_std[i,j,:] += stdR/reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697aa666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 1.01)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(nn,R2.mean(axis=0)[:,0].detach().numpy(),fmt='o',yerr=R2_std.mean(axis=0)[:,0].detach().numpy())\n",
    "plt.errorbar(nn,R2.mean(axis=0)[:,1].detach().numpy(),fmt='o',yerr=R2_std.mean(axis=0)[:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.ylim(0.7,1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e67376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 40, 60, 80, 100, 120, 140]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed729f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8927, 0.9081],\n",
       "        [0.9901, 0.9763],\n",
       "        [0.9954, 0.9880],\n",
       "        [0.9969, 0.9925],\n",
       "        [0.9979, 0.9945],\n",
       "        [0.9983, 0.9958],\n",
       "        [0.9986, 0.9968]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2.mean(axis=0)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c9821",
   "metadata": {},
   "source": [
    "# Emulator trained with 17/18 meshes and evaluated on the left out mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed54d894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 200., 300., 400., 500., 600., 700., 800.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(100,800,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40f6432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.9975, 0.9944],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.9975, 0.9944],\n",
      "         [0.9975, 0.9949]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.9975, 0.9944],\n",
      "         [0.9975, 0.9949]],\n",
      "\n",
      "        [[0.9977, 0.9945],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.9975, 0.9944],\n",
      "         [0.9975, 0.9949]],\n",
      "\n",
      "        [[0.9977, 0.9945],\n",
      "         [0.9976, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.9975, 0.9944],\n",
      "         [0.9975, 0.9949]],\n",
      "\n",
      "        [[0.9977, 0.9945],\n",
      "         [0.9976, 0.9953],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.9975, 0.9944],\n",
      "         [0.9975, 0.9949]],\n",
      "\n",
      "        [[0.9977, 0.9945],\n",
      "         [0.9976, 0.9953],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9714, 0.9586],\n",
      "         [0.9675, 0.9600],\n",
      "         [0.9713, 0.9530],\n",
      "         [0.9781, 0.9596],\n",
      "         [0.9731, 0.9383]],\n",
      "\n",
      "        [[0.9878, 0.9762],\n",
      "         [0.9886, 0.9771],\n",
      "         [0.9884, 0.9783],\n",
      "         [0.9893, 0.9788],\n",
      "         [0.9898, 0.9760]],\n",
      "\n",
      "        [[0.9922, 0.9857],\n",
      "         [0.9934, 0.9857],\n",
      "         [0.9936, 0.9853],\n",
      "         [0.9946, 0.9831],\n",
      "         [0.9933, 0.9864]],\n",
      "\n",
      "        [[0.9935, 0.9901],\n",
      "         [0.9951, 0.9884],\n",
      "         [0.9961, 0.9888],\n",
      "         [0.9962, 0.9906],\n",
      "         [0.9956, 0.9914]],\n",
      "\n",
      "        [[0.9959, 0.9921],\n",
      "         [0.9966, 0.9921],\n",
      "         [0.9968, 0.9925],\n",
      "         [0.9971, 0.9940],\n",
      "         [0.9970, 0.9927]],\n",
      "\n",
      "        [[0.9966, 0.9936],\n",
      "         [0.9969, 0.9927],\n",
      "         [0.9966, 0.9936],\n",
      "         [0.9972, 0.9952],\n",
      "         [0.9970, 0.9941]],\n",
      "\n",
      "        [[0.9971, 0.9943],\n",
      "         [0.9971, 0.9944],\n",
      "         [0.9974, 0.9946],\n",
      "         [0.9975, 0.9944],\n",
      "         [0.9975, 0.9949]],\n",
      "\n",
      "        [[0.9977, 0.9945],\n",
      "         [0.9976, 0.9953],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9980, 0.9959]]])\n"
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "train_p = np.linspace(100,800,8)\n",
    "R2_test = torch.zeros(len(train_p),reps,2)\n",
    "R2_leftout= torch.zeros(len(train_p),reps,2)\n",
    "for i in range(len(train_p)):\n",
    "    for j in range(reps):\n",
    "        X=torch.cat(train_input_modes[0:17])[:,0:15]\n",
    "        y=torch.cat(train_output_modes[0:17])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            train_size=int(train_p[i]),\n",
    "            random_state=j\n",
    "        )\n",
    "        emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "        meanR, stdR = emulator.R2_sample(torch.cat(test_input_modes[0:17])[:,0:15],torch.cat(test_output_modes[0:17]),1000)\n",
    "        \n",
    "        R2_test[i,j,:]+=meanR\n",
    "        \n",
    "        meanR, stdR=emulator.R2_sample(test_input_modes[17][:,0:15],test_output_modes[17],1000) \n",
    "        R2_leftout[i,j,:] = meanR\n",
    "        print(R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caa15b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor([0.9667, 0.9569])\n",
      "tensor([0.3452, 0.0843])\n",
      "0\n",
      "1\n",
      "tensor([0.9525, 0.9651])\n",
      "tensor([0.7088, 0.8330])\n",
      "0\n",
      "1\n",
      "tensor([0.9500, 0.9614])\n",
      "tensor([0.8902, 0.7622])\n",
      "0\n",
      "1\n",
      "tensor([0.9365, 0.9476])\n",
      "tensor([0.0708, 0.7870])\n",
      "0\n",
      "1\n",
      "tensor([0.9537, 0.9313])\n",
      "tensor([0.3825, 0.8512])\n",
      "0\n",
      "1\n",
      "tensor([0.9666, 0.9555])\n",
      "tensor([0.8779, 0.5128])\n",
      "0\n",
      "1\n",
      "tensor([0.9441, 0.9621])\n",
      "tensor([0.2754, 0.2718])\n",
      "0\n",
      "1\n",
      "tensor([0.9213, 0.9600])\n",
      "tensor([-0.7956,  0.9429])\n",
      "0\n",
      "1\n",
      "tensor([0.9138, 0.9543])\n",
      "tensor([-1.4595,  0.7099])\n",
      "0\n",
      "1\n",
      "tensor([0.9582, 0.9274])\n",
      "tensor([0.5649, 0.9060])\n",
      "0\n",
      "1\n",
      "tensor([0.9562, 0.9559])\n",
      "tensor([0.5710, 0.5901])\n",
      "0\n",
      "1\n",
      "tensor([0.9271, 0.9682])\n",
      "tensor([0.7121, 0.6120])\n",
      "0\n",
      "1\n",
      "tensor([0.9589, 0.9566])\n",
      "tensor([0.7401, 0.7675])\n",
      "0\n",
      "1\n",
      "tensor([0.9421, 0.9568])\n",
      "tensor([0.6785, 0.7703])\n",
      "0\n",
      "1\n",
      "tensor([0.9579, 0.9471])\n",
      "tensor([0.9120, 0.6272])\n",
      "0\n",
      "1\n",
      "tensor([0.9634, 0.9562])\n",
      "tensor([-0.4408, -0.0565])\n",
      "0\n",
      "1\n",
      "tensor([0.9384, 0.9642])\n",
      "tensor([-1.3677, -0.1270])\n",
      "0\n",
      "1\n",
      "tensor([0.9540, 0.9500])\n",
      "tensor([ 0.7644, -0.0345])\n",
      "0\n",
      "1\n",
      "tensor([0.9437, 0.9505])\n",
      "tensor([-2.2229, -0.0849])\n",
      "0\n",
      "1\n",
      "tensor([0.9630, 0.9206])\n",
      "tensor([-6.6574,  0.7203])\n",
      "0\n",
      "1\n",
      "tensor([0.9692, 0.9569])\n",
      "tensor([0.4828, 0.8232])\n",
      "0\n",
      "1\n",
      "tensor([0.9425, 0.9586])\n",
      "tensor([0.5094, 0.8602])\n",
      "0\n",
      "1\n",
      "tensor([0.9551, 0.9614])\n",
      "tensor([0.9316, 0.5792])\n",
      "0\n",
      "1\n",
      "tensor([0.9306, 0.9461])\n",
      "tensor([0.8965, 0.9042])\n",
      "0\n",
      "1\n",
      "tensor([0.9581, 0.9231])\n",
      "tensor([0.8983, 0.8586])\n",
      "0\n",
      "1\n",
      "tensor([0.9691, 0.9537])\n",
      "tensor([0.9454, 0.9577])\n",
      "0\n",
      "1\n",
      "tensor([0.9358, 0.9642])\n",
      "tensor([0.9398, 0.9337])\n",
      "0\n",
      "1\n",
      "tensor([0.9510, 0.9422])\n",
      "tensor([0.9627, 0.8585])\n",
      "0\n",
      "1\n",
      "tensor([0.9369, 0.9251])\n",
      "tensor([0.9514, 0.9429])\n",
      "0\n",
      "1\n",
      "tensor([0.9570, 0.9424])\n",
      "tensor([0.9264, 0.8864])\n",
      "0\n",
      "1\n",
      "tensor([0.9527, 0.9602])\n",
      "tensor([0.9395, 0.9093])\n",
      "0\n",
      "1\n",
      "tensor([0.9346, 0.9626])\n",
      "tensor([0.9628, 0.7942])\n",
      "0\n",
      "1\n",
      "tensor([0.9540, 0.9460])\n",
      "tensor([0.8561, 0.8815])\n",
      "0\n",
      "1\n",
      "tensor([0.9310, 0.9283])\n",
      "tensor([0.9637, 0.9473])\n",
      "0\n",
      "1\n",
      "tensor([0.9589, 0.9289])\n",
      "tensor([0.9175, 0.9172])\n",
      "0\n",
      "1\n",
      "tensor([0.9576, 0.9581])\n",
      "tensor([0.9095, 0.7039])\n",
      "0\n",
      "1\n",
      "tensor([0.9421, 0.9567])\n",
      "tensor([0.6353, 0.7637])\n",
      "0\n",
      "1\n",
      "tensor([0.9452, 0.9537])\n",
      "tensor([0.8517, 0.7323])\n",
      "0\n",
      "1\n",
      "tensor([0.9201, 0.9427])\n",
      "tensor([0.6468, 0.7794])\n",
      "0\n",
      "1\n",
      "tensor([0.9533, 0.9346])\n",
      "tensor([0.8738, 0.8093])\n",
      "0\n",
      "1\n",
      "tensor([0.9536, 0.9615])\n",
      "tensor([0.8309, 0.9185])\n",
      "0\n",
      "1\n",
      "tensor([0.9448, 0.9547])\n",
      "tensor([0.9472, 0.7470])\n",
      "0\n",
      "1\n",
      "tensor([0.9417, 0.9373])\n",
      "tensor([0.8308, 0.6609])\n",
      "0\n",
      "1\n",
      "tensor([0.9369, 0.9494])\n",
      "tensor([0.7802, 0.4985])\n",
      "0\n",
      "1\n",
      "tensor([0.9606, 0.9456])\n",
      "tensor([0.8591, 0.8140])\n",
      "0\n",
      "1\n",
      "tensor([0.9466, 0.9564])\n",
      "tensor([0.8247, 0.6650])\n",
      "0\n",
      "1\n",
      "tensor([0.9410, 0.9518])\n",
      "tensor([0.4210, 0.6907])\n",
      "0\n",
      "1\n",
      "tensor([0.9543, 0.9573])\n",
      "tensor([0.2756, 0.7746])\n",
      "0\n",
      "1\n",
      "tensor([0.9399, 0.9534])\n",
      "tensor([0.6336, 0.9681])\n",
      "0\n",
      "1\n",
      "tensor([0.9680, 0.9379])\n",
      "tensor([0.4409, 0.9263])\n",
      "0\n",
      "1\n",
      "tensor([0.9524, 0.9543])\n",
      "tensor([0.8499, 0.9382])\n",
      "0\n",
      "1\n",
      "tensor([0.9475, 0.9606])\n",
      "tensor([0.8942, 0.9144])\n",
      "0\n",
      "1\n",
      "tensor([0.9478, 0.9284])\n",
      "tensor([0.7302, 0.8607])\n",
      "0\n",
      "1\n",
      "tensor([0.9373, 0.9591])\n",
      "tensor([0.8940, 0.8509])\n",
      "0\n",
      "1\n",
      "tensor([0.9663, 0.9354])\n",
      "tensor([0.3055, 0.8232])\n",
      "0\n",
      "1\n",
      "tensor([0.9350, 0.9550])\n",
      "tensor([0.8261, 0.9033])\n",
      "0\n",
      "1\n",
      "tensor([0.9462, 0.9494])\n",
      "tensor([0.9720, 0.9250])\n",
      "0\n",
      "1\n",
      "tensor([0.9510, 0.9375])\n",
      "tensor([0.9435, 0.9558])\n",
      "0\n",
      "1\n",
      "tensor([0.9358, 0.9517])\n",
      "tensor([0.9124, 0.7609])\n",
      "0\n",
      "1\n",
      "tensor([0.9670, 0.9390])\n",
      "tensor([0.9121, 0.9212])\n",
      "0\n",
      "1\n",
      "tensor([0.9570, 0.9620])\n",
      "tensor([0.6079, 0.9124])\n",
      "0\n",
      "1\n",
      "tensor([0.9550, 0.9474])\n",
      "tensor([0.7503, 0.8792])\n",
      "0\n",
      "1\n",
      "tensor([0.9504, 0.9451])\n",
      "tensor([ 0.8678, -0.1267])\n",
      "0\n",
      "1\n",
      "tensor([0.9362, 0.9665])\n",
      "tensor([0.8286, 0.7117])\n",
      "0\n",
      "1\n",
      "tensor([0.9643, 0.9397])\n",
      "tensor([0.8411, 0.7612])\n",
      "0\n",
      "1\n",
      "tensor([0.9503, 0.9614])\n",
      "tensor([0.7315, 0.9288])\n",
      "0\n",
      "1\n",
      "tensor([0.9616, 0.9600])\n",
      "tensor([0.3524, 0.8418])\n",
      "0\n",
      "1\n",
      "tensor([0.9502, 0.9424])\n",
      "tensor([0.7291, 0.7501])\n",
      "0\n",
      "1\n",
      "tensor([0.9663, 0.9554])\n",
      "tensor([0.8978, 0.7644])\n",
      "0\n",
      "1\n",
      "tensor([0.9640, 0.9427])\n",
      "tensor([0.3953, 0.7380])\n",
      "0\n",
      "1\n",
      "tensor([0.9708, 0.9629])\n",
      "tensor([0.6000, 0.7716])\n",
      "0\n",
      "1\n",
      "tensor([0.9642, 0.9577])\n",
      "tensor([0.6390, 0.7795])\n",
      "0\n",
      "1\n",
      "tensor([0.9723, 0.9457])\n",
      "tensor([0.6672, 0.9748])\n",
      "0\n",
      "1\n",
      "tensor([0.9652, 0.9690])\n",
      "tensor([0.5620, 0.7253])\n",
      "0\n",
      "1\n",
      "tensor([0.9707, 0.9481])\n",
      "tensor([0.6371, 0.9199])\n",
      "0\n",
      "1\n",
      "tensor([0.9031, 0.9658])\n",
      "tensor([0.7296, 0.8436])\n",
      "0\n",
      "1\n",
      "tensor([0.9525, 0.9601])\n",
      "tensor([0.9549, 0.7378])\n",
      "0\n",
      "1\n",
      "tensor([0.9362, 0.9469])\n",
      "tensor([0.8467, 0.7095])\n",
      "0\n",
      "1\n",
      "tensor([0.9481, 0.9543])\n",
      "tensor([0.9337, 0.8831])\n",
      "0\n",
      "1\n",
      "tensor([0.9535, 0.9542])\n",
      "tensor([0.6173, 0.8131])\n",
      "0\n",
      "1\n",
      "tensor([0.9694, 0.9593])\n",
      "tensor([0.1813, 0.6293])\n",
      "0\n",
      "1\n",
      "tensor([0.9564, 0.9586])\n",
      "tensor([0.6148, 0.8499])\n",
      "0\n",
      "1\n",
      "tensor([0.9495, 0.9478])\n",
      "tensor([-0.9943,  0.8801])\n",
      "0\n",
      "1\n",
      "tensor([0.9571, 0.9603])\n",
      "tensor([-0.2759,  0.8859])\n",
      "0\n",
      "1\n",
      "tensor([0.9561, 0.9409])\n",
      "tensor([0.8258, 0.8517])\n",
      "0\n",
      "1\n",
      "tensor([0.9637, 0.9579])\n",
      "tensor([0.2143, 0.8433])\n",
      "0\n",
      "1\n",
      "tensor([0.9483, 0.9601])\n",
      "tensor([0.9288, 0.9236])\n",
      "0\n",
      "1\n",
      "tensor([0.9338, 0.9484])\n",
      "tensor([0.4799, 0.8523])\n",
      "0\n",
      "1\n",
      "tensor([0.9608, 0.9622])\n",
      "tensor([0.1259, 0.9272])\n",
      "0\n",
      "1\n",
      "tensor([0.9529, 0.9396])\n",
      "tensor([0.7147, 0.9478])\n",
      "0\n",
      "1\n",
      "tensor([0.9678, 0.9643])\n",
      "tensor([0.2416, 0.9377])\n",
      "0\n",
      "1\n",
      "tensor([0.9644, 0.9643])\n",
      "tensor([0.7742, 0.9580])\n",
      "0\n",
      "1\n",
      "tensor([0.9560, 0.9469])\n",
      "tensor([0.1575, 0.9021])\n",
      "0\n",
      "1\n",
      "tensor([0.9727, 0.9567])\n",
      "tensor([-0.1044,  0.8600])\n",
      "0\n",
      "1\n",
      "tensor([0.9655, 0.9276])\n",
      "tensor([0.4563, 0.9402])\n",
      "0\n",
      "1\n",
      "tensor([0.9864, 0.9756])\n",
      "tensor([0.4509, 0.7717])\n",
      "0\n",
      "1\n",
      "tensor([0.9828, 0.9770])\n",
      "tensor([0.7562, 0.7147])\n",
      "0\n",
      "1\n",
      "tensor([0.9832, 0.9790])\n",
      "tensor([0.6371, 0.8179])\n",
      "0\n",
      "1\n",
      "tensor([0.9845, 0.9815])\n",
      "tensor([0.6321, 0.8419])\n",
      "0\n",
      "1\n",
      "tensor([0.9871, 0.9657])\n",
      "tensor([0.3889, 0.8480])\n",
      "0\n",
      "1\n",
      "tensor([0.9863, 0.9758])\n",
      "tensor([-1.3077,  0.7939])\n",
      "0\n",
      "1\n",
      "tensor([0.9844, 0.9822])\n",
      "tensor([-0.3477,  0.8749])\n",
      "0\n",
      "1\n",
      "tensor([0.9845, 0.9771])\n",
      "tensor([-0.1052,  0.7969])\n",
      "0\n",
      "1\n",
      "tensor([0.9855, 0.9832])\n",
      "tensor([-0.4833,  0.8244])\n",
      "0\n",
      "1\n",
      "tensor([0.9868, 0.9669])\n",
      "tensor([-1.9906,  0.6629])\n",
      "0\n",
      "1\n",
      "tensor([0.9874, 0.9783])\n",
      "tensor([0.5990, 0.7651])\n",
      "0\n",
      "1\n",
      "tensor([0.9832, 0.9824])\n",
      "tensor([0.7973, 0.6650])\n",
      "0\n",
      "1\n",
      "tensor([0.9829, 0.9806])\n",
      "tensor([0.5634, 0.7426])\n",
      "0\n",
      "1\n",
      "tensor([0.9848, 0.9814])\n",
      "tensor([0.5609, 0.7471])\n",
      "0\n",
      "1\n",
      "tensor([0.9882, 0.9749])\n",
      "tensor([0.6998, 0.6857])\n",
      "0\n",
      "1\n",
      "tensor([0.9888, 0.9762])\n",
      "tensor([-2.7590,  0.3190])\n",
      "0\n",
      "1\n",
      "tensor([0.9857, 0.9827])\n",
      "tensor([-2.5694,  0.7576])\n",
      "0\n",
      "1\n",
      "tensor([0.9843, 0.9801])\n",
      "tensor([-1.1160,  0.3001])\n",
      "0\n",
      "1\n",
      "tensor([0.9859, 0.9759])\n",
      "tensor([-0.4006, -0.6908])\n",
      "0\n",
      "1\n",
      "tensor([0.9887, 0.9738])\n",
      "tensor([-2.8354,  0.6446])\n",
      "0\n",
      "1\n",
      "tensor([0.9875, 0.9759])\n",
      "tensor([0.5959, 0.9302])\n",
      "0\n",
      "1\n",
      "tensor([0.9844, 0.9822])\n",
      "tensor([0.5987, 0.9593])\n",
      "0\n",
      "1\n",
      "tensor([0.9860, 0.9797])\n",
      "tensor([0.4188, 0.9634])\n",
      "0\n",
      "1\n",
      "tensor([0.9862, 0.9803])\n",
      "tensor([0.5185, 0.9548])\n",
      "0\n",
      "1\n",
      "tensor([0.9879, 0.9717])\n",
      "tensor([0.5787, 0.8687])\n",
      "0\n",
      "1\n",
      "tensor([0.9883, 0.9761])\n",
      "tensor([0.8386, 0.9425])\n",
      "0\n",
      "1\n",
      "tensor([0.9853, 0.9818])\n",
      "tensor([0.8962, 0.9001])\n",
      "0\n",
      "1\n",
      "tensor([0.9840, 0.9791])\n",
      "tensor([0.8412, 0.9692])\n",
      "0\n",
      "1\n",
      "tensor([0.9827, 0.9782])\n",
      "tensor([0.9410, 0.9602])\n",
      "0\n",
      "1\n",
      "tensor([0.9883, 0.9716])\n",
      "tensor([0.8430, 0.9489])\n",
      "0\n",
      "1\n",
      "tensor([0.9879, 0.9755])\n",
      "tensor([0.9705, 0.9606])\n",
      "0\n",
      "1\n",
      "tensor([0.9838, 0.9802])\n",
      "tensor([0.9767, 0.9181])\n",
      "0\n",
      "1\n",
      "tensor([0.9860, 0.9803])\n",
      "tensor([0.9519, 0.9667])\n",
      "0\n",
      "1\n",
      "tensor([0.9868, 0.9810])\n",
      "tensor([0.9494, 0.8565])\n",
      "0\n",
      "1\n",
      "tensor([0.9864, 0.9699])\n",
      "tensor([0.9760, 0.9293])\n",
      "0\n",
      "1\n",
      "tensor([0.9826, 0.9765])\n",
      "tensor([0.8867, 0.7944])\n",
      "0\n",
      "1\n",
      "tensor([0.9842, 0.9791])\n",
      "tensor([0.8245, 0.7655])\n",
      "0\n",
      "1\n",
      "tensor([0.9839, 0.9783])\n",
      "tensor([0.8468, 0.7317])\n",
      "0\n",
      "1\n",
      "tensor([0.9853, 0.9826])\n",
      "tensor([0.9191, 0.7187])\n",
      "0\n",
      "1\n",
      "tensor([0.9866, 0.9720])\n",
      "tensor([0.9165, 0.8120])\n",
      "0\n",
      "1\n",
      "tensor([0.9845, 0.9766])\n",
      "tensor([0.8774, 0.9389])\n",
      "0\n",
      "1\n",
      "tensor([0.9850, 0.9791])\n",
      "tensor([0.8880, 0.6713])\n",
      "0\n",
      "1\n",
      "tensor([0.9815, 0.9777])\n",
      "tensor([0.7402, 0.8380])\n",
      "0\n",
      "1\n",
      "tensor([0.9858, 0.9779])\n",
      "tensor([0.5542, 0.6328])\n",
      "0\n",
      "1\n",
      "tensor([0.9854, 0.9716])\n",
      "tensor([0.8859, 0.8678])\n",
      "0\n",
      "1\n",
      "tensor([0.9867, 0.9797])\n",
      "tensor([0.4911, 0.7706])\n",
      "0\n",
      "1\n",
      "tensor([0.9849, 0.9807])\n",
      "tensor([0.1386, 0.8052])\n",
      "0\n",
      "1\n",
      "tensor([0.9843, 0.9774])\n",
      "tensor([0.0427, 0.7892])\n",
      "0\n",
      "1\n",
      "tensor([0.9887, 0.9824])\n",
      "tensor([0.3387, 0.7863])\n",
      "0\n",
      "1\n",
      "tensor([0.9876, 0.9757])\n",
      "tensor([0.2610, 0.6898])\n",
      "0\n",
      "1\n",
      "tensor([0.9845, 0.9776])\n",
      "tensor([0.6498, 0.9312])\n",
      "0\n",
      "1\n",
      "tensor([0.9851, 0.9839])\n",
      "tensor([0.8509, 0.7294])\n",
      "0\n",
      "1\n",
      "tensor([0.9840, 0.9796])\n",
      "tensor([0.8718, 0.9183])\n",
      "0\n",
      "1\n",
      "tensor([0.9877, 0.9782])\n",
      "tensor([0.9471, 0.8518])\n",
      "0\n",
      "1\n",
      "tensor([0.9879, 0.9740])\n",
      "tensor([0.8357, 0.8499])\n",
      "0\n",
      "1\n",
      "tensor([0.9859, 0.9741])\n",
      "tensor([0.9804, 0.9828])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9856, 0.9846])\n",
      "tensor([0.9675, 0.9668])\n",
      "0\n",
      "1\n",
      "tensor([0.9842, 0.9781])\n",
      "tensor([0.7179, 0.9755])\n",
      "0\n",
      "1\n",
      "tensor([0.9881, 0.9813])\n",
      "tensor([0.9513, 0.9516])\n",
      "0\n",
      "1\n",
      "tensor([0.9875, 0.9740])\n",
      "tensor([0.9742, 0.9663])\n",
      "0\n",
      "1\n",
      "tensor([0.9845, 0.9779])\n",
      "tensor([0.8569, 0.7266])\n",
      "0\n",
      "1\n",
      "tensor([0.9854, 0.9830])\n",
      "tensor([0.8099, 0.8642])\n",
      "0\n",
      "1\n",
      "tensor([0.9852, 0.9753])\n",
      "tensor([0.7630, 0.7578])\n",
      "0\n",
      "1\n",
      "tensor([0.9894, 0.9811])\n",
      "tensor([0.6989, 0.1554])\n",
      "0\n",
      "1\n",
      "tensor([0.9877, 0.9714])\n",
      "tensor([0.7681, 0.6869])\n",
      "0\n",
      "1\n",
      "tensor([0.9847, 0.9769])\n",
      "tensor([0.8214, 0.8590])\n",
      "0\n",
      "1\n",
      "tensor([0.9854, 0.9783])\n",
      "tensor([0.8595, 0.6269])\n",
      "0\n",
      "1\n",
      "tensor([0.9838, 0.9783])\n",
      "tensor([0.7747, 0.6052])\n",
      "0\n",
      "1\n",
      "tensor([0.9872, 0.9823])\n",
      "tensor([0.5721, 0.7206])\n",
      "0\n",
      "1\n",
      "tensor([0.9874, 0.9758])\n",
      "tensor([0.8474, 0.7598])\n",
      "0\n",
      "1\n",
      "tensor([0.9894, 0.9801])\n",
      "tensor([0.6738, 0.7980])\n",
      "0\n",
      "1\n",
      "tensor([0.9877, 0.9810])\n",
      "tensor([0.6483, 0.6932])\n",
      "0\n",
      "1\n",
      "tensor([0.9850, 0.9805])\n",
      "tensor([0.5352, 0.7139])\n",
      "0\n",
      "1\n",
      "tensor([0.9906, 0.9823])\n",
      "tensor([0.6305, 0.6550])\n",
      "0\n",
      "1\n",
      "tensor([0.9909, 0.9796])\n",
      "tensor([0.7157, 0.7481])\n",
      "0\n",
      "1\n",
      "tensor([0.9856, 0.9816])\n",
      "tensor([0.9216, 0.8924])\n",
      "0\n",
      "1\n",
      "tensor([0.9853, 0.9822])\n",
      "tensor([0.9795, 0.7319])\n",
      "0\n",
      "1\n",
      "tensor([0.9840, 0.9804])\n",
      "tensor([0.9658, 0.7657])\n",
      "0\n",
      "1\n",
      "tensor([0.9859, 0.9832])\n",
      "tensor([0.9527, 0.8900])\n",
      "0\n",
      "1\n",
      "tensor([0.9859, 0.9755])\n",
      "tensor([0.8758, 0.8314])\n",
      "0\n",
      "1\n",
      "tensor([0.9840, 0.9788])\n",
      "tensor([0.7376, 0.8688])\n",
      "0\n",
      "1\n",
      "tensor([0.9855, 0.9829])\n",
      "tensor([0.2589, 0.6621])\n",
      "0\n",
      "1\n",
      "tensor([0.9868, 0.9798])\n",
      "tensor([0.0752, 0.7099])\n",
      "0\n",
      "1\n",
      "tensor([0.9882, 0.9838])\n",
      "tensor([-1.6181,  0.7204])\n",
      "0\n",
      "1\n",
      "tensor([0.9852, 0.9765])\n",
      "tensor([0.1143, 0.9032])\n",
      "0\n",
      "1\n",
      "tensor([0.9854, 0.9779])\n",
      "tensor([0.1966, 0.9431])\n",
      "0\n",
      "1\n",
      "tensor([0.9853, 0.9771])\n",
      "tensor([0.8650, 0.9233])\n",
      "0\n",
      "1\n",
      "tensor([0.9827, 0.9773])\n",
      "tensor([0.4459, 0.9270])\n",
      "0\n",
      "1\n",
      "tensor([0.9887, 0.9813])\n",
      "tensor([0.3864, 0.9549])\n",
      "0\n",
      "1\n",
      "tensor([0.9844, 0.9753])\n",
      "tensor([0.8343, 0.9071])\n",
      "0\n",
      "1\n",
      "tensor([0.9834, 0.9788])\n",
      "tensor([0.3439, 0.8647])\n",
      "0\n",
      "1\n",
      "tensor([0.9858, 0.9800])\n",
      "tensor([0.4396, 0.9105])\n",
      "0\n",
      "1\n",
      "tensor([0.9867, 0.9800])\n",
      "tensor([0.0605, 0.9134])\n",
      "0\n",
      "1\n",
      "tensor([0.9886, 0.9821])\n",
      "tensor([0.5029, 0.9042])\n",
      "0\n",
      "1\n",
      "tensor([0.9880, 0.9754])\n",
      "tensor([0.5268, 0.9521])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9819])\n",
      "tensor([0.4871, 0.5291])\n",
      "0\n",
      "1\n",
      "tensor([0.9905, 0.9871])\n",
      "tensor([0.8909, 0.5451])\n",
      "0\n",
      "1\n",
      "tensor([0.9913, 0.9878])\n",
      "tensor([0.6637, 0.8323])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9884])\n",
      "tensor([0.7754, 0.8665])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9841])\n",
      "tensor([0.6015, 0.8596])\n",
      "0\n",
      "1\n",
      "tensor([0.9934, 0.9821])\n",
      "tensor([-2.4489,  0.9130])\n",
      "0\n",
      "1\n",
      "tensor([0.9918, 0.9871])\n",
      "tensor([-1.5093,  0.7322])\n",
      "0\n",
      "1\n",
      "tensor([0.9913, 0.9886])\n",
      "tensor([-1.4452,  0.9204])\n",
      "0\n",
      "1\n",
      "tensor([0.9927, 0.9881])\n",
      "tensor([-0.7989,  0.8147])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9840])\n",
      "tensor([-1.2353,  0.6871])\n",
      "0\n",
      "1\n",
      "tensor([0.9923, 0.9854])\n",
      "tensor([0.6320, 0.6779])\n",
      "0\n",
      "1\n",
      "tensor([0.9898, 0.9894])\n",
      "tensor([0.7386, 0.7177])\n",
      "0\n",
      "1\n",
      "tensor([0.9903, 0.9888])\n",
      "tensor([0.5926, 0.7354])\n",
      "0\n",
      "1\n",
      "tensor([0.9917, 0.9876])\n",
      "tensor([0.8587, 0.7710])\n",
      "0\n",
      "1\n",
      "tensor([0.9938, 0.9867])\n",
      "tensor([0.8446, 0.7685])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9830])\n",
      "tensor([-4.1171,  0.3324])\n",
      "0\n",
      "1\n",
      "tensor([0.9909, 0.9854])\n",
      "tensor([-2.9240,  0.7590])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9881])\n",
      "tensor([-3.1470,  0.3873])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9861])\n",
      "tensor([-2.7418, -2.4046])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9847])\n",
      "tensor([-3.3110,  0.7361])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9846])\n",
      "tensor([0.4030, 0.9359])\n",
      "0\n",
      "1\n",
      "tensor([0.9904, 0.9889])\n",
      "tensor([0.5372, 0.9569])\n",
      "0\n",
      "1\n",
      "tensor([0.9926, 0.9885])\n",
      "tensor([0.5313, 0.9229])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9872])\n",
      "tensor([0.5007, 0.9601])\n",
      "0\n",
      "1\n",
      "tensor([0.9932, 0.9869])\n",
      "tensor([0.5456, 0.9532])\n",
      "0\n",
      "1\n",
      "tensor([0.9931, 0.9843])\n",
      "tensor([0.8091, 0.9607])\n",
      "0\n",
      "1\n",
      "tensor([0.9907, 0.9887])\n",
      "tensor([0.8778, 0.9337])\n",
      "0\n",
      "1\n",
      "tensor([0.9922, 0.9877])\n",
      "tensor([0.8273, 0.9713])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9861])\n",
      "tensor([0.8138, 0.9707])\n",
      "0\n",
      "1\n",
      "tensor([0.9927, 0.9854])\n",
      "tensor([0.8506, 0.9725])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9842])\n",
      "tensor([0.9636, 0.9504])\n",
      "0\n",
      "1\n",
      "tensor([0.9910, 0.9886])\n",
      "tensor([0.9842, 0.8942])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9877])\n",
      "tensor([0.9493, 0.8979])\n",
      "0\n",
      "1\n",
      "tensor([0.9908, 0.9866])\n",
      "tensor([0.8790, 0.9226])\n",
      "0\n",
      "1\n",
      "tensor([0.9939, 0.9849])\n",
      "tensor([0.9831, 0.9711])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9836])\n",
      "tensor([0.9037, 0.8000])\n",
      "0\n",
      "1\n",
      "tensor([0.9917, 0.9873])\n",
      "tensor([0.9045, 0.7683])\n",
      "0\n",
      "1\n",
      "tensor([0.9914, 0.9875])\n",
      "tensor([0.8931, 0.8467])\n",
      "0\n",
      "1\n",
      "tensor([0.9921, 0.9887])\n",
      "tensor([0.8671, 0.6780])\n",
      "0\n",
      "1\n",
      "tensor([0.9917, 0.9854])\n",
      "tensor([0.9251, 0.7876])\n",
      "0\n",
      "1\n",
      "tensor([0.9915, 0.9853])\n",
      "tensor([0.3527, 0.8476])\n",
      "0\n",
      "1\n",
      "tensor([0.9915, 0.9881])\n",
      "tensor([0.9360, 0.7584])\n",
      "0\n",
      "1\n",
      "tensor([0.9926, 0.9878])\n",
      "tensor([0.8603, 0.6460])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9857])\n",
      "tensor([0.9388, 0.7128])\n",
      "0\n",
      "1\n",
      "tensor([0.9936, 0.9870])\n",
      "tensor([0.9235, 0.6964])\n",
      "0\n",
      "1\n",
      "tensor([0.9918, 0.9843])\n",
      "tensor([0.8795, 0.7638])\n",
      "0\n",
      "1\n",
      "tensor([0.9898, 0.9878])\n",
      "tensor([0.2865, 0.7404])\n",
      "0\n",
      "1\n",
      "tensor([0.9925, 0.9885])\n",
      "tensor([0.0753, 0.8827])\n",
      "0\n",
      "1\n",
      "tensor([0.9936, 0.9880])\n",
      "tensor([0.2798, 0.8168])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9871])\n",
      "tensor([0.9336, 0.7328])\n",
      "0\n",
      "1\n",
      "tensor([0.9915, 0.9850])\n",
      "tensor([0.8687, 0.9496])\n",
      "0\n",
      "1\n",
      "tensor([0.9923, 0.9891])\n",
      "tensor([0.9047, 0.8766])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9884])\n",
      "tensor([0.8413, 0.9371])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9867])\n",
      "tensor([ 0.7871, -0.1209])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9876])\n",
      "tensor([0.8843, 0.8646])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9850])\n",
      "tensor([0.9410, 0.9787])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9883])\n",
      "tensor([0.9824, 0.9721])\n",
      "0\n",
      "1\n",
      "tensor([0.9932, 0.9888])\n",
      "tensor([0.9836, 0.9763])\n",
      "0\n",
      "1\n",
      "tensor([0.9918, 0.9874])\n",
      "tensor([0.9633, 0.9491])\n",
      "0\n",
      "1\n",
      "tensor([0.9936, 0.9863])\n",
      "tensor([0.9817, 0.9634])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9851])\n",
      "tensor([0.9249, 0.2998])\n",
      "0\n",
      "1\n",
      "tensor([0.9927, 0.9887])\n",
      "tensor([0.7655, 0.7726])\n",
      "0\n",
      "1\n",
      "tensor([0.9928, 0.9884])\n",
      "tensor([0.8110, 0.5485])\n",
      "0\n",
      "1\n",
      "tensor([0.9940, 0.9896])\n",
      "tensor([0.7320, 0.4188])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9849])\n",
      "tensor([0.8088, 0.6081])\n",
      "0\n",
      "1\n",
      "tensor([0.9918, 0.9845])\n",
      "tensor([0.8229, 0.8880])\n",
      "0\n",
      "1\n",
      "tensor([0.9909, 0.9881])\n",
      "tensor([0.8199, 0.8491])\n",
      "0\n",
      "1\n",
      "tensor([0.9922, 0.9885])\n",
      "tensor([0.7644, 0.6858])\n",
      "0\n",
      "1\n",
      "tensor([0.9922, 0.9871])\n",
      "tensor([0.4430, 0.7692])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9867])\n",
      "tensor([0.8680, 0.8103])\n",
      "0\n",
      "1\n",
      "tensor([0.9931, 0.9861])\n",
      "tensor([0.6957, 0.7406])\n",
      "0\n",
      "1\n",
      "tensor([0.9921, 0.9872])\n",
      "tensor([0.7177, 0.7793])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9878])\n",
      "tensor([0.5997, 0.7302])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9894])\n",
      "tensor([0.5893, 0.7443])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9875])\n",
      "tensor([0.7477, 0.7493])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9881])\n",
      "tensor([0.9704, 0.9213])\n",
      "0\n",
      "1\n",
      "tensor([0.9912, 0.9899])\n",
      "tensor([0.9739, 0.7985])\n",
      "0\n",
      "1\n",
      "tensor([0.9937, 0.9879])\n",
      "tensor([0.9704, 0.7921])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9884])\n",
      "tensor([0.9714, 0.8714])\n",
      "0\n",
      "1\n",
      "tensor([0.9932, 0.9877])\n",
      "tensor([0.9707, 0.7821])\n",
      "0\n",
      "1\n",
      "tensor([0.9922, 0.9867])\n",
      "tensor([0.3384, 0.7879])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9865])\n",
      "tensor([-1.8386,  0.7356])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9875])\n",
      "tensor([0.1635, 0.7504])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9869])\n",
      "tensor([-1.9718,  0.9069])\n",
      "0\n",
      "1\n",
      "tensor([0.9937, 0.9872])\n",
      "tensor([0.1727, 0.5289])\n",
      "0\n",
      "1\n",
      "tensor([0.9915, 0.9868])\n",
      "tensor([0.2452, 0.9374])\n",
      "0\n",
      "1\n",
      "tensor([0.9915, 0.9860])\n",
      "tensor([0.6187, 0.9295])\n",
      "0\n",
      "1\n",
      "tensor([0.9927, 0.9863])\n",
      "tensor([0.6252, 0.9378])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9859])\n",
      "tensor([0.6313, 0.9474])\n",
      "0\n",
      "1\n",
      "tensor([0.9925, 0.9865])\n",
      "tensor([0.9132, 0.8710])\n",
      "0\n",
      "1\n",
      "tensor([0.9926, 0.9862])\n",
      "tensor([0.3290, 0.8543])\n",
      "0\n",
      "1\n",
      "tensor([0.9914, 0.9829])\n",
      "tensor([0.4913, 0.8834])\n",
      "0\n",
      "1\n",
      "tensor([0.9934, 0.9863])\n",
      "tensor([0.4746, 0.9337])\n",
      "0\n",
      "1\n",
      "tensor([0.9939, 0.9864])\n",
      "tensor([0.5595, 0.8364])\n",
      "0\n",
      "1\n",
      "tensor([0.9937, 0.9870])\n",
      "tensor([0.4122, 0.8978])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9908])\n",
      "tensor([0.5423, 0.3431])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9906])\n",
      "tensor([0.7919, 0.3363])\n",
      "0\n",
      "1\n",
      "tensor([0.9938, 0.9917])\n",
      "tensor([0.7207, 0.8002])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9916])\n",
      "tensor([0.6885, 0.8542])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9907])\n",
      "tensor([0.6264, 0.7389])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9912])\n",
      "tensor([-0.2191,  0.8882])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9900])\n",
      "tensor([-2.1055,  0.8849])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9912])\n",
      "tensor([-1.0657,  0.9137])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9924])\n",
      "tensor([-0.7321,  0.9016])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9899])\n",
      "tensor([-0.9544,  0.6637])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9923])\n",
      "tensor([0.6463, 0.7826])\n",
      "0\n",
      "1\n",
      "tensor([0.9938, 0.9910])\n",
      "tensor([0.6438, 0.7845])\n",
      "0\n",
      "1\n",
      "tensor([0.9938, 0.9917])\n",
      "tensor([0.6357, 0.7402])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9918])\n",
      "tensor([0.6443, 0.8222])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9902])\n",
      "tensor([0.8629, 0.7616])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9913])\n",
      "tensor([-10.6850,   0.9182])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9888])\n",
      "tensor([-19.5627,   0.3565])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9953, 0.9905])\n",
      "tensor([-2.8371,  0.1376])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9909])\n",
      "tensor([-3.0253,  0.1591])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9900])\n",
      "tensor([-3.8539,  0.7544])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9915])\n",
      "tensor([0.9762, 0.9147])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9909])\n",
      "tensor([0.5590, 0.9624])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9918])\n",
      "tensor([0.5981, 0.9215])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9922])\n",
      "tensor([0.5452, 0.9593])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9916])\n",
      "tensor([0.4690, 0.9496])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9920])\n",
      "tensor([0.8015, 0.9627])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9905])\n",
      "tensor([0.9354, 0.9818])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9906])\n",
      "tensor([0.7849, 0.8990])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9910])\n",
      "tensor([0.8150, 0.9688])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9897])\n",
      "tensor([0.8208, 0.9757])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9900])\n",
      "tensor([0.8877, 0.9561])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9908])\n",
      "tensor([0.8845, 0.9265])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9906])\n",
      "tensor([0.9520, 0.8906])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9917])\n",
      "tensor([0.9660, 0.9272])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9890])\n",
      "tensor([0.9759, 0.8782])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9912])\n",
      "tensor([0.9516, 0.7760])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9901])\n",
      "tensor([0.8817, 0.7497])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9906])\n",
      "tensor([0.9036, 0.7942])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9922])\n",
      "tensor([0.8782, 0.7053])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9915])\n",
      "tensor([0.9244, 0.7752])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9908])\n",
      "tensor([0.3455, 0.8504])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9902])\n",
      "tensor([0.9290, 0.8104])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9908])\n",
      "tensor([0.9208, 0.6497])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9915])\n",
      "tensor([0.9573, 0.6273])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9916])\n",
      "tensor([0.5383, 0.7103])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9902])\n",
      "tensor([0.8861, 0.6933])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9908])\n",
      "tensor([0.9226, 0.8027])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9916])\n",
      "tensor([-0.0088,  0.9507])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9924])\n",
      "tensor([0.7730, 0.8332])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9909])\n",
      "tensor([0.7457, 0.7388])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9913])\n",
      "tensor([0.8046, 0.9534])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9915])\n",
      "tensor([0.9665, 0.9049])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9914])\n",
      "tensor([0.7929, 0.7459])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9922])\n",
      "tensor([0.7622, 0.4969])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9915])\n",
      "tensor([0.8521, 0.8417])\n",
      "0\n",
      "1\n",
      "tensor([0.9936, 0.9912])\n",
      "tensor([0.9840, 0.9758])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9918])\n",
      "tensor([0.9840, 0.9680])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9909])\n",
      "tensor([0.9872, 0.9675])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9923])\n",
      "tensor([0.9726, 0.9464])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9905])\n",
      "tensor([0.9828, 0.9651])\n",
      "0\n",
      "1\n",
      "tensor([0.9940, 0.9912])\n",
      "tensor([0.8990, 0.2606])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9910])\n",
      "tensor([0.7512, 0.7553])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9918])\n",
      "tensor([0.8426, 0.4144])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9930])\n",
      "tensor([0.7559, 0.5871])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9903])\n",
      "tensor([0.8616, 0.4949])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9897])\n",
      "tensor([0.8374, 0.8056])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9912])\n",
      "tensor([0.7710, 0.7449])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9905])\n",
      "tensor([0.7900, 0.6634])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9917])\n",
      "tensor([0.6651, 0.7880])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9898])\n",
      "tensor([0.9185, 0.7817])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9915])\n",
      "tensor([0.7165, 0.7263])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9905])\n",
      "tensor([0.7517, 0.7646])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9905])\n",
      "tensor([0.6731, 0.7025])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9920])\n",
      "tensor([0.7121, 0.7462])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9906])\n",
      "tensor([0.7029, 0.7343])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9915])\n",
      "tensor([0.9735, 0.7822])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9916])\n",
      "tensor([0.9814, 0.8090])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9899])\n",
      "tensor([0.9099, 0.7991])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9933])\n",
      "tensor([0.9810, 0.8124])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9906])\n",
      "tensor([0.9754, 0.7826])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9901])\n",
      "tensor([-0.0148,  0.9181])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9902])\n",
      "tensor([-1.7572,  0.8612])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9902])\n",
      "tensor([-0.0383,  0.6555])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9922])\n",
      "tensor([-2.2976,  0.9131])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9915])\n",
      "tensor([0.2782, 0.4685])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9892])\n",
      "tensor([0.1870, 0.9503])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9902])\n",
      "tensor([0.8292, 0.9501])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9883])\n",
      "tensor([0.2618, 0.9473])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9908])\n",
      "tensor([0.8322, 0.9578])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9904])\n",
      "tensor([0.6366, 0.9213])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9903])\n",
      "tensor([0.1626, 0.8717])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9876])\n",
      "tensor([0.5061, 0.8655])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9899])\n",
      "tensor([0.4318, 0.9276])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9912])\n",
      "tensor([0.5611, 0.8413])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9908])\n",
      "tensor([0.2824, 0.8713])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9929])\n",
      "tensor([0.7309, 0.7510])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9924])\n",
      "tensor([0.8151, 0.7896])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9928])\n",
      "tensor([0.7051, 0.6495])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9936])\n",
      "tensor([0.8001, 0.7899])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9921])\n",
      "tensor([0.6129, 0.8454])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9922])\n",
      "tensor([-2.2321,  0.5125])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9920])\n",
      "tensor([-1.9226,  0.8610])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9931])\n",
      "tensor([-1.7529,  0.8894])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9932])\n",
      "tensor([-0.7137,  0.6345])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9921])\n",
      "tensor([-2.1038,  0.8979])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9923])\n",
      "tensor([0.9517, 0.7867])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9920])\n",
      "tensor([0.6517, 0.8177])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9933])\n",
      "tensor([0.6920, 0.7476])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9930])\n",
      "tensor([0.6050, 0.8178])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9923])\n",
      "tensor([0.8341, 0.7862])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9933])\n",
      "tensor([-2.8181,  0.3793])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9912])\n",
      "tensor([-3.7194,  0.3653])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9923])\n",
      "tensor([-4.2106, -1.5269])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9935])\n",
      "tensor([-3.4891,  0.5398])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9929])\n",
      "tensor([-4.0696,  0.5103])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9945])\n",
      "tensor([0.9756, 0.9503])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9929])\n",
      "tensor([0.5385, 0.9633])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9930])\n",
      "tensor([0.5128, 0.9485])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9938])\n",
      "tensor([0.6784, 0.9576])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9933])\n",
      "tensor([0.9680, 0.9527])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9929])\n",
      "tensor([0.9835, 0.9682])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9923])\n",
      "tensor([0.8676, 0.9688])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9926])\n",
      "tensor([0.7962, 0.9595])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9934])\n",
      "tensor([0.8108, 0.9614])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9922])\n",
      "tensor([0.9271, 0.9640])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9925])\n",
      "tensor([0.8608, 0.9727])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9926])\n",
      "tensor([0.9040, 0.9364])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9924])\n",
      "tensor([0.9505, 0.9550])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9931])\n",
      "tensor([0.9685, 0.9403])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9918])\n",
      "tensor([0.9761, 0.9599])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9933])\n",
      "tensor([0.9638, 0.7566])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9920])\n",
      "tensor([0.9246, 0.7649])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9920])\n",
      "tensor([0.9679, 0.8196])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9937])\n",
      "tensor([0.8687, 0.7674])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9932])\n",
      "tensor([0.9404, 0.7905])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9932])\n",
      "tensor([0.9108, 0.8672])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9917])\n",
      "tensor([0.9462, 0.8079])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9912])\n",
      "tensor([0.9494, 0.7769])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9933])\n",
      "tensor([0.8513, 0.6826])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9928])\n",
      "tensor([0.5644, 0.7385])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9931])\n",
      "tensor([0.0674, 0.9172])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9922])\n",
      "tensor([0.9312, 0.7772])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9935])\n",
      "tensor([0.4759, 0.8989])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9937])\n",
      "tensor([0.3067, 0.7985])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9931])\n",
      "tensor([0.7271, 0.7853])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9932])\n",
      "tensor([0.9458, 0.8228])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9923])\n",
      "tensor([0.9089, 0.9223])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9936])\n",
      "tensor([0.8098, 0.7517])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9940])\n",
      "tensor([0.7746, 0.5936])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9939])\n",
      "tensor([0.7791, 0.9377])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9933])\n",
      "tensor([0.9847, 0.9718])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9932])\n",
      "tensor([0.9713, 0.9719])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9936])\n",
      "tensor([0.9892, 0.9627])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9947])\n",
      "tensor([0.9919, 0.9575])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9941])\n",
      "tensor([0.9762, 0.9660])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9918])\n",
      "tensor([0.9161, 0.4081])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9926])\n",
      "tensor([0.8158, 0.7845])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9928])\n",
      "tensor([0.8488, 0.5172])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9944])\n",
      "tensor([0.7946, 0.5418])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9936])\n",
      "tensor([0.8508, 0.5927])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9927])\n",
      "tensor([0.8652, 0.8531])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9930])\n",
      "tensor([0.8343, 0.8348])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9934])\n",
      "tensor([0.8623, 0.6287])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9937])\n",
      "tensor([0.8608, 0.7613])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9932])\n",
      "tensor([0.8941, 0.7865])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9935])\n",
      "tensor([0.7234, 0.7666])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9920])\n",
      "tensor([0.7846, 0.7772])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9929])\n",
      "tensor([0.7154, 0.6712])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9947])\n",
      "tensor([0.6562, 0.7295])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9967, 0.9934])\n",
      "tensor([0.7739, 0.7507])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9940])\n",
      "tensor([0.9579, 0.8023])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9941])\n",
      "tensor([0.9792, 0.8079])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9918])\n",
      "tensor([0.7520, 0.8036])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9944])\n",
      "tensor([0.7992, 0.8090])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9932])\n",
      "tensor([0.9122, 0.7833])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9926])\n",
      "tensor([-0.6592,  0.9186])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9915])\n",
      "tensor([-2.1824,  0.6725])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9919])\n",
      "tensor([-0.1917,  0.6616])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9941])\n",
      "tensor([-1.7529,  0.7081])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9934])\n",
      "tensor([0.3933, 0.4898])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9919])\n",
      "tensor([0.8535, 0.9537])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9913])\n",
      "tensor([0.5742, 0.9449])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9920])\n",
      "tensor([0.3711, 0.9417])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9935])\n",
      "tensor([0.8065, 0.9476])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9923])\n",
      "tensor([0.4899, 0.9069])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9924])\n",
      "tensor([0.4725, 0.8639])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9898])\n",
      "tensor([0.1244, 0.8802])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9917])\n",
      "tensor([0.0507, 0.9209])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9934])\n",
      "tensor([0.4309, 0.8690])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9923])\n",
      "tensor([-0.1559,  0.8764])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9941])\n",
      "tensor([0.8849, 0.5949])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9936])\n",
      "tensor([0.7551, 0.6587])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9947])\n",
      "tensor([0.7041, 0.6814])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9936])\n",
      "tensor([0.8136, 0.8614])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9934])\n",
      "tensor([0.6014, 0.4998])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9934])\n",
      "tensor([-2.3414,  0.8521])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9942])\n",
      "tensor([-2.1217,  0.8593])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9942])\n",
      "tensor([-1.6275,  0.9217])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9937])\n",
      "tensor([-1.3822,  0.6614])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9925])\n",
      "tensor([-2.0078,  0.9089])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9938])\n",
      "tensor([0.9577, 0.7738])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9936])\n",
      "tensor([0.6428, 0.7048])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9941])\n",
      "tensor([0.6210, 0.7459])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9934])\n",
      "tensor([0.6318, 0.8192])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9925])\n",
      "tensor([0.8415, 0.7552])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9937])\n",
      "tensor([-15.7540,   0.2008])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9936])\n",
      "tensor([-3.0579,  0.8373])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9940])\n",
      "tensor([-26.5045,   0.1788])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9941])\n",
      "tensor([-4.1449,  0.7658])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9931])\n",
      "tensor([-4.4602,  0.8658])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9954])\n",
      "tensor([0.9665, 0.9586])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9944])\n",
      "tensor([0.9741, 0.9604])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9944])\n",
      "tensor([0.4915, 0.8984])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9947])\n",
      "tensor([0.7066, 0.9631])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9940])\n",
      "tensor([0.5211, 0.9466])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9947])\n",
      "tensor([0.8053, 0.9716])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9941])\n",
      "tensor([0.9777, 0.9640])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9940])\n",
      "tensor([0.7983, 0.8967])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9942])\n",
      "tensor([0.7931, 0.9706])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9929])\n",
      "tensor([0.9266, 0.9726])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9946])\n",
      "tensor([0.8816, 0.9259])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9938])\n",
      "tensor([0.9694, 0.9401])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9941])\n",
      "tensor([0.9536, 0.9168])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9952])\n",
      "tensor([0.9633, 0.9282])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9932])\n",
      "tensor([0.9201, 0.9521])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9943])\n",
      "tensor([0.9737, 0.7623])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9931])\n",
      "tensor([0.9712, 0.7655])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9933])\n",
      "tensor([0.9050, 0.7666])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9946])\n",
      "tensor([0.9267, 0.7670])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9934])\n",
      "tensor([0.9350, 0.7932])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9945])\n",
      "tensor([0.9307, 0.8152])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9935])\n",
      "tensor([0.9078, 0.7301])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9936])\n",
      "tensor([0.9439, 0.7989])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9942])\n",
      "tensor([0.7559, 0.7168])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9936])\n",
      "tensor([0.7228, 0.6328])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9945])\n",
      "tensor([0.6137, 0.8614])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9933])\n",
      "tensor([0.9009, 0.8157])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9937])\n",
      "tensor([0.6335, 0.9150])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9948])\n",
      "tensor([0.2968, 0.7918])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9930])\n",
      "tensor([0.9236, 0.9472])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9954])\n",
      "tensor([0.8256, 0.8806])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9938])\n",
      "tensor([0.8358, 0.7172])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9936])\n",
      "tensor([0.9479, 0.6565])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9954])\n",
      "tensor([0.8276, 0.6901])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9940])\n",
      "tensor([0.9607, 0.9331])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9954])\n",
      "tensor([0.9712, 0.9755])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9942])\n",
      "tensor([0.9867, 0.9680])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9942])\n",
      "tensor([0.9846, 0.9692])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9951])\n",
      "tensor([0.9849, 0.9690])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9950])\n",
      "tensor([0.9773, 0.9645])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9946])\n",
      "tensor([0.9224, 0.8337])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9936])\n",
      "tensor([0.9641, 0.7833])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9941])\n",
      "tensor([0.9635, 0.4156])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9950])\n",
      "tensor([0.8132, 0.5158])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9938])\n",
      "tensor([0.8470, 0.8529])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9943])\n",
      "tensor([0.8392, 0.8291])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9937])\n",
      "tensor([0.7547, 0.8438])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9934])\n",
      "tensor([0.8383, 0.7582])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9950])\n",
      "tensor([0.8212, 0.7877])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9937])\n",
      "tensor([0.8087, 0.7529])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9941])\n",
      "tensor([0.7331, 0.7717])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9937])\n",
      "tensor([0.7799, 0.7420])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9946])\n",
      "tensor([0.7081, 0.7003])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9953])\n",
      "tensor([0.6159, 0.7370])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9933])\n",
      "tensor([0.7370, 0.7474])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9940])\n",
      "tensor([0.7675, 0.9347])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9953])\n",
      "tensor([0.9800, 0.8099])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9926])\n",
      "tensor([0.8904, 0.7926])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9950])\n",
      "tensor([0.9202, 0.8286])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9939])\n",
      "tensor([0.9104, 0.7990])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9940])\n",
      "tensor([0.8698, 0.7263])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9927])\n",
      "tensor([-2.8770,  0.6361])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9942])\n",
      "tensor([-0.6927,  0.6315])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9940])\n",
      "tensor([-0.0574,  0.8855])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9945])\n",
      "tensor([0.3459, 0.8747])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9937])\n",
      "tensor([0.8630, 0.9496])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9930])\n",
      "tensor([0.4855, 0.9465])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9922])\n",
      "tensor([0.2305, 0.9620])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9934])\n",
      "tensor([-0.0273,  0.9499])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9943])\n",
      "tensor([0.4967, 0.9418])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9936])\n",
      "tensor([-0.0657,  0.9010])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9923])\n",
      "tensor([0.4982, 0.8737])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9941])\n",
      "tensor([-0.0301,  0.9095])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9943])\n",
      "tensor([-0.3632,  0.8950])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9944])\n",
      "tensor([0.0263, 0.9007])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9950])\n",
      "tensor([0.8690, 0.6591])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9946])\n",
      "tensor([0.7530, 0.7506])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9950])\n",
      "tensor([0.8787, 0.6286])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9941])\n",
      "tensor([0.8332, 0.8500])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9942])\n",
      "tensor([0.6257, 0.6831])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9948])\n",
      "tensor([-2.4351,  0.8545])\n",
      "0\n",
      "1\n",
      "tensor([0.9980, 0.9950])\n",
      "tensor([-2.2119,  0.8879])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9953])\n",
      "tensor([-1.4282,  0.8966])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9944])\n",
      "tensor([-2.4496,  0.6360])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9940])\n",
      "tensor([-1.9276,  0.8981])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9950])\n",
      "tensor([0.6659, 0.8241])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9945])\n",
      "tensor([0.6340, 0.7378])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9948])\n",
      "tensor([0.6170, 0.7368])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9942])\n",
      "tensor([0.6280, 0.8336])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9941])\n",
      "tensor([0.6580, 0.8167])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9943])\n",
      "tensor([-4.9801,  0.4170])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9944])\n",
      "tensor([-4.5683,  0.6702])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9950])\n",
      "tensor([-4.3742,  0.4017])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9947])\n",
      "tensor([-4.5680,  0.8679])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9941])\n",
      "tensor([-4.0132,  0.6419])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9953])\n",
      "tensor([0.9647, 0.8127])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9951])\n",
      "tensor([0.4410, 0.9573])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9953])\n",
      "tensor([0.5497, 0.8559])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9954])\n",
      "tensor([0.6297, 0.9562])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9951])\n",
      "tensor([0.5377, 0.9479])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9951])\n",
      "tensor([0.9837, 0.9745])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9944])\n",
      "tensor([0.7837, 0.9142])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9945])\n",
      "tensor([0.7979, 0.8983])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9950])\n",
      "tensor([0.7899, 0.9559])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9942])\n",
      "tensor([0.9440, 0.9451])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9951])\n",
      "tensor([0.8283, 0.8935])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9942])\n",
      "tensor([0.9616, 0.9451])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9946])\n",
      "tensor([0.9588, 0.9392])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9948])\n",
      "tensor([0.9689, 0.9084])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9946])\n",
      "tensor([0.9730, 0.9272])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9971, 0.9952])\n",
      "tensor([0.8943, 0.8021])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9937])\n",
      "tensor([0.9780, 0.7673])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9944])\n",
      "tensor([0.9723, 0.7330])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9951])\n",
      "tensor([0.9357, 0.7687])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9939])\n",
      "tensor([0.9185, 0.8112])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9952])\n",
      "tensor([0.9379, 0.7147])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9936])\n",
      "tensor([0.9331, 0.7444])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9944])\n",
      "tensor([0.9325, 0.7891])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9949])\n",
      "tensor([0.6766, 0.7208])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9939])\n",
      "tensor([0.4666, 0.7278])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9954])\n",
      "tensor([0.9210, 0.8880])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9940])\n",
      "tensor([0.9672, 0.8340])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9940])\n",
      "tensor([0.8688, 0.9259])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9954])\n",
      "tensor([0.4112, 0.8122])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9943])\n",
      "tensor([0.9179, 0.9303])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9956])\n",
      "tensor([0.9565, 0.7333])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9947])\n",
      "tensor([0.9148, 0.9087])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9941])\n",
      "tensor([0.9564, 0.6540])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9947])\n",
      "tensor([0.8076, 0.6313])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9949])\n",
      "tensor([0.8125, 0.9274])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9955])\n",
      "tensor([0.9721, 0.9799])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9944])\n",
      "tensor([0.9881, 0.9692])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9951])\n",
      "tensor([0.9795, 0.9679])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9957])\n",
      "tensor([0.9888, 0.9694])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9952])\n",
      "tensor([0.9778, 0.9649])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9947])\n",
      "tensor([0.9597, 0.6874])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9943])\n",
      "tensor([0.8202, 0.8009])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9949])\n",
      "tensor([0.9577, 0.5020])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9953])\n",
      "tensor([0.8021, 0.4663])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9946])\n",
      "tensor([0.8421, 0.8362])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9946])\n",
      "tensor([0.8326, 0.8288])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9946])\n",
      "tensor([0.7824, 0.8407])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9943])\n",
      "tensor([0.7877, 0.8110])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9948])\n",
      "tensor([0.7916, 0.8186])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9943])\n",
      "tensor([0.7723, 0.7551])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9945])\n",
      "tensor([0.7331, 0.7834])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9948])\n",
      "tensor([0.7660, 0.7460])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9952])\n",
      "tensor([0.7068, 0.7173])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9955])\n",
      "tensor([0.6851, 0.7380])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9947])\n",
      "tensor([0.7430, 0.7510])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9952])\n",
      "tensor([0.9471, 0.8095])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9957])\n",
      "tensor([0.7470, 0.8040])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9950])\n",
      "tensor([0.9138, 0.7839])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9956])\n",
      "tensor([0.9026, 0.8313])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9952])\n",
      "tensor([0.9740, 0.8119])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9945])\n",
      "tensor([-1.6990,  0.6857])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9939])\n",
      "tensor([-2.5549,  0.7427])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9946])\n",
      "tensor([0.8207, 0.7091])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9946])\n",
      "tensor([-0.0288,  0.9235])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9948])\n",
      "tensor([-0.3323,  0.6598])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9945])\n",
      "tensor([0.2232, 0.9459])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9937])\n",
      "tensor([0.3733, 0.9395])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9948])\n",
      "tensor([0.9180, 0.8954])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9947])\n",
      "tensor([0.2234, 0.9532])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9947])\n",
      "tensor([0.4815, 0.9462])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9941])\n",
      "tensor([0.4578, 0.9160])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9936])\n",
      "tensor([0.4888, 0.8826])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9943])\n",
      "tensor([0.4594, 0.9143])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9950])\n",
      "tensor([-0.6127,  0.8899])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9947])\n",
      "tensor([-0.1609,  0.9192])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9956])\n",
      "tensor([0.8777, 0.6690])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9960])\n",
      "tensor([0.7636, 0.5305])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9957])\n",
      "tensor([0.8594, 0.7325])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9950])\n",
      "tensor([0.7892, 0.8614])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9950])\n",
      "tensor([0.7127, 0.6103])\n",
      "0\n",
      "1\n",
      "tensor([0.9980, 0.9947])\n",
      "tensor([-2.2115,  0.9431])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9958])\n",
      "tensor([-0.6463,  0.8773])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9956])\n",
      "tensor([-0.9819,  0.8543])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9952])\n",
      "tensor([-2.4848,  0.8167])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9940])\n",
      "tensor([-1.9005,  0.8942])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9946])\n",
      "tensor([0.6324, 0.7493])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9954])\n",
      "tensor([0.6473, 0.7411])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9953])\n",
      "tensor([0.6022, 0.7307])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9949])\n",
      "tensor([0.6303, 0.8482])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9944])\n",
      "tensor([0.7464, 0.7497])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9951])\n",
      "tensor([-3.6963,  0.5175])\n",
      "0\n",
      "1\n",
      "tensor([0.9981, 0.9953])\n",
      "tensor([-4.7041,  0.5166])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9957])\n",
      "tensor([-4.4762,  0.3545])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9951])\n",
      "tensor([-4.1452,  0.8913])\n",
      "0\n",
      "1\n",
      "tensor([0.9980, 0.9946])\n",
      "tensor([-3.9365,  0.6231])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9955])\n",
      "tensor([0.9662, 0.8139])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9955])\n",
      "tensor([0.4550, 0.9632])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9956])\n",
      "tensor([0.5387, 0.8345])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9960])\n",
      "tensor([0.5749, 0.9543])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9956])\n",
      "tensor([0.5548, 0.9420])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9953])\n",
      "tensor([0.8047, 0.9238])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9953])\n",
      "tensor([0.8161, 0.9611])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9954])\n",
      "tensor([0.8161, 0.9034])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9956])\n",
      "tensor([0.7950, 0.9561])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9947])\n",
      "tensor([0.9332, 0.9346])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9954])\n",
      "tensor([0.8103, 0.9231])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9947])\n",
      "tensor([0.8444, 0.9449])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9955])\n",
      "tensor([0.9637, 0.9303])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9956])\n",
      "tensor([0.9768, 0.8776])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9951])\n",
      "tensor([0.9743, 0.9325])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9958])\n",
      "tensor([0.9783, 0.7447])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9945])\n",
      "tensor([0.9724, 0.7568])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9954])\n",
      "tensor([0.9676, 0.7326])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9958])\n",
      "tensor([0.9224, 0.7798])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9950])\n",
      "tensor([0.9610, 0.8095])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9954])\n",
      "tensor([0.7472, 0.7130])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9949])\n",
      "tensor([0.9418, 0.7007])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9952])\n",
      "tensor([0.7912, 0.7937])\n",
      "0\n",
      "1\n",
      "tensor([0.9981, 0.9956])\n",
      "tensor([0.7452, 0.7419])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9950])\n",
      "tensor([0.5872, 0.7486])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9955])\n",
      "tensor([0.8326, 0.8917])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9950])\n",
      "tensor([0.5510, 0.8464])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9956])\n",
      "tensor([0.8351, 0.9301])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9959])\n",
      "tensor([0.3836, 0.8274])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9947])\n",
      "tensor([0.7867, 0.8999])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9960])\n",
      "tensor([0.9581, 0.7347])\n",
      "0\n",
      "1\n",
      "tensor([0.9981, 0.9954])\n",
      "tensor([0.8590, 0.9045])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9949])\n",
      "tensor([0.9529, 0.7614])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9958])\n",
      "tensor([0.8181, 0.6571])\n",
      "0\n",
      "1\n",
      "tensor([0.9981, 0.9952])\n",
      "tensor([0.8071, 0.9291])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9958])\n",
      "tensor([0.9789, 0.9759])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9953])\n",
      "tensor([0.9797, 0.9707])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9953])\n",
      "tensor([0.9851, 0.9697])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9963])\n",
      "tensor([0.9854, 0.9712])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9950])\n",
      "tensor([0.9780, 0.9696])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9954])\n",
      "tensor([0.9545, 0.5587])\n",
      "0\n",
      "1\n",
      "tensor([0.9981, 0.9951])\n",
      "tensor([0.7852, 0.3974])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9956])\n",
      "tensor([0.9484, 0.4806])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9960])\n",
      "tensor([0.8915, 0.8564])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9951])\n",
      "tensor([0.8749, 0.8308])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9954])\n",
      "tensor([0.7943, 0.7871])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9955])\n",
      "tensor([0.7566, 0.8102])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9954])\n",
      "tensor([0.9646, 0.9511])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9954])\n",
      "tensor([0.7748, 0.8030])\n",
      "0\n",
      "1\n",
      "tensor([0.9981, 0.9948])\n",
      "tensor([0.7833, 0.7550])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9951])\n",
      "tensor([0.7353, 0.7767])\n",
      "0\n",
      "1\n",
      "tensor([0.9980, 0.9959])\n",
      "tensor([0.7538, 0.7452])\n",
      "0\n",
      "1\n",
      "tensor([0.9980, 0.9956])\n",
      "tensor([0.7067, 0.7159])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9961])\n",
      "tensor([0.5884, 0.7440])\n",
      "0\n",
      "1\n",
      "tensor([0.9981, 0.9950])\n",
      "tensor([0.7441, 0.7513])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9961])\n",
      "tensor([0.9574, 0.8129])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9965])\n",
      "tensor([0.9772, 0.8080])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9954])\n",
      "tensor([0.9008, 0.7787])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9964])\n",
      "tensor([0.6864, 0.8265])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9957])\n",
      "tensor([0.9075, 0.8206])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9954])\n",
      "tensor([0.8081, 0.6551])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9953])\n",
      "tensor([-2.5466,  0.9321])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9954])\n",
      "tensor([-0.1412,  0.6437])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9953])\n",
      "tensor([-0.0467,  0.9152])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9956])\n",
      "tensor([-0.0963,  0.6512])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9954])\n",
      "tensor([0.8907, 0.9486])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9955])\n",
      "tensor([0.8352, 0.9490])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9951])\n",
      "tensor([0.1253, 0.8728])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9950])\n",
      "tensor([0.0746, 0.9509])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9952])\n",
      "tensor([0.4370, 0.9506])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9951])\n",
      "tensor([0.4694, 0.9240])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9979, 0.9956])\n",
      "tensor([0.4348, 0.8924])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9956])\n",
      "tensor([0.0159, 0.8994])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9956])\n",
      "tensor([-0.3033,  0.9060])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9956])\n",
      "tensor([-0.0912,  0.9000])\n"
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "train_p = np.linspace(100,800,8)\n",
    "R2_test = torch.zeros(len(train_p),reps,len(meshes),2)\n",
    "R2_leftout= torch.zeros(len(train_p),reps,len(meshes),2)\n",
    "for k in range(len(train_p)):\n",
    "    for i in range(len(meshes)):\n",
    "        for j in range(reps):\n",
    "            X=torch.cat(train_input_modes[0:i]+train_input_modes[i+1:])[:,0:15]\n",
    "            y=torch.cat(train_output_modes[:i]+train_output_modes[i+1:])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=int(train_p[k]),\n",
    "                random_state=j\n",
    "            )\n",
    "            X_test= torch.cat(test_input_modes[0:i]+test_input_modes[i+1:])[:,0:15]\n",
    "            y_test=torch.cat(test_output_modes[:i]+test_output_modes[i+1:])\n",
    "            emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "\n",
    "            meanR, stdR = meanR, stdR = emulator.R2_sample(X_test,y_test,1000)\n",
    "            R2_test[k,j,i,:]=meanR\n",
    "\n",
    "            meanR, stdR=emulator.R2_sample(test_input_modes[i][:,0:15],test_output_modes[i],1000) \n",
    "            R2_leftout[k,j,i,:] = meanR\n",
    "            print(R2_test[k,j,i,:])\n",
    "            print(R2_leftout[k,j,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c76b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ad603b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0912,  0.9000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "110fb965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4859,  0.7612],\n",
       "        [ 0.4449,  0.7915],\n",
       "        [ 0.3483,  0.7629],\n",
       "        [ 0.1232,  0.7866],\n",
       "        [ 0.3140,  0.7787],\n",
       "        [-0.0562,  0.8120],\n",
       "        [ 0.2519,  0.8131],\n",
       "        [ 0.3006,  0.8112]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_leftout.mean(axis=[1,2])[:8].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3da43b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9512, 0.9511],\n",
       "        [0.9859, 0.9783],\n",
       "        [0.9925, 0.9868],\n",
       "        [0.9950, 0.9909],\n",
       "        [0.9962, 0.9929],\n",
       "        [0.9968, 0.9940],\n",
       "        [0.9974, 0.9947],\n",
       "        [0.9977, 0.9954]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_test.mean(axis=[1,2])[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796ab181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(train_p[:8],R2_test.mean(axis=[1,2])[:8,0].detach().numpy(),fmt='o',yerr=R2_test.std(axis=[1,2])[:8,0].detach().numpy())\n",
    "plt.errorbar(train_p[:8],R2_test.mean(axis=[1,2])[:8,1].detach().numpy(),fmt='o',yerr=R2_test.std(axis=[1,2])[:8,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Training Points')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTLatentNLeftin.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "401de880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(train_p[:8],R2_leftout.mean(axis=[1,2])[:8,0].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=[1,2])[:8,0].detach().numpy())\n",
    "plt.errorbar(train_p[:8],R2_leftout.mean(axis=[1,2])[:8,1].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=[1,2])[:8,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Training Points')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTLatentNLeftout.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eb0391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$R^2$')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "me=3\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(train_p[:8],R2_leftout[:,:,me].mean(axis=1)[:8,0].detach().numpy(),fmt='o',yerr=R2_leftout[:,:,me].std(axis=1)[:8,0].detach().numpy())\n",
    "plt.errorbar(train_p[:8],R2_leftout[:,:,me].mean(axis=1)[:8,1].detach().numpy(),fmt='o',yerr=R2_leftout[:,:,me].std(axis=1)[:8,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Training Points')\n",
    "plt.ylabel('$R^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58e4d7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 200., 300., 400., 500., 600., 700., 800.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5e36b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 19, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_leftout.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f64eae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8005, -1.6450,  0.6517, -4.1916,  0.6179,  0.8330,  0.9139,  0.9603,\n",
       "         0.7625,  0.6778,  0.8791,  0.9814,  0.8909,  0.8147,  0.7057,  0.8858,\n",
       "        -0.4045,  0.4726,  0.1051])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_leftout.mean(axis=1)[7,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f286261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[7,:,0].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[7,:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[7,:,1].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[7,:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTLatent800Leftout.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38a08a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120.8960, 119.6680], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_output[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24ec63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ab80dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b67ee9d0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_input[3][:,3],all_output[3][:,0],'o')\n",
    "plt.plot(all_input[comp][:,3],all_output[comp][:,0],'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb25a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.7487,   0.2863,   2.9323,   0.3908,   0.4751,   2.8957, -15.0770,\n",
       "         28.8892,   8.6449,   4.7580,   1.3011,   5.9833, -12.7411,  29.4289,\n",
       "          0.9004,  -3.1176,   5.3607,  -4.3849,  -5.0521,  -0.3224,   1.7646,\n",
       "          0.2812,   0.3799,   1.4111], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_modes[3][6:15][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ace1af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cb89cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b6792090>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r2check = torch.zeros(len(meshes),2)\n",
    "for i in range(len(meshes)):\n",
    "    #print(torch.sum(torch.abs(test_input_modes[check][6:15][0]-test_input_modes[i][6:15][0])))\n",
    "    r2check[i]=emulators[i].R2(test_input[check],test_output[check])\n",
    "\n",
    "r2check=r2check.detach().numpy()\n",
    "plt.plot(r2check,'o')\n",
    "plt.legend(('A_TAT','V_TAT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3e1d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "228bcf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(224.9606, dtype=torch.float64)\n",
      "\n",
      "tensor(167.3161, dtype=torch.float64)\n",
      "\n",
      "tensor(203.0570, dtype=torch.float64)\n",
      "\n",
      "tensor(0., dtype=torch.float64)\n",
      "\n",
      "tensor(230.9404, dtype=torch.float64)\n",
      "\n",
      "tensor(241.2827, dtype=torch.float64)\n",
      "\n",
      "tensor(221.6740, dtype=torch.float64)\n",
      "\n",
      "tensor(197.9337, dtype=torch.float64)\n",
      "\n",
      "tensor(257.0497, dtype=torch.float64)\n",
      "\n",
      "tensor(222.4901, dtype=torch.float64)\n",
      "\n",
      "tensor(259.4040, dtype=torch.float64)\n",
      "\n",
      "tensor(257.9458, dtype=torch.float64)\n",
      "\n",
      "tensor(191.6570, dtype=torch.float64)\n",
      "\n",
      "tensor(233.2693, dtype=torch.float64)\n",
      "\n",
      "tensor(210.3870, dtype=torch.float64)\n",
      "\n",
      "tensor(176.0385, dtype=torch.float64)\n",
      "\n",
      "tensor(201.6297, dtype=torch.float64)\n",
      "\n",
      "tensor(227.6242, dtype=torch.float64)\n",
      "\n",
      "tensor(211.0256, dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(meshes)):\n",
    "    print(torch.sum(torch.abs(test_input_modes[3][6:15][0]-test_input_modes[i][6:15][0])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dbdcc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$R^2$')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[7,:,0].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[7,:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[7,:,1].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[7,:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d021bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.9972, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.9972, 0.9958],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.9972, 0.9958],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9979, 0.9949]]])\n"
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "\n",
    "R2_test = torch.zeros(len(meshes),reps,2)\n",
    "R2_leftout= torch.zeros(len(meshes),reps,2)\n",
    "for i in range(len(meshes)):\n",
    "    for j in range(reps):\n",
    "        X=torch.cat(train_input_modes[0:i]+train_input_modes[i+1:])[:,0:15]\n",
    "        y=torch.cat(train_output_modes[:i]+train_output_modes[i+1:])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            train_size=800,\n",
    "            random_state=j+1\n",
    "        )\n",
    "        X_test= torch.cat(test_input_modes[0:i]+test_input_modes[i+1:])[:,0:15]\n",
    "        y_test=torch.cat(test_output_modes[:i]+test_output_modes[i+1:])\n",
    "        emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "        \n",
    "        meanR, stdR = emulator.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        R2_test[i,j,:]=meanR\n",
    "        \n",
    "        meanR, stdR=emulator.R2_sample(test_input_modes[i][:,0:15],test_output_modes[i],1000) \n",
    "        R2_leftout[i,j,:] = meanR\n",
    "        print(R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900, 15])\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9600, 0.9834],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "torch.Size([1800, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4625552892684937 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.132591724395752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3149105310440063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1619206666946411 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4455400705337524 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2014954090118408 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2668914794921875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3148146867752075 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.570530652999878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1161659955978394 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0732165575027466 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2088576555252075 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0254359245300293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7823798656463623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4442801475524902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6301296949386597 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.281448245048523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1493229866027832 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2636395692825317 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6473190784454346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.332882046699524 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.079277515411377 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5930168628692627 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3182892799377441 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6321877241134644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.903699517250061 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.936606526374817 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.983797550201416 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4051064252853394 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.61078679561615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4964251518249512 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.532096028327942 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5299042463302612 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7676578760147095 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3296480178833008 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6747504472732544 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.50166654586792 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4260333776474 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4162123203277588 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4662822484970093 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.420390009880066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4018936157226562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.898108959197998 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.321775197982788 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3911134004592896 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6362839937210083 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3267210721969604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2899376153945923 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5169998407363892 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4665805101394653 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2447763681411743 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5478298664093018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7467246055603027 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3729939460754395 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8873008489608765 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5388680696487427 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6538021564483643 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8380810022354126 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.47191321849823 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3491942882537842 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5211368799209595 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3823050260543823 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5761760473251343 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1858813762664795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3876798152923584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3156687021255493 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.400022029876709 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4128196239471436 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7509723901748657 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8310847282409668 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.261846661567688 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8687773942947388 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3985919952392578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6744064092636108 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9429470300674438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6357998847961426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8891105651855469 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2475461959838867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.144946336746216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4959614276885986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5753810405731201 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5587964057922363 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.796898365020752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8383961915969849 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3705317974090576 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1333327293395996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9987274408340454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4007998704910278 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2781435251235962 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6857184171676636 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3569025993347168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3480974435806274 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1934956312179565 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1144503355026245 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4499258995056152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6328949928283691 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.945038914680481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4856038093566895 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2291038036346436 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.481848955154419 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3784860372543335 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.50339937210083 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3325891494750977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5475010871887207 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3568294048309326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4583237171173096 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3635421991348267 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2772926092147827 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4573060274124146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9107714891433716 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.310269832611084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4041587114334106 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6587353944778442 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3707120418548584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8785251379013062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7172088623046875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3394691944122314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3289275169372559 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.392327904701233 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3966524600982666 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5771708488464355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2773733139038086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4282633066177368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.558749794960022 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2832413911819458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.897858738899231 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4176381826400757 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4635027647018433 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.589793086051941 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2097134590148926 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6159971952438354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.411194920539856 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5922402143478394 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.520060420036316 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.63733971118927 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4708586931228638 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.372116208076477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4859286546707153 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3939656019210815 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7409358024597168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3562668561935425 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2659128904342651 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7738391160964966 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2941228151321411 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.576833724975586 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8225234746932983 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3628170490264893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1228914260864258 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2472517490386963 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3493424654006958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.383994698524475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7185699939727783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3025702238082886 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7660313844680786 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6145973205566406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2596789598464966 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0149667263031006 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5881400108337402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5737065076828003 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5121842622756958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.161685824394226 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1202976703643799 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.444377064704895 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6371185779571533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.904943823814392 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5699652433395386 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5574885606765747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4072754383087158 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7442044019699097 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5312014818191528 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3654835224151611 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6023997068405151 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.188967227935791 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5306472778320312 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2237547636032104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.157079815864563 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5168406963348389 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5310555696487427 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6026225090026855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6588530540466309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7063689231872559 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7205817699432373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4621644020080566 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.744606852531433 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.897101879119873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9776701927185059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.041908025741577 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9408611059188843 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7551220655441284 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8979412317276 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1227625608444214 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5100349187850952 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4816908836364746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3439735174179077 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.597388505935669 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5440239906311035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4179881811141968 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3775757551193237 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2448757886886597 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5191341638565063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.896843671798706 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8019013404846191 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4554439783096313 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3961567878723145 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5986206531524658 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4893494844436646 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8999518156051636 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0961798429489136 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6884421110153198 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6303627490997314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2720447778701782 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.044032335281372 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7002476453781128 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5443332195281982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4740885496139526 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7084442377090454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.040358543395996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1113708019256592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5703147649765015 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.311374306678772 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5733662843704224 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6939235925674438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6929569244384766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8778823614120483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.851263165473938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7202644348144531 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2999789714813232 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9542025327682495 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4267535209655762 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.340601682662964 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.505669593811035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3197877407073975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6880491971969604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8936899900436401 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7055026292800903 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.590250015258789 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.355889320373535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8670185804367065 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.762484073638916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7237839698791504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6949782371520996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.944785237312317 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3807411193847656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5679458379745483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8926440477371216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9416508674621582 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.114847421646118 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5173771381378174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.665497899055481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3367526531219482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9470603466033936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5380340814590454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4535964727401733 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8614169359207153 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5385489463806152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1608731746673584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.213977813720703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9132723808288574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8014578819274902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.662862777709961 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.233459234237671 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7273354530334473 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.876763939857483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.091348648071289 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1989588737487793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7797013521194458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6999284029006958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6720402240753174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0783274173736572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8729878664016724 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6837986707687378 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6590244770050049 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.155486583709717 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.039872884750366 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2042129039764404 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5300227403640747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5921639204025269 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3664950132369995 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5840965509414673 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7458688020706177 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.291245460510254 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8520747423171997 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.791183590888977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.932318091392517 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2083780765533447 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5427311658859253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9736980199813843 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9267691373825073 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1814661026000977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9955815076828003 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4023866653442383 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2858433723449707 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.204343795776367 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0384111404418945 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8752124309539795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1321022510528564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8701262474060059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6624866724014282 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.344211220741272 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8834941387176514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5699537992477417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9801304340362549 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0810208320617676 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7266944646835327 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4347816705703735 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.440128207206726 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.098736047744751 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0656936168670654 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4803398847579956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8865970373153687 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7230467796325684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2073858976364136 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3275172710418701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5832480192184448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2982559204101562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4225044250488281 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6845874786376953 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5557667016983032 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7235077619552612 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2512767314910889 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5201359987258911 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8127654790878296 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5280593633651733 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7011387348175049 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.109492778778076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5453342199325562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6668720245361328 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8050510883331299 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1996848583221436 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.509374737739563 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4342191219329834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5636310577392578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4927277565002441 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9934319257736206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7534053325653076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3904753923416138 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6023989915847778 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0678133964538574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.860920786857605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2029318809509277 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4513545036315918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.780530571937561 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6988216638565063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5973553657531738 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.673784613609314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5315825939178467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2952407598495483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4129453897476196 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.654703974723816 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.054837226867676 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4436081647872925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9967708587646484 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7455421686172485 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2106537818908691 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3225958347320557 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.27753746509552 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7525078058242798 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7490779161453247 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9062767028808594 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6134647130966187 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.636654257774353 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4316247701644897 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7101256847381592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5948150157928467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2280853986740112 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3813326358795166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3547637462615967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4732810258865356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7237350940704346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.752119779586792 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0564637184143066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4328805208206177 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0051186084747314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0567514896392822 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3423287868499756 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1261956691741943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6519602537155151 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4052433967590332 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4557311534881592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5346603393554688 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2721524238586426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5989402532577515 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8554824590682983 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5119566917419434 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3148772716522217 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9221590757369995 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7464028596878052 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.850676417350769 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7695773839950562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.724771738052368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3066518306732178 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2707083225250244 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6149344444274902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0587921142578125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.554145336151123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.864306926727295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9298774003982544 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.488921880722046 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.914072275161743 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.292527675628662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.013042688369751 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.698734998703003 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.221745491027832 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.598071336746216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3421051502227783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7038906812667847 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0244176387786865 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0331199169158936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8789243698120117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4280316829681396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3312439918518066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.877765417098999 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2780416011810303 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.179182529449463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4601852893829346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9552130699157715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7125403881073 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.896360158920288 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.293351888656616 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.319225549697876 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2116448879241943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.446221113204956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4120090007781982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.987372636795044 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3452256917953491 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4461055994033813 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0975873470306396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.450713872909546 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0201828479766846 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8658976554870605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5002514123916626 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.11991286277771 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.96168053150177 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.485431432723999 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8789594173431396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2889325618743896 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8974220752716064 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8158985376358032 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8701202869415283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0925283432006836 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1137053966522217 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2128868103027344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6109790802001953 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5630779266357422 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7400046586990356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5733810663223267 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6095792055130005 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0884339809417725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9714452028274536 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5838773250579834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8249999284744263 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3471405506134033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5981570482254028 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0177066326141357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.162726640701294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.301835536956787 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6894588470458984 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8977749347686768 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7657397985458374 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3139612674713135 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0047414302825928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6817009449005127 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8537694215774536 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9512642621994019 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.880014419555664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0064876079559326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0999491214752197 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6902976036071777 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1809585094451904 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3675389289855957 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.644697427749634 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2905173301696777 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.074939012527466 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.306741952896118 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5544402599334717 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0802175998687744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.916052222251892 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.464897871017456 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5456345081329346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0788304805755615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1464805603027344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.827033281326294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3749170303344727 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.172776460647583 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.294766902923584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9408833980560303 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0906982421875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2378876209259033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9029109477996826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5547550916671753 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2053897380828857 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7397196292877197 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4254095554351807 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.355140447616577 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9347038269042969 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.434032678604126 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.139580726623535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.33015513420105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7241742610931396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.737037420272827 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3995745182037354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2096657752990723 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4620463848114014 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3987646102905273 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.599606513977051 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.150935649871826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6623629331588745 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3386361598968506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9384584426879883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0479040145874023 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3568296432495117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.602842330932617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.795742988586426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1101105213165283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.163707971572876 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.325045347213745 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4833812713623047 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2534353733062744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3316755294799805 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3218185901641846 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7958598136901855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3336071968078613 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4695777893066406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2665677070617676 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1840434074401855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.858435034751892 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9180353879928589 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2546160221099854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0397515296936035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8122388124465942 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.251619338989258 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.594313144683838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.079232931137085 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.534722089767456 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.742008924484253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6786084175109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.695833683013916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9056334495544434 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4049509763717651 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.88258695602417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.863990306854248 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8193697929382324 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2804698944091797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4355664253234863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7942185401916504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2363221645355225 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.815860629081726 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4691033363342285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3757197856903076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7989203929901123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7100467681884766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.073322057723999 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.677842617034912 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9003840684890747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2024991512298584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.446212887763977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0313432216644287 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3210339546203613 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3577566146850586 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9705203771591187 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0455620288848877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4257192611694336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7795307636260986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8847030401229858 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7581722736358643 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.228996515274048 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.403557062149048 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.882375717163086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.997697353363037 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3497955799102783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.722330093383789 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9656500816345215 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8610584735870361 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1811089515686035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8182048797607422 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7874783277511597 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.90520441532135 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.166551113128662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9379520416259766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5132901668548584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.195101022720337 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.926832675933838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8688945770263672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.017758369445801 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7727909088134766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9852045774459839 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8912562131881714 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4869728088378906 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7753167152404785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1318814754486084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7053563594818115 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8436598777770996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8351248502731323 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5909885168075562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5500988960266113 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.408113956451416 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.005478620529175 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.781924843788147 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0339303016662598 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6089122295379639 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4235520362854004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.519721031188965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7202484607696533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7268693447113037 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.48648738861084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8250923156738281 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6814767122268677 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5575684309005737 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8848309516906738 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1248667240142822 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3490827083587646 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8575160503387451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8022621870040894 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9696682691574097 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4255645275115967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8434855937957764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8330551385879517 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2990193367004395 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5399081707000732 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6215277910232544 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.269151210784912 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.260078191757202 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.730763554573059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6026474237442017 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5414433479309082 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8160518407821655 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.850890040397644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0329983234405518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.955670952796936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8054660558700562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3177716732025146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.139662742614746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5678484439849854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.251235008239746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3061439990997314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6778782606124878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2668917179107666 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9817239046096802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.629364490509033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7800383567810059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4897208213806152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.073235511779785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6089224815368652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4374990463256836 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.933424472808838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.986764907836914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.400916814804077 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9636237621307373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5972177982330322 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0201966762542725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2584002017974854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.433518409729004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2540676593780518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.04231333732605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9967097043991089 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4666049480438232 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7053508758544922 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2808611392974854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.043732166290283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.790611982345581 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8474361896514893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2433440685272217 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9119805097579956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.085249662399292 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.69035005569458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.420957326889038 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6597909927368164 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6432651281356812 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.832143783569336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0376741886138916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4118645191192627 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5904346704483032 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6734650135040283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7829114198684692 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.522078514099121 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.381815195083618 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.054316520690918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2197425365448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8561094999313354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4637534618377686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9412739276885986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3026556968688965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.940070390701294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2052927017211914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3269760608673096 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7008895874023438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.030654191970825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0158421993255615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9089152812957764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9920296669006348 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7400959730148315 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5689914226531982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8190934658050537 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.033801794052124 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8970386981964111 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.247685194015503 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9558018445968628 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6696701049804688 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.905513048171997 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5784976482391357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0165281295776367 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7879875898361206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8746856451034546 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4507784843444824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7323001623153687 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2470812797546387 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.499743700027466 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9236668348312378 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9756461381912231 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6174449920654297 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8248251676559448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7798094749450684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8107956647872925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7389734983444214 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5283260345458984 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.139071464538574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1490933895111084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7103636264801025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9432501792907715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2880659103393555 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.542941093444824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6428333520889282 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.155548334121704 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8847945928573608 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.194692850112915 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.677776575088501 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9410099983215332 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8922263383865356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.644450306892395 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4856088161468506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2182681560516357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.071150064468384 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2690958976745605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7745438814163208 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2057504653930664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0501973628997803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9423394203186035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7088406085968018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.764437198638916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.059222936630249 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.793748378753662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.994359016418457 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.581688642501831 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8351094722747803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2480692863464355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.149195671081543 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4554176330566406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9082027673721313 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.711787223815918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.643360137939453 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5865824222564697 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.188767433166504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6581907272338867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.544426202774048 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.520688772201538 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.537414789199829 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0800249576568604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.072842597961426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.497920036315918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.226316213607788 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.098219871520996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9487435817718506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.799689292907715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.096334218978882 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.012223243713379 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8008711338043213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.559096336364746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.500701904296875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8088645935058594 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2749826908111572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3188464641571045 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2340807914733887 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.768873929977417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9145522117614746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.653366804122925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5544016361236572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.176614284515381 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7836623191833496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.34299898147583 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2162303924560547 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.843956232070923 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3292717933654785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0713677406311035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0415289402008057 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4891111850738525 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4976866245269775 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.301008701324463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.467606544494629 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8330435752868652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7436232566833496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.556941509246826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.380281448364258 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9018019437789917 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9571053981781006 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.431123971939087 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8045296669006348 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4583866596221924 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.068295955657959 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3096699714660645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9914391040802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0417611598968506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.290057897567749 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.85921311378479 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.963194727897644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.291820049285889 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2845706939697266 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4628093242645264 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2195591926574707 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0125861167907715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.299164056777954 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.440981388092041 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2544667720794678 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9667840003967285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2292115688323975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6828677654266357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.414893865585327 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.61568284034729 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.34741473197937 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6444051265716553 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.11551833152771 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.486266851425171 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4189412593841553 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8771464824676514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.292691946029663 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2788736820220947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1281578540802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.564653158187866 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2130212783813477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.34871244430542 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.940629005432129 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4060280323028564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.892001152038574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.653571844100952 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7330873012542725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.111645460128784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3081789016723633 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2953569889068604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0136070251464844 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.771225690841675 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8006844520568848 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6901419162750244 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7725555896759033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8594815731048584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.222510576248169 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0296530723571777 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.011212110519409 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.862474203109741 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.46105694770813 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.968768358230591 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8596785068511963 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2502694129943848 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.285919666290283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7402384281158447 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8706815242767334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.601966619491577 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1348698139190674 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6034600734710693 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.55915904045105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1642580032348633 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.426215171813965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.549410820007324 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4337189197540283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2881650924682617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4604499340057373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7393381595611572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4818081855773926 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1564416885375977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.608165740966797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4719066619873047 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7108993530273438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.776047706604004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.558455467224121 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6127068996429443 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.451889991760254 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2079789638519287 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.386016845703125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.261230707168579 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.963427782058716 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.64528226852417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9679250717163086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.011821746826172 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7382192611694336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.187544822692871 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.317514657974243 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.925668239593506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.127171277999878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.224393606185913 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3949687480926514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7865049839019775 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.451775550842285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1841437816619873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.186021327972412 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1308858394622803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6911799907684326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0771753787994385 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.394824504852295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5582289695739746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.518484592437744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8088083267211914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.895681858062744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.998687982559204 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.986786961555481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8138132095336914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9230589866638184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6370255947113037 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3430817127227783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8755173683166504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.817768096923828 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.045499324798584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6706383228302 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.51175856590271 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8639769554138184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.102179765701294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2304434776306152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.292673587799072 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.737030267715454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2705423831939697 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.175091505050659 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1288719177246094 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8180861473083496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.668515682220459 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6762328147888184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.541015863418579 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6200242042541504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3986544609069824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0745694637298584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5383245944976807 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.011484384536743 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.562319040298462 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7258076667785645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2658445835113525 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9153196811676025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4457144737243652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.307483673095703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6215168237686157 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7245798110961914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.303006410598755 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.539292335510254 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.349428653717041 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6527867317199707 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.177349328994751 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.160442590713501 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3627560138702393 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.677031993865967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.078791856765747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7180500030517578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4157955646514893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9791784286499023 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1423134803771973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9938651323318481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.322009801864624 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3615705966949463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3632781505584717 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.353644609451294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.42568039894104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6160287857055664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.95584774017334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8231956958770752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.436344861984253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0113563537597656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0478310585021973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9814515113830566 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7598652839660645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.046243667602539 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7864456176757812 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.756201982498169 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1398465633392334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1691324710845947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5424556732177734 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1179580688476562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5416016578674316 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.578901767730713 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3397390842437744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.907826542854309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.752246379852295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.900629997253418 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0484206676483154 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2524824142456055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7358245849609375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.108060359954834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.053574562072754 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.809037446975708 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.396759033203125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6743993759155273 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.443018913269043 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7178714275360107 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.097003936767578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4997775554656982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.066812753677368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2784838676452637 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1229066848754883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.782360315322876 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.217041015625 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.506253957748413 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6549172401428223 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2304630279541016 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2890472412109375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.662309408187866 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2985665798187256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.405975103378296 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2661402225494385 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.301973581314087 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3553662300109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7862417697906494 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.316617488861084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2105443477630615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5944902896881104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6167500019073486 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5476036071777344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.627467393875122 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.692821741104126 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.018725633621216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6411924362182617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.360565185546875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6696062088012695 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4907419681549072 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.561756134033203 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.986454963684082 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8927533626556396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6295663118362427 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7781074047088623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6186609268188477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.262583017349243 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.848418951034546 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.457477569580078 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4469799995422363 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.560633659362793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.493461847305298 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2726356983184814 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1151299476623535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.790008783340454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.469194173812866 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8983349800109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0322561264038086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9216485023498535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5968852043151855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.83754563331604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.839869499206543 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2556612491607666 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.976689100265503 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7246904373168945 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3762505054473877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1413862705230713 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4684083461761475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.635056257247925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.695089101791382 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.731376886367798 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.790360927581787 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8393044471740723 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4758636951446533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7769012451171875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.748302936553955 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0363564491271973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.783245086669922 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2205841541290283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.080169439315796 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.600038528442383 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7179369926452637 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0196235179901123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.955827474594116 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0513391494750977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3456785678863525 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9707884788513184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2239181995391846 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.342350721359253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5173110961914062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6414711475372314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3417367935180664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7581419944763184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.095970392227173 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.432940721511841 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.846996545791626 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.656362771987915 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.801513433456421 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3842687606811523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.52207088470459 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9600, 0.9834],\n",
      "         [0.9689, 0.9896],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "torch.Size([2700, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2309849262237549 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2950711250305176 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1838092803955078 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2731975317001343 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5795749425888062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.666306972503662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.026660680770874 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.116829752922058 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2325230836868286 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1213454008102417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.076943039894104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.153770923614502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2110724449157715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0735396146774292 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5482045412063599 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7343776226043701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2102493047714233 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.071963906288147 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6434425115585327 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6272257566452026 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2149181365966797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.783032178878784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8785187005996704 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6898200511932373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.765656590461731 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9521515369415283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9085124731063843 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1044926643371582 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4637640714645386 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6132495403289795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4789241552352905 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0785044431686401 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.024691104888916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.375118613243103 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9710991382598877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5679066181182861 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.43466317653656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.724473714828491 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9751933813095093 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5520577430725098 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5787220001220703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2622954845428467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.755782961845398 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6658767461776733 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6041370630264282 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.754699468612671 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0117578506469727 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5352057218551636 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7490806579589844 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1113213300704956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.889215350151062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.568551778793335 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.022585868835449 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0494606494903564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.696707248687744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2652459144592285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7401034832000732 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.727709174156189 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0294365882873535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.551837921142578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9193198680877686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9881092309951782 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.247692823410034 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.87766432762146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7351943254470825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0179593563079834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9984891414642334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9599661827087402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.904186964035034 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3267688751220703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.67389714717865 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.253202199935913 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.286790609359741 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1485595703125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.661716103553772 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.624361276626587 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3210082054138184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.307438611984253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.393821954727173 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.827615737915039 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.130066990852356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3331096172332764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.784684419631958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2963454723358154 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.302626848220825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.694986581802368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7580739259719849 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6189286708831787 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1426336765289307 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.22512149810791 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5306379795074463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.252901315689087 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6071412563323975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1220510005950928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.140299081802368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.116481304168701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9248392581939697 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4495954513549805 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7483091354370117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.285853862762451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.614727020263672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.127879619598389 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.091058254241943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5935680866241455 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6567631959915161 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9871599674224854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.61871075630188 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.712291717529297 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.134185552597046 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2749907970428467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.442927837371826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7189371585845947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.574577808380127 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4508109092712402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.916429042816162 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8495118618011475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3209023475646973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.1844096183776855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6507651805877686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.005069255828857 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.504822254180908 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.272993803024292 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3343420028686523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.382354259490967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7332417964935303 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.432473659515381 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3678207397460938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.608421802520752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.178816318511963 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6247804164886475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.185932636260986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.661544322967529 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.369323253631592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.0158467292785645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8820860385894775 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5196797847747803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.72172212600708 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1476309299468994 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7954567670822144 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.793773889541626 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3164050579071045 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.86056661605835 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0053603649139404 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9133951663970947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.34311056137085 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5289604663848877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.062891006469727 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9832228422164917 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.242844581604004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.157318353652954 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0347795486450195 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.09396505355835 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.506963729858398 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.954483985900879 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3581154346466064 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.155812740325928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.216677665710449 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.418456554412842 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.049587726593018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0557150840759277 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.466264724731445 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6261439323425293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7736666202545166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.209377765655518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.9806365966796875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.597549915313721 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.680464029312134 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.631136894226074 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.562463283538818 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.914478778839111 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.697429180145264 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.344892501831055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.660362720489502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.706432342529297 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.296590805053711 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.5997161865234375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.542601108551025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.254159927368164 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.621078014373779 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.216159343719482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.92536735534668 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.566519737243652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.922095775604248 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.983786106109619 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.003537178039551 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.457202434539795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.888575553894043 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.491391181945801 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.768244743347168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.99308443069458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.648205757141113 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6428728103637695 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.866603374481201 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.596574783325195 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.407346725463867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.639838218688965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.3262786865234375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.849704265594482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.781339645385742 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.948731422424316 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.437728404998779 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.5317583084106445 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.470001697540283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.102295398712158 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.208213806152344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.396842956542969 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.1296796798706055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.538236618041992 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.091653347015381 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.167872905731201 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.807194232940674 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.2515950202941895 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 9.498760223388672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.539027690887451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.50099778175354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6309494972229004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.098584175109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9209136962890625 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.761391043663025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.757007122039795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.377554893493652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.164789199829102 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.906801223754883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.700036525726318 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.678764343261719 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.091287136077881 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.173388957977295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8616530895233154 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6288504600524902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.383009910583496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.164034843444824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8259973526000977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.801673412322998 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.538633346557617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.8342204093933105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.305229663848877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.709549427032471 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.946803569793701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.995602130889893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.574620246887207 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.642133712768555 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.8532843589782715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1791868209838867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.753501892089844 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.296660900115967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.350996494293213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5111753940582275 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.0210418701171875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.09614086151123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.736504554748535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.488227367401123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.545495986938477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.440654277801514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.199103832244873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.22359561920166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.552327632904053 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.005727767944336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.917771339416504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.6442694664001465 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.778757095336914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.720926761627197 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.760716438293457 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.776191234588623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.785745620727539 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.551332473754883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.448212146759033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.399126052856445 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.737106800079346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.319139003753662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.011897563934326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7486555576324463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.906189441680908 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.10975456237793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.891479730606079 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.412258625030518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.253632068634033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.185446262359619 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.266439437866211 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9195592403411865 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.373198509216309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.840335845947266 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 9.246081352233887 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.084449768066406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.42322301864624 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.7167649269104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.857380390167236 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.684889316558838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.592999458312988 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.500860214233398 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.495929718017578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.176487684249878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.163661479949951 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.3828444480896 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.930462837219238 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3918983936309814 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.725203514099121 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.397834300994873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.481225967407227 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8755853176116943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.208805561065674 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.387300968170166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.121760845184326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.700952529907227 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.891415596008301 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.307953357696533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.420260906219482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.273263454437256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.305157661437988 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.1279215812683105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.596443176269531 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.5935282707214355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.491422176361084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.70602035522461 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.860839366912842 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.828268051147461 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.976170063018799 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.39323616027832 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.695310592651367 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.252292633056641 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.902225971221924 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.264398574829102 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.646096229553223 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.745605945587158 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.189659595489502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 9.946061134338379 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.883677959442139 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.960862159729004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.310976028442383 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.523402690887451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.459255695343018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.470667839050293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.587211608886719 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.130735397338867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.448935031890869 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 11.098639488220215 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.861426830291748 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.425339221954346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.228089809417725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.690291881561279 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.171877861022949 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.989083766937256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.207509517669678 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.190942287445068 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.413920879364014 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.297163009643555 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.448283672332764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.306084156036377 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.885621547698975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.250957012176514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0217673778533936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.35591721534729 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.141060709953308 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.494790554046631 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4002410173416138 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3050854206085205 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.242073655128479 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8161091804504395 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.05247494578361511 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.07178482413291931 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.6002e-01,  9.8342e-01],\n",
      "         [ 9.6889e-01,  9.8955e-01],\n",
      "         [-3.4147e+03,  9.1754e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]]])\n",
      "torch.Size([3600, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4473837614059448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2865245342254639 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3390322923660278 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1872363090515137 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1952146291732788 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0652501583099365 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.622771143913269 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.099994421005249 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3737345933914185 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3013731241226196 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3417079448699951 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3711512088775635 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2610878944396973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.132704257965088 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0403733253479004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2472076416015625 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4770561456680298 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3612607717514038 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1568204164505005 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5397361516952515 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.743123173713684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0718814134597778 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1217446327209473 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7075344324111938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2259392738342285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5848606824874878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2400888204574585 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0847450494766235 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1330363750457764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8984853029251099 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1715736389160156 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.133371114730835 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4255156517028809 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1563903093338013 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9359078407287598 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1589884757995605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5481549501419067 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1671663522720337 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.173221468925476 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1704187393188477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.229885458946228 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8002619743347168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8446277379989624 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1111392974853516 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1553326845169067 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.605444312095642 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.539906620979309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0279273986816406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6436419486999512 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1714131832122803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.108327031135559 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6961476802825928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6335948705673218 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2949358224868774 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6081619262695312 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5703572034835815 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.08478045463562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.295704960823059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5089889764785767 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4130507707595825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4591633081436157 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.94231116771698 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.747708797454834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2979962825775146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4892874956130981 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4859864711761475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3100289106369019 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2046977281570435 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3624049425125122 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5958997011184692 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7833678722381592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3284748792648315 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.059418797492981 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2496211528778076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3855406045913696 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.045834541320801 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0526455640792847 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3527638912200928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8751219511032104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3598792552947998 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.114367961883545 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.316432237625122 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0501558780670166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.473688006401062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1967048645019531 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8670876026153564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4344325065612793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4423798322677612 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5715560913085938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9814915657043457 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1193519830703735 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6838321685791016 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3583133220672607 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1681078672409058 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8253183364868164 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2160208225250244 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0316017866134644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4349652528762817 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6813836097717285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7093113660812378 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1676528453826904 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4712119102478027 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.251128911972046 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.423264741897583 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3992559909820557 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5790276527404785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.030466318130493 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5262985229492188 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0258665084838867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1420862674713135 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6183102130889893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6266762018203735 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.468433141708374 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.619355320930481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.462383270263672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4255238771438599 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1853926181793213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.580559492111206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4778335094451904 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8020364046096802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.933844804763794 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7589561939239502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5184948444366455 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7374916076660156 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0944862365722656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4035109281539917 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.184004306793213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.051765203475952 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.235465168952942 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3979605436325073 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2610087394714355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2752459049224854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1167893409729004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6966238021850586 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5018086433410645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.408137559890747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.04783034324646 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8658738136291504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0267187356948853 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.369513988494873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.497981548309326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4040164947509766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8837589025497437 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.293280839920044 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1148300170898438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7530570030212402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4137489795684814 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.751044511795044 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.464849591255188 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6426918506622314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.095278024673462 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9490959644317627 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.959629774093628 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5148508548736572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6248879432678223 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8942296504974365 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7519357204437256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.245861053466797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3817520141601562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.876579523086548 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.213315963745117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.300501823425293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3837822675704956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.096118450164795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.620988607406616 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2911187410354614 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.034003734588623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.948505163192749 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9580636024475098 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.341200351715088 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.002416610717773 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.7593183517456055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.082108497619629 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.719164252281189 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.439410448074341 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3660597801208496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9509401321411133 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4994633197784424 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.967935800552368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6868059635162354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.0781168937683105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.593642711639404 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.772334337234497 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9111911058425903 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.707195997238159 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.255224227905273 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.126585841178894 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.955301523208618 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3915233612060547 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.128305435180664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.997175455093384 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.591473340988159 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1942700147628784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.39008548855781555 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.19255974888801575 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.026656869798898697 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.18597199022769928 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.6002e-01,  9.8342e-01],\n",
      "         [ 9.6889e-01,  9.8955e-01],\n",
      "         [-3.4147e+03,  9.1754e-01],\n",
      "         [-2.4586e+00,  9.4299e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]]])\n",
      "torch.Size([4500, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0556652545928955 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6479984521865845 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2626482248306274 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0442605018615723 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0473194122314453 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2080250978469849 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6533793210983276 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.303242802619934 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6209629774093628 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6045883893966675 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.287673830986023 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3102854490280151 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7090884447097778 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1183241605758667 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.138137698173523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6477482318878174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9603095054626465 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3433842658996582 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6098716259002686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3801831007003784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.643303632736206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5762584209442139 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5187995433807373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5667179822921753 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3652621507644653 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.424780011177063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6995060443878174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4380794763565063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.419169306755066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5265135765075684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.658369541168213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4188203811645508 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3760501146316528 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.012994408607483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3309948444366455 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.132585048675537 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6179360151290894 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1454081535339355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "\n",
    "R2_test = torch.zeros(len(meshes),reps,2)\n",
    "R2_leftout= torch.zeros(len(meshes),reps,2)\n",
    "for i in range(len(meshes)):\n",
    "    Xs=[]\n",
    "    Ys=[]\n",
    "    for j in range(reps):\n",
    "        for k in range(len(meshes)):\n",
    "            if k!=i:\n",
    "                X=train_input_modes[k][:,0:15]\n",
    "                y=train_output_modes[k]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X,\n",
    "                    y,\n",
    "                    train_size=50,\n",
    "                    random_state=j+k\n",
    "                )\n",
    "                Xs.append(X_train)\n",
    "                Ys.append(y_train)\n",
    "\n",
    "        X_train=torch.cat(Xs,)\n",
    "        y_train=torch.cat(Ys,)\n",
    "        print(X_train.shape)\n",
    "        X_test= torch.cat(test_input_modes[0:i]+test_input_modes[i+1:])[:,0:15]\n",
    "        y_test=torch.cat(test_output_modes[:i]+test_output_modes[i+1:])\n",
    "        emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=2000)\n",
    "        \n",
    "        meanR, stdR = emulator.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        R2_test[i,j,:]=meanR\n",
    "        \n",
    "        meanR, stdR=emulator.R2_sample(test_input_modes[i][:,0:15],test_output_modes[i],1000) \n",
    "        R2_leftout[i,j,:] = meanR\n",
    "        print(R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[:,0].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[:,1].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meshes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d99a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[:,0].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[:,1].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.legend(['A_TAT','V_TAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array(R2_leftout),axis=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fb7a5",
   "metadata": {},
   "source": [
    "# Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafeff3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " value = ('likelihood.noise_covar.raw_noise', Parameter containing:\n",
      "tensor([-13.0669], requires_grad=True))\n",
      " value = ('covar_module.raw_outputscale', Parameter containing:\n",
      "tensor(0.4679, requires_grad=True))\n",
      " value = ('covar_module.base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[32.8634, 24.1502, 26.6298,  1.3238,  6.8045,  1.6815]],\n",
      "       requires_grad=True))\n",
      " value = ('mean_module.weights', Parameter containing:\n",
      "tensor([[ 7.6845e-03],\n",
      "        [-5.6172e-03],\n",
      "        [ 1.5326e-03],\n",
      "        [-1.8826e+00],\n",
      "        [-2.5208e-01],\n",
      "        [-6.6706e-01]], requires_grad=True))\n",
      " value = ('mean_module.bias', Parameter containing:\n",
      "tensor([2.4689], requires_grad=True))\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:208: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3618.)\n",
      "  prediction=torch.stack(prediction).T\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  MSE_mean = torch.tensor(MSE_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:225: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  MSE_std = torch.tensor(MSE_score.std(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     40\u001b[0m         m0 \u001b[38;5;241m=\u001b[39m emulators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(X_train[a,:])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#         y_adjust = torch.tensor(y_train[a] - m0)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         delta_1 = GPE.ensemble(X_train[a,:],y_adjust,mean_func=\"linear\",training_iter=500)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#         MSE[i,j] += ((emulator_0.predict(X_test)+delta_1.predict(X_test)-torch.tensor(y_test))**2).mean(axis=0).detach().numpy()\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         R2[i,j] += (1-((emulator_0.predict(X_test)+delta_1.predict(X_test)-y_test)**2).mean(axis=0)/y_test.var(axis=0)).detach().numpy()\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m         delta_1\u001b[38;5;241m=\u001b[39mGPE\u001b[38;5;241m.\u001b[39mensemble(X_train[a,:],y_train[a],mean_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrepancy_cohort\u001b[39m\u001b[38;5;124m\"\u001b[39m,training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,ref_emulator\u001b[38;5;241m=\u001b[39memulators[\u001b[38;5;241m1\u001b[39m:],a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m         MSEaM,MSEaSTD \u001b[38;5;241m=\u001b[39m delta_1\u001b[38;5;241m.\u001b[39mMSE_sample(X_test,y_test,\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     49\u001b[0m         R2aM,R2aSTD \u001b[38;5;241m=\u001b[39m delta_1\u001b[38;5;241m.\u001b[39mR2_sample(X_test,y_test,\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:22\u001b[0m, in \u001b[0;36mensemble.__init__\u001b[0;34m(self, X_train, y_train, mean_func, training_iter, kernel, kernel_params, ref_emulator, a, a_indicator)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m=\u001b[39mkernel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_params\u001b[38;5;241m=\u001b[39mkernel_params\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_ensemble()\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:105\u001b[0m, in \u001b[0;36mensemble.create_ensemble\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Output from model\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m models[i](X)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, Y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:268\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    265\u001b[0m             torch\u001b[38;5;241m.\u001b[39mequal(train_input, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m train_input, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m length_safe_zip(train_inputs, inputs)\n\u001b[1;32m    266\u001b[0m         ):\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must train on the training inputs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Prior mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GP_functions.py:70\u001b[0m, in \u001b[0;36mExactGPModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 70\u001b[0m     mean_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_module(x)\n\u001b[1;32m     71\u001b[0m     covar_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module(x)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpytorch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal(mean_x, covar_x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/means/mean.py:22\u001b[0m, in \u001b[0;36mMean.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Mean, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GP_functions.py:149\u001b[0m, in \u001b[0;36mDiscrepancyMeanCohort.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model[i]\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_likelihood[i]\u001b[38;5;241m.\u001b[39meval() \n\u001b[0;32m--> 149\u001b[0m     res2\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_likelihood[i](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model[i](x))\u001b[38;5;241m.\u001b[39mmean\n\u001b[1;32m    151\u001b[0m res\u001b[38;5;241m=\u001b[39mres1\u001b[38;5;241m+\u001b[39mres2\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[0;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_strategy\u001b[38;5;241m.\u001b[39mexact_prediction(full_mean, full_covar)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:290\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    285\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    286\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_mean(test_mean, test_train_covar),\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_covar(test_test_covar, test_train_covar),\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:342\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    340\u001b[0m test_train_covar \u001b[38;5;241m=\u001b[39m to_dense(test_train_covar)\n\u001b[1;32m    341\u001b[0m train_test_covar \u001b[38;5;241m=\u001b[39m test_train_covar\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m covar_correction_rhs \u001b[38;5;241m=\u001b[39m train_train_covar\u001b[38;5;241m.\u001b[39msolve(train_test_covar)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# For efficiency\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(test_test_covar):\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# We can use addmm in the 2d case\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2334\u001b[0m, in \u001b[0;36mLinearOperator.solve\u001b[0;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m   2332\u001b[0m func \u001b[38;5;241m=\u001b[39m Solve\n\u001b[1;32m   2333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(), \u001b[38;5;28;01mFalse\u001b[39;00m, right_tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(),\n\u001b[1;32m   2338\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2341\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation(),\n\u001b[1;32m   2342\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/functions/_solve.py:53\u001b[0m, in \u001b[0;36mSolve.forward\u001b[0;34m(ctx, representation_tree, has_left, *args)\u001b[0m\n\u001b[1;32m     51\u001b[0m     res \u001b[38;5;241m=\u001b[39m left_tensor \u001b[38;5;241m@\u001b[39m res\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     solves \u001b[38;5;241m=\u001b[39m _solve(linear_op, right_tensor)\n\u001b[1;32m     54\u001b[0m     res \u001b[38;5;241m=\u001b[39m solves\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mis_vector:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/functions/_solve.py:17\u001b[0m, in \u001b[0;36m_solve\u001b[0;34m(linear_op, rhs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39msolve(rhs)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfast_computations\u001b[38;5;241m.\u001b[39msolves\u001b[38;5;241m.\u001b[39moff() \u001b[38;5;129;01mor\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_cholesky_size\u001b[38;5;241m.\u001b[39mvalue():\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39mcholesky()\u001b[38;5;241m.\u001b[39m_cholesky_solve(rhs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:1303\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cholesky(upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1305\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:522\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(evaluated_mat\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m cholesky \u001b[38;5;241m=\u001b[39m psd_safe_cholesky(evaluated_mat, upper\u001b[38;5;241m=\u001b[39mupper)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(cholesky, upper\u001b[38;5;241m=\u001b[39mupper)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:65\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     L \u001b[38;5;241m=\u001b[39m _psd_safe_cholesky(A, out\u001b[38;5;241m=\u001b[39mout, jitter\u001b[38;5;241m=\u001b[39mjitter, max_tries\u001b[38;5;241m=\u001b[39mmax_tries)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:20\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m (out, torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32, device\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m---> 20\u001b[0m L, info \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky_ex(A, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mtrace_mode\u001b[38;5;241m.\u001b[39mon() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m L\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# x_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/input/xlabels_EP.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "# y_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/output/ylabels.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "# y_labels\n",
    "\n",
    "# inputData_0 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/01/X_EP.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "# outputData_0 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/01/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "\n",
    "# inputData_1 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/02/X_EP.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "# outputData_1 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/02/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "\n",
    "X0 = train_input[0]\n",
    "Y0 = train_output[0]\n",
    "X1 = train_input[1]\n",
    "Y1 = train_output[1]\n",
    "\n",
    "\n",
    "for param in emulators[0].models[0].named_parameters():\n",
    "    print(f' value = {param}')\n",
    "\n",
    "# split original dataset in training, validation and testing sets\n",
    "X_train=X0\n",
    "y_train=Y0\n",
    "X_test=test_input[0]\n",
    "y_test=test_output[0]\n",
    "\n",
    "p = int(X1.shape[0]*0.05)\n",
    "n = int(X_train.shape[0]/p)\n",
    "reps = 5\n",
    "MSE = torch.zeros((n,reps,2))\n",
    "R2 = torch.zeros((n,reps,2))\n",
    "MSE_p = torch.zeros((n,reps,2))\n",
    "R2_p = torch.zeros((n,reps,2))\n",
    "MSEa = torch.zeros((n,reps,2))\n",
    "R2a = torch.zeros((n,reps,2))\n",
    "for i in range(n):\n",
    "    for j in range(reps):\n",
    "        a=np.random.choice(range(X_train.shape[0]),(i+1)*p,replace=False)\n",
    "        m0 = emulators[0].predict(X_train[a,:])\n",
    "#         y_adjust = torch.tensor(y_train[a] - m0)\n",
    "#         delta_1 = GPE.ensemble(X_train[a,:],y_adjust,mean_func=\"linear\",training_iter=500)\n",
    "#         MSE[i,j] += ((emulator_0.predict(X_test)+delta_1.predict(X_test)-torch.tensor(y_test))**2).mean(axis=0).detach().numpy()\n",
    "#         R2[i,j] += (1-((emulator_0.predict(X_test)+delta_1.predict(X_test)-y_test)**2).mean(axis=0)/y_test.var(axis=0)).detach().numpy()\n",
    "        \n",
    "        \n",
    "        delta_1=GPE.ensemble(X_train[a,:],y_train[a],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators[1:],a=None)\n",
    "        MSEaM,MSEaSTD = delta_1.MSE_sample(X_test,y_test,1000)\n",
    "        R2aM,R2aSTD = delta_1.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        MSE[i,j] += MSEaM\n",
    "        R2[i,j] += R2aM\n",
    "        \n",
    "        \n",
    "        delta_1=GPE.ensemble(X_train[a,:],y_train[a],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators[1]],a=None)\n",
    "        MSEaM,MSEaSTD = delta_1.MSE_sample(X_test,y_test,1000)\n",
    "        R2aM,R2aSTD = delta_1.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        MSEa[i,j] += MSEaM\n",
    "        R2a[i,j] += R2aM\n",
    "        \n",
    "        delta_1p = GPE.ensemble(X_train[a,:],y_train[a],mean_func=\"linear\",training_iter=500)\n",
    "        MSEaM,MSEaSTD = delta_1p.MSE_sample(X_test,y_test,1000)\n",
    "        R2aM,R2aSTD = delta_1p.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        MSE_p[i,j] += MSEaM\n",
    "        R2_p[i,j] += R2aM\n",
    "\n",
    "x = np.linspace(9,162,18)\n",
    "\n",
    "x\n",
    "\n",
    "MSE.mean(axis=1)[0]\n",
    "\n",
    "MSE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed33931",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(7,144,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afc7fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$m$')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,MSE.mean(axis=1))\n",
    "plt.fill_between(x, MSE.mean(axis=1)[:,0]+MSE.std(axis=1)[:,0], y2=MSE.mean(axis=1)[:,0]-MSE.std(axis=1)[:,0],alpha=0.4)\n",
    "plt.fill_between(x, MSE.mean(axis=1)[:,1]+MSE.std(axis=1)[:,1], y2=MSE.mean(axis=1)[:,1]-MSE.std(axis=1)[:,1],alpha=0.4)\n",
    "plt.legend(y_labels.values)\n",
    "\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "#plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea26be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0053e+03, 4.4139e+02],\n",
       "         [3.5017e+03, 9.0324e+02],\n",
       "         [1.2513e+03, 1.5707e+02],\n",
       "         [1.2043e+05, 1.0591e+04],\n",
       "         [1.7407e+03, 9.0006e+01]],\n",
       "\n",
       "        [[2.0532e+02, 1.6733e+02],\n",
       "         [5.4058e+02, 7.5744e+01],\n",
       "         [7.2628e+02, 8.4713e+01],\n",
       "         [8.1053e+02, 1.7521e+02],\n",
       "         [1.7129e+03, 7.9795e+01]],\n",
       "\n",
       "        [[7.4946e+02, 1.7684e+02],\n",
       "         [1.1037e+02, 4.9126e+01],\n",
       "         [1.5816e+03, 7.4548e+01],\n",
       "         [4.3494e+01, 9.8884e+01],\n",
       "         [6.6827e+01, 1.4606e+02]],\n",
       "\n",
       "        [[8.6498e+01, 7.5162e+01],\n",
       "         [8.9355e+01, 7.4566e+01],\n",
       "         [5.1005e+01, 2.8990e+01],\n",
       "         [1.2719e+02, 3.4361e+01],\n",
       "         [8.9162e+01, 3.0088e+01]],\n",
       "\n",
       "        [[1.2944e+02, 1.7104e+01],\n",
       "         [5.1697e+01, 1.3738e+01],\n",
       "         [3.8570e+01, 3.5743e+01],\n",
       "         [3.3745e+01, 1.3179e+01],\n",
       "         [7.2499e+01, 2.7964e+01]],\n",
       "\n",
       "        [[2.1092e+01, 1.8025e+01],\n",
       "         [3.5971e+01, 1.0903e+01],\n",
       "         [3.7181e+01, 1.3848e+01],\n",
       "         [1.4264e+01, 1.1445e+01],\n",
       "         [2.9713e+01, 1.0910e+01]],\n",
       "\n",
       "        [[2.7014e+01, 2.3552e+01],\n",
       "         [1.6403e+01, 9.3167e+00],\n",
       "         [1.6788e+01, 1.9167e+01],\n",
       "         [2.3071e+01, 1.2372e+01],\n",
       "         [2.5421e+01, 1.4559e+01]],\n",
       "\n",
       "        [[2.6143e+01, 5.5196e+00],\n",
       "         [1.8508e+01, 1.2842e+01],\n",
       "         [1.2904e+01, 5.3217e+00],\n",
       "         [3.1449e+01, 5.6140e+00],\n",
       "         [1.4481e+01, 2.4161e+01]],\n",
       "\n",
       "        [[1.2947e+01, 4.7685e+00],\n",
       "         [3.1480e+01, 3.4169e+00],\n",
       "         [1.8848e+01, 4.5880e+00],\n",
       "         [1.1944e+01, 8.9958e+00],\n",
       "         [2.7563e+01, 5.4206e+00]],\n",
       "\n",
       "        [[7.1000e+00, 4.4458e+00],\n",
       "         [1.6906e+01, 4.9741e+00],\n",
       "         [8.7950e+00, 6.8949e+00],\n",
       "         [1.9972e+01, 1.0886e+01],\n",
       "         [9.1423e+00, 3.6059e+00]],\n",
       "\n",
       "        [[1.3355e+01, 3.0664e+00],\n",
       "         [9.4912e+00, 3.7495e+00],\n",
       "         [9.7016e+00, 2.8478e+00],\n",
       "         [1.5395e+01, 4.4468e+00],\n",
       "         [2.3764e+01, 3.3102e+00]],\n",
       "\n",
       "        [[1.0877e+01, 3.3919e+00],\n",
       "         [6.7196e+00, 4.5628e+00],\n",
       "         [8.2384e+00, 2.3935e+00],\n",
       "         [1.0873e+01, 6.3427e+00],\n",
       "         [7.3827e+00, 2.6047e+00]],\n",
       "\n",
       "        [[1.2919e+01, 2.7290e+00],\n",
       "         [7.4372e+00, 3.6573e+00],\n",
       "         [6.3350e+00, 2.9353e+00],\n",
       "         [5.5454e+00, 3.9927e+00],\n",
       "         [9.6886e+00, 4.3525e+00]],\n",
       "\n",
       "        [[1.3912e+01, 3.7539e+00],\n",
       "         [1.3012e+01, 2.8719e+00],\n",
       "         [7.2153e+00, 2.5166e+00],\n",
       "         [9.2395e+00, 2.5724e+00],\n",
       "         [1.9121e+01, 3.2068e+00]],\n",
       "\n",
       "        [[7.3895e+00, 2.9830e+00],\n",
       "         [5.5064e+00, 3.5055e+00],\n",
       "         [6.0174e+00, 2.3664e+00],\n",
       "         [7.3022e+00, 2.6336e+00],\n",
       "         [6.7025e+00, 2.3824e+00]],\n",
       "\n",
       "        [[7.3279e+00, 2.4586e+00],\n",
       "         [7.5969e+00, 1.9485e+00],\n",
       "         [7.8020e+00, 2.9870e+00],\n",
       "         [9.5943e+00, 2.3441e+00],\n",
       "         [4.9980e+00, 2.3096e+00]],\n",
       "\n",
       "        [[5.9309e+00, 2.1835e+00],\n",
       "         [4.1038e+00, 2.6204e+00],\n",
       "         [6.6806e+00, 3.0402e+00],\n",
       "         [6.9768e+00, 2.1905e+00],\n",
       "         [4.1995e+00, 2.1789e+00]],\n",
       "\n",
       "        [[4.6404e+00, 2.1350e+00],\n",
       "         [6.4411e+00, 2.3611e+00],\n",
       "         [4.5094e+00, 2.1972e+00],\n",
       "         [6.5910e+00, 1.6137e+00],\n",
       "         [5.6537e+00, 1.8290e+00]],\n",
       "\n",
       "        [[4.8037e+00, 1.8246e+00],\n",
       "         [5.4191e+00, 2.0841e+00],\n",
       "         [4.7112e+00, 3.1480e+00],\n",
       "         [4.8532e+00, 2.3028e+00],\n",
       "         [5.5880e+00, 1.9339e+00]],\n",
       "\n",
       "        [[5.0640e+00, 1.7976e+00],\n",
       "         [4.6904e+00, 1.9077e+00],\n",
       "         [5.0408e+00, 2.0327e+00],\n",
       "         [4.7101e+00, 1.7274e+00],\n",
       "         [6.1555e+00, 1.9489e+00]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MSE.mean(axis=1)[:,0]+MSE.std(axis=1)[:,0]\n",
    "\n",
    "MSE.mean(axis=1)[:,0]-MSE.std(axis=1)[:,0]\n",
    "\n",
    "delta_1p.MSE(X_test,y_test)/reps\n",
    "\n",
    "plt.plot(x,MSE.mean(axis=1),'o') \n",
    "plt.legend(y_labels.values)\n",
    "\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "plt.yscale('log')\n",
    "\n",
    "np.hstack((MSE_p,MSE))\n",
    "\n",
    "MSE_p.mean(axis=1)[:,0]+MSE_p.std(axis=1)[:,0]\n",
    "\n",
    "MSE_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e880643a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.3029e+04, 4.5698e+03],\n",
       "        [5.6121e+02, 5.0124e+01],\n",
       "        [6.6699e+02, 5.2082e+01],\n",
       "        [2.6961e+01, 2.4030e+01],\n",
       "        [3.8924e+01, 9.9179e+00],\n",
       "        [9.8304e+00, 3.0475e+00],\n",
       "        [4.9026e+00, 5.6294e+00],\n",
       "        [7.8964e+00, 8.1764e+00],\n",
       "        [8.7092e+00, 2.1163e+00],\n",
       "        [5.6862e+00, 2.9046e+00],\n",
       "        [5.8292e+00, 6.3382e-01],\n",
       "        [1.9533e+00, 1.6281e+00],\n",
       "        [2.9756e+00, 6.8957e-01],\n",
       "        [4.6004e+00, 5.1059e-01],\n",
       "        [8.1565e-01, 4.7906e-01],\n",
       "        [1.6409e+00, 3.7506e-01],\n",
       "        [1.3574e+00, 3.8370e-01],\n",
       "        [9.7432e-01, 3.0096e-01],\n",
       "        [3.9897e-01, 5.2850e-01],\n",
       "        [5.9862e-01, 1.2132e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_p.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c80519db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 237.9059, -156.6279,   61.6800,   26.2652,   17.8138,   16.8370,\n",
       "          12.8005,   11.8472,    6.6968,    8.5121,    6.8649,    5.4093,\n",
       "           7.8996,    5.7679,    5.8229,    4.2209,    4.5928,    4.6761,\n",
       "           4.5335])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_p.mean(axis=1)[1:,0]-MSE_p.std(axis=1)[1:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a8961da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x[1:],np.hstack((MSE.mean(axis=1),MSE_p.mean(axis=1),MSEa.mean(axis=1)))[1:,[0,2,4]],'o') \n",
    "plt.fill_between(x[1:], MSE.mean(axis=1)[1:,0]+MSE.std(axis=1)[1:,0], y2=MSE.mean(axis=1)[1:,0]-MSE.std(axis=1)[1:,0],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSE_p.mean(axis=1)[1:,0]+MSE_p.std(axis=1)[1:,0], y2=MSE_p.mean(axis=1)[1:,0]-MSE_p.std(axis=1)[1:,0],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSEa.mean(axis=1)[1:,0]+MSEa.std(axis=1)[1:,0], y2=MSEa.mean(axis=1)[1:,0]-MSEa.std(axis=1)[1:,0],alpha=0.4)\n",
    "plt.legend(['$\\delta_a$','$f_1$','$\\delta_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "plt.yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d559838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.7694,   0.3104],\n",
       "         [  0.1963,  -0.4105],\n",
       "         [  0.7127,   0.7558],\n",
       "         [-26.6294, -15.5394],\n",
       "         [  0.6007,   0.8595]],\n",
       "\n",
       "        [[  0.9535,   0.7383],\n",
       "         [  0.8762,   0.8811],\n",
       "         [  0.8316,   0.8678],\n",
       "         [  0.8151,   0.7263],\n",
       "         [  0.6067,   0.8759]],\n",
       "\n",
       "        [[  0.8289,   0.7240],\n",
       "         [  0.9748,   0.9236],\n",
       "         [  0.6373,   0.8837],\n",
       "         [  0.9901,   0.8446],\n",
       "         [  0.9848,   0.7714]],\n",
       "\n",
       "        [[  0.9801,   0.8822],\n",
       "         [  0.9796,   0.8842],\n",
       "         [  0.9883,   0.9546],\n",
       "         [  0.9707,   0.9463],\n",
       "         [  0.9796,   0.9531]],\n",
       "\n",
       "        [[  0.9703,   0.9735],\n",
       "         [  0.9881,   0.9782],\n",
       "         [  0.9908,   0.9448],\n",
       "         [  0.9922,   0.9793],\n",
       "         [  0.9829,   0.9563]],\n",
       "\n",
       "        [[  0.9952,   0.9719],\n",
       "         [  0.9916,   0.9827],\n",
       "         [  0.9916,   0.9781],\n",
       "         [  0.9968,   0.9821],\n",
       "         [  0.9932,   0.9830]],\n",
       "\n",
       "        [[  0.9938,   0.9632],\n",
       "         [  0.9962,   0.9854],\n",
       "         [  0.9961,   0.9700],\n",
       "         [  0.9948,   0.9808],\n",
       "         [  0.9942,   0.9772]],\n",
       "\n",
       "        [[  0.9939,   0.9913],\n",
       "         [  0.9957,   0.9801],\n",
       "         [  0.9970,   0.9916],\n",
       "         [  0.9928,   0.9913],\n",
       "         [  0.9967,   0.9621]],\n",
       "\n",
       "        [[  0.9971,   0.9925],\n",
       "         [  0.9928,   0.9947],\n",
       "         [  0.9957,   0.9927],\n",
       "         [  0.9972,   0.9860],\n",
       "         [  0.9937,   0.9916]],\n",
       "\n",
       "        [[  0.9983,   0.9930],\n",
       "         [  0.9961,   0.9923],\n",
       "         [  0.9980,   0.9894],\n",
       "         [  0.9956,   0.9827],\n",
       "         [  0.9979,   0.9944]],\n",
       "\n",
       "        [[  0.9969,   0.9951],\n",
       "         [  0.9978,   0.9941],\n",
       "         [  0.9978,   0.9956],\n",
       "         [  0.9964,   0.9930],\n",
       "         [  0.9945,   0.9949]],\n",
       "\n",
       "        [[  0.9975,   0.9946],\n",
       "         [  0.9985,   0.9929],\n",
       "         [  0.9981,   0.9963],\n",
       "         [  0.9975,   0.9898],\n",
       "         [  0.9983,   0.9959]],\n",
       "\n",
       "        [[  0.9971,   0.9957],\n",
       "         [  0.9983,   0.9943],\n",
       "         [  0.9985,   0.9953],\n",
       "         [  0.9987,   0.9938],\n",
       "         [  0.9978,   0.9932]],\n",
       "\n",
       "        [[  0.9969,   0.9942],\n",
       "         [  0.9971,   0.9956],\n",
       "         [  0.9983,   0.9960],\n",
       "         [  0.9979,   0.9960],\n",
       "         [  0.9956,   0.9950]],\n",
       "\n",
       "        [[  0.9983,   0.9953],\n",
       "         [  0.9987,   0.9944],\n",
       "         [  0.9986,   0.9962],\n",
       "         [  0.9983,   0.9958],\n",
       "         [  0.9985,   0.9964]],\n",
       "\n",
       "        [[  0.9982,   0.9962],\n",
       "         [  0.9982,   0.9969],\n",
       "         [  0.9982,   0.9954],\n",
       "         [  0.9978,   0.9963],\n",
       "         [  0.9989,   0.9964]],\n",
       "\n",
       "        [[  0.9986,   0.9965],\n",
       "         [  0.9991,   0.9960],\n",
       "         [  0.9985,   0.9953],\n",
       "         [  0.9984,   0.9966],\n",
       "         [  0.9990,   0.9966]],\n",
       "\n",
       "        [[  0.9989,   0.9967],\n",
       "         [  0.9985,   0.9963],\n",
       "         [  0.9989,   0.9965],\n",
       "         [  0.9985,   0.9974],\n",
       "         [  0.9987,   0.9971]],\n",
       "\n",
       "        [[  0.9989,   0.9972],\n",
       "         [  0.9988,   0.9968],\n",
       "         [  0.9989,   0.9952],\n",
       "         [  0.9989,   0.9965],\n",
       "         [  0.9987,   0.9970]],\n",
       "\n",
       "        [[  0.9988,   0.9972],\n",
       "         [  0.9989,   0.9970],\n",
       "         [  0.9989,   0.9968],\n",
       "         [  0.9989,   0.9973],\n",
       "         [  0.9986,   0.9970]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cddf2fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4280, -1.6653],\n",
       "         [-0.7949,  0.6093],\n",
       "         [-0.1380,  0.7408],\n",
       "         [-1.1618,  0.0952],\n",
       "         [ 0.3337,  0.8395]],\n",
       "\n",
       "        [[ 0.9749,  0.9596],\n",
       "         [ 0.9832,  0.8988],\n",
       "         [ 0.9810,  0.8144],\n",
       "         [ 0.9646,  0.8687],\n",
       "         [ 0.9875,  0.9861]],\n",
       "\n",
       "        [[ 0.9933,  0.8202],\n",
       "         [ 0.9923,  0.9873],\n",
       "         [ 0.9960,  0.9785],\n",
       "         [ 0.9487,  0.9942],\n",
       "         [ 0.9690,  0.9152]],\n",
       "\n",
       "        [[ 0.9964,  0.9930],\n",
       "         [ 0.9907,  0.9944],\n",
       "         [ 0.9851,  0.9883],\n",
       "         [ 0.9715,  0.9693],\n",
       "         [ 0.9898,  0.9788]],\n",
       "\n",
       "        [[ 0.9909,  0.9945],\n",
       "         [ 0.9961,  0.9954],\n",
       "         [ 0.9930,  0.9961],\n",
       "         [ 0.9962,  0.9943],\n",
       "         [ 0.9948,  0.9934]],\n",
       "\n",
       "        [[ 0.9958,  0.9891],\n",
       "         [ 0.9966,  0.9960],\n",
       "         [ 0.9975,  0.9949],\n",
       "         [ 0.9989,  0.9959],\n",
       "         [ 0.9940,  0.9947]],\n",
       "\n",
       "        [[ 0.9948,  0.9943],\n",
       "         [ 0.9982,  0.9897],\n",
       "         [ 0.9977,  0.9972],\n",
       "         [ 0.9965,  0.9952],\n",
       "         [ 0.9976,  0.9974]],\n",
       "\n",
       "        [[ 0.9956,  0.9946],\n",
       "         [ 0.9967,  0.9963],\n",
       "         [ 0.9984,  0.9961],\n",
       "         [ 0.9966,  0.9966],\n",
       "         [ 0.9974,  0.9933]],\n",
       "\n",
       "        [[ 0.9979,  0.9963],\n",
       "         [ 0.9958,  0.9981],\n",
       "         [ 0.9964,  0.9954],\n",
       "         [ 0.9977,  0.9976],\n",
       "         [ 0.9972,  0.9969]],\n",
       "\n",
       "        [[ 0.9973,  0.9955],\n",
       "         [ 0.9973,  0.9965],\n",
       "         [ 0.9984,  0.9969],\n",
       "         [ 0.9966,  0.9955],\n",
       "         [ 0.9977,  0.9954]],\n",
       "\n",
       "        [[ 0.9973,  0.9967],\n",
       "         [ 0.9978,  0.9957],\n",
       "         [ 0.9977,  0.9956],\n",
       "         [ 0.9978,  0.9960],\n",
       "         [ 0.9970,  0.9964]],\n",
       "\n",
       "        [[ 0.9977,  0.9977],\n",
       "         [ 0.9984,  0.9978],\n",
       "         [ 0.9982,  0.9969],\n",
       "         [ 0.9980,  0.9975],\n",
       "         [ 0.9979,  0.9962]],\n",
       "\n",
       "        [[ 0.9984,  0.9980],\n",
       "         [ 0.9984,  0.9968],\n",
       "         [ 0.9981,  0.9975],\n",
       "         [ 0.9982,  0.9962],\n",
       "         [ 0.9976,  0.9953]],\n",
       "\n",
       "        [[ 0.9977,  0.9969],\n",
       "         [ 0.9976,  0.9974],\n",
       "         [ 0.9983,  0.9979],\n",
       "         [ 0.9982,  0.9980],\n",
       "         [ 0.9979,  0.9977]],\n",
       "\n",
       "        [[ 0.9981,  0.9962],\n",
       "         [ 0.9981,  0.9977],\n",
       "         [ 0.9983,  0.9978],\n",
       "         [ 0.9984,  0.9974],\n",
       "         [ 0.9982,  0.9980]],\n",
       "\n",
       "        [[ 0.9981,  0.9980],\n",
       "         [ 0.9980,  0.9978],\n",
       "         [ 0.9983,  0.9976],\n",
       "         [ 0.9982,  0.9980],\n",
       "         [ 0.9982,  0.9982]],\n",
       "\n",
       "        [[ 0.9980,  0.9971],\n",
       "         [ 0.9988,  0.9980],\n",
       "         [ 0.9979,  0.9971],\n",
       "         [ 0.9981,  0.9977],\n",
       "         [ 0.9985,  0.9982]],\n",
       "\n",
       "        [[ 0.9984,  0.9981],\n",
       "         [ 0.9983,  0.9983],\n",
       "         [ 0.9984,  0.9980],\n",
       "         [ 0.9984,  0.9980],\n",
       "         [ 0.9984,  0.9983]],\n",
       "\n",
       "        [[ 0.9984,  0.9981],\n",
       "         [ 0.9981,  0.9980],\n",
       "         [ 0.9984,  0.9972],\n",
       "         [ 0.9983,  0.9980],\n",
       "         [ 0.9983,  0.9981]],\n",
       "\n",
       "        [[ 0.9984,  0.9981],\n",
       "         [ 0.9985,  0.9983],\n",
       "         [ 0.9982,  0.9982],\n",
       "         [ 0.9984,  0.9983],\n",
       "         [ 0.9984,  0.9981]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ff6e831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.7299, -13.4359],\n",
       "         [ -0.4653,   0.3035],\n",
       "         [ -0.9375,  -1.4426],\n",
       "         [  0.7532,  -1.3478],\n",
       "         [  0.7942,   0.5478]],\n",
       "\n",
       "        [[  0.9748,   0.8843],\n",
       "         [  0.9621,   0.5900],\n",
       "         [  0.8393,   0.9504],\n",
       "         [  0.9729,   0.9679],\n",
       "         [  0.9557,   0.9612]],\n",
       "\n",
       "        [[  0.9910,   0.9662],\n",
       "         [  0.9803,   0.9930],\n",
       "         [  0.9909,   0.9862],\n",
       "         [  0.9709,   0.9916],\n",
       "         [  0.9886,   0.9889]],\n",
       "\n",
       "        [[  0.9972,   0.9931],\n",
       "         [  0.9828,   0.9924],\n",
       "         [  0.9957,   0.9945],\n",
       "         [  0.9942,   0.9855],\n",
       "         [  0.9826,   0.9708]],\n",
       "\n",
       "        [[  0.9940,   0.9897],\n",
       "         [  0.9971,   0.9981],\n",
       "         [  0.9752,   0.9925],\n",
       "         [  0.9982,   0.9943],\n",
       "         [  0.9899,   0.9912]],\n",
       "\n",
       "        [[  0.9951,   0.9945],\n",
       "         [  0.9914,   0.9988],\n",
       "         [  0.9973,   0.9972],\n",
       "         [  0.9956,   0.9978],\n",
       "         [  0.9873,   0.9955]],\n",
       "\n",
       "        [[  0.9929,   0.9978],\n",
       "         [  0.9978,   0.9916],\n",
       "         [  0.9978,   0.9979],\n",
       "         [  0.9987,   0.9948],\n",
       "         [  0.9976,   0.9975]],\n",
       "\n",
       "        [[  0.9954,   0.9965],\n",
       "         [  0.9987,   0.9984],\n",
       "         [  0.9984,   0.9970],\n",
       "         [  0.9986,   0.9968],\n",
       "         [  0.9980,   0.9981]],\n",
       "\n",
       "        [[  0.9960,   0.9960],\n",
       "         [  0.9964,   0.9870],\n",
       "         [  0.9965,   0.9982],\n",
       "         [  0.9989,   0.9866],\n",
       "         [  0.9991,   0.9981]],\n",
       "\n",
       "        [[  0.9986,   0.9982],\n",
       "         [  0.9989,   0.9930],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9986,   0.9977],\n",
       "         [  0.9992,   0.9978]],\n",
       "\n",
       "        [[  0.9988,   0.9981],\n",
       "         [  0.9992,   0.9977],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9983,   0.9980],\n",
       "         [  0.9990,   0.9986]],\n",
       "\n",
       "        [[  0.9994,   0.9977],\n",
       "         [  0.9994,   0.9978],\n",
       "         [  0.9992,   0.9967],\n",
       "         [  0.9991,   0.9988],\n",
       "         [  0.9994,   0.9982]],\n",
       "\n",
       "        [[  0.9993,   0.9925],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9993,   0.9973],\n",
       "         [  0.9991,   0.9979]],\n",
       "\n",
       "        [[  0.9993,   0.9979],\n",
       "         [  0.9985,   0.9981],\n",
       "         [  0.9995,   0.9979],\n",
       "         [  0.9989,   0.9972],\n",
       "         [  0.9994,   0.9969]],\n",
       "\n",
       "        [[  0.9992,   0.9982],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9992,   0.9984],\n",
       "         [  0.9994,   0.9982],\n",
       "         [  0.9994,   0.9985]],\n",
       "\n",
       "        [[  0.9992,   0.9984],\n",
       "         [  0.9995,   0.9982],\n",
       "         [  0.9992,   0.9978],\n",
       "         [  0.9987,   0.9983],\n",
       "         [  0.9992,   0.9974]],\n",
       "\n",
       "        [[  0.9989,   0.9985],\n",
       "         [  0.9995,   0.9986],\n",
       "         [  0.9984,   0.9982],\n",
       "         [  0.9995,   0.9981],\n",
       "         [  0.9993,   0.9987]],\n",
       "\n",
       "        [[  0.9994,   0.9984],\n",
       "         [  0.9993,   0.9945],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9984],\n",
       "         [  0.9995,   0.9987]],\n",
       "\n",
       "        [[  0.9993,   0.9985],\n",
       "         [  0.9991,   0.9979],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9989,   0.9985],\n",
       "         [  0.9993,   0.9986]],\n",
       "\n",
       "        [[  0.9992,   0.9978],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9993,   0.9983],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9986]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c8ef4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.7299, -13.4359],\n",
       "         [ -0.4653,   0.3035],\n",
       "         [ -0.9375,  -1.4426],\n",
       "         [  0.7532,  -1.3478],\n",
       "         [  0.7942,   0.5478]],\n",
       "\n",
       "        [[  0.9748,   0.8843],\n",
       "         [  0.9621,   0.5900],\n",
       "         [  0.8393,   0.9504],\n",
       "         [  0.9729,   0.9679],\n",
       "         [  0.9557,   0.9612]],\n",
       "\n",
       "        [[  0.9910,   0.9662],\n",
       "         [  0.9803,   0.9930],\n",
       "         [  0.9909,   0.9862],\n",
       "         [  0.9709,   0.9916],\n",
       "         [  0.9886,   0.9889]],\n",
       "\n",
       "        [[  0.9972,   0.9931],\n",
       "         [  0.9828,   0.9924],\n",
       "         [  0.9957,   0.9945],\n",
       "         [  0.9942,   0.9855],\n",
       "         [  0.9826,   0.9708]],\n",
       "\n",
       "        [[  0.9940,   0.9897],\n",
       "         [  0.9971,   0.9981],\n",
       "         [  0.9752,   0.9925],\n",
       "         [  0.9982,   0.9943],\n",
       "         [  0.9899,   0.9912]],\n",
       "\n",
       "        [[  0.9951,   0.9945],\n",
       "         [  0.9914,   0.9988],\n",
       "         [  0.9973,   0.9972],\n",
       "         [  0.9956,   0.9978],\n",
       "         [  0.9873,   0.9955]],\n",
       "\n",
       "        [[  0.9929,   0.9978],\n",
       "         [  0.9978,   0.9916],\n",
       "         [  0.9978,   0.9979],\n",
       "         [  0.9987,   0.9948],\n",
       "         [  0.9976,   0.9975]],\n",
       "\n",
       "        [[  0.9954,   0.9965],\n",
       "         [  0.9987,   0.9984],\n",
       "         [  0.9984,   0.9970],\n",
       "         [  0.9986,   0.9968],\n",
       "         [  0.9980,   0.9981]],\n",
       "\n",
       "        [[  0.9960,   0.9960],\n",
       "         [  0.9964,   0.9870],\n",
       "         [  0.9965,   0.9982],\n",
       "         [  0.9989,   0.9866],\n",
       "         [  0.9991,   0.9981]],\n",
       "\n",
       "        [[  0.9986,   0.9982],\n",
       "         [  0.9989,   0.9930],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9986,   0.9977],\n",
       "         [  0.9992,   0.9978]],\n",
       "\n",
       "        [[  0.9988,   0.9981],\n",
       "         [  0.9992,   0.9977],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9983,   0.9980],\n",
       "         [  0.9990,   0.9986]],\n",
       "\n",
       "        [[  0.9994,   0.9977],\n",
       "         [  0.9994,   0.9978],\n",
       "         [  0.9992,   0.9967],\n",
       "         [  0.9991,   0.9988],\n",
       "         [  0.9994,   0.9982]],\n",
       "\n",
       "        [[  0.9993,   0.9925],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9993,   0.9973],\n",
       "         [  0.9991,   0.9979]],\n",
       "\n",
       "        [[  0.9993,   0.9979],\n",
       "         [  0.9985,   0.9981],\n",
       "         [  0.9995,   0.9979],\n",
       "         [  0.9989,   0.9972],\n",
       "         [  0.9994,   0.9969]],\n",
       "\n",
       "        [[  0.9992,   0.9982],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9992,   0.9984],\n",
       "         [  0.9994,   0.9982],\n",
       "         [  0.9994,   0.9985]],\n",
       "\n",
       "        [[  0.9992,   0.9984],\n",
       "         [  0.9995,   0.9982],\n",
       "         [  0.9992,   0.9978],\n",
       "         [  0.9987,   0.9983],\n",
       "         [  0.9992,   0.9974]],\n",
       "\n",
       "        [[  0.9989,   0.9985],\n",
       "         [  0.9995,   0.9986],\n",
       "         [  0.9984,   0.9982],\n",
       "         [  0.9995,   0.9981],\n",
       "         [  0.9993,   0.9987]],\n",
       "\n",
       "        [[  0.9994,   0.9984],\n",
       "         [  0.9993,   0.9945],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9984],\n",
       "         [  0.9995,   0.9987]],\n",
       "\n",
       "        [[  0.9993,   0.9985],\n",
       "         [  0.9991,   0.9979],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9989,   0.9985],\n",
       "         [  0.9993,   0.9986]],\n",
       "\n",
       "        [[  0.9992,   0.9978],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9993,   0.9983],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9986]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c07bf934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x[1:],np.hstack((MSE.mean(axis=1),MSE_p.mean(axis=1),MSEa.mean(axis=1)))[1:,[1,3,5]],'o') \n",
    "plt.fill_between(x[1:], MSE.mean(axis=1)[1:,1]+MSE.std(axis=1)[1:,1], y2=MSE.mean(axis=1)[1:,1]-MSE.std(axis=1)[1:,1],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSE_p.mean(axis=1)[1:,1]+MSE_p.std(axis=1)[1:,1], y2=MSE_p.mean(axis=1)[1:,1]-MSE_p.std(axis=1)[1:,1],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSEa.mean(axis=1)[1:,1]+MSEa.std(axis=1)[1:,1], y2=MSEa.mean(axis=1)[1:,1]-MSEa.std(axis=1)[1:,1],alpha=0.4)\n",
    "plt.legend(['$\\delta_a$','$f_1$','$\\delta_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "plt.yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a34fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,np.hstack((R2.mean(axis=1),R2_p.mean(axis=1)))[:,[0,2]],'o') \n",
    "plt.legend(['$\\delta_1$','$f_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,np.hstack((R2.mean(axis=1),R2_p.mean(axis=1)))[:,[1,3]],'o') \n",
    "plt.legend(['$\\delta_1$','$f_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,MSE_p.mean(axis=1)-MSE.mean(axis=1),'o')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,R2_p.mean(axis=1)-R2.mean(axis=1),'o')\n",
    "plt.legend(y_labels.values)\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_p\n",
    "\n",
    "R2_p-R2\n",
    "\n",
    "plt.plot(x,R2.mean(axis=1),'o')\n",
    "plt.legend(y_labels.values)\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6081e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13668c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d76d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec2e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b626b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    #('scaler', StandardScaler()),\n",
    "    ('preprocessor', PolynomialFeatures(degree=1, include_bias=False,interaction_only=False)),\n",
    "    ('lasso', LassoCV(n_alphas=1000,max_iter=10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b84a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m0_mat(y_test,emulators,x_test,output):\n",
    "\n",
    "    m0=torch.zeros((y_test.shape[0],len(emulators)))\n",
    "    for i in range(len(emulators)):\n",
    "        m0[:,i]=(emulators[i].predict(x_test)[:,output]-y_train.mean(axis=0)[output])/y_train.std(axis=0)[output]\n",
    "\n",
    "\n",
    "    return m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f336bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy(a,y_train,m0,output):\n",
    "    m_t = (m0-y_train.mean(axis=0))/y_train.std(axis=0)\n",
    "    y_t = (y_train-y_train.mean(axis=0))/y_train.std(axis=0)\n",
    "    a=torch.tensor(a)\n",
    "    res = ((a*m_t-y_t)**2).mean(axis=0).detach().numpy()\n",
    "    return res[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080fcbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:208: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3618.)\n",
      "  prediction=torch.stack(prediction).T\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor([0.9552, 0.8688], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0454, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.78643296]\n",
      "[1.03861305]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9629, 0.9253], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0454, 0.0486, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.71415911]\n",
      "[0.99313263]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9864, 0.9912], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0518, 0.0000, 0.0000],\n",
      "         [0.0454, 0.0486, 0.0521, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.34170325]\n",
      "[0.96508579]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9594, 0.9342], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0518, 0.0499, 0.0000],\n",
      "         [0.0454, 0.0486, 0.0521, 0.0489, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.35915821]\n",
      "[0.89885475]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.6255, 0.7594], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0518, 0.0499, 0.0326],\n",
      "         [0.0454, 0.0486, 0.0521, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.75881644]\n",
      "[0.94657515]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9882, 0.6934], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0505, 0.0518, 0.0499, 0.0326],\n",
      "         [0.0813, 0.0486, 0.0521, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.96189814]\n",
      "[0.97596303]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.7960, 0.9484], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.0518, 0.0499, 0.0326],\n",
      "         [0.0813, 0.0978, 0.0521, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.89271044]\n",
      "[0.99160148]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9180, 0.5819], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.1000, 0.0499, 0.0326],\n",
      "         [0.0813, 0.0978, 0.0826, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.97157977]\n",
      "[0.91008065]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.7494, 0.8679], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.1000, 0.0859, 0.0326],\n",
      "         [0.0813, 0.0978, 0.0826, 0.0941, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.88437692]\n",
      "[1.01081197]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9312, 0.9868], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.1000, 0.0859, 0.0814],\n",
      "         [0.0813, 0.0978, 0.0826, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.84492806]\n",
      "[1.01515787]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.8359, 0.9575], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.0919, 0.1000, 0.0859, 0.0814],\n",
      "         [0.1312, 0.0978, 0.0826, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.81394902]\n",
      "[0.8382223]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9611, 0.9568], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1000, 0.0859, 0.0814],\n",
      "         [0.1312, 0.1480, 0.0826, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.78299223]\n",
      "[0.94273276]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9675, 0.9875], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1507, 0.0859, 0.0814],\n",
      "         [0.1312, 0.1480, 0.1345, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.99151738]\n",
      "[0.96837806]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9761, 0.9703], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1507, 0.1370, 0.0814],\n",
      "         [0.1312, 0.1480, 0.1345, 0.1451, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.79199399]\n",
      "[0.57044319]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9586, 0.8748], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1507, 0.1370, 0.1318],\n",
      "         [0.1312, 0.1480, 0.1345, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.60603136]\n",
      "[0.52425433]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9534, 0.8806], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1422, 0.1507, 0.1370, 0.1318],\n",
      "         [0.1765, 0.1480, 0.1345, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.82920999]\n",
      "[1.06142236]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.6862, 0.9870], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1507, 0.1370, 0.1318],\n",
      "         [0.1765, 0.1996, 0.1345, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.01726546]\n",
      "[0.67125115]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.4974, 0.9368], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1766, 0.1370, 0.1318],\n",
      "         [0.1765, 0.1996, 0.1837, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.00381223]\n",
      "[0.4073458]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9897, 0.9805], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1766, 0.1890, 0.1318],\n",
      "         [0.1765, 0.1996, 0.1837, 0.1958, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.14658024]\n",
      "[0.95680637]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9293, 0.9246], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1766, 0.1890, 0.1803],\n",
      "         [0.1765, 0.1996, 0.1837, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.03248437]\n",
      "[1.04499828]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9596, 0.9911], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.1779, 0.1766, 0.1890, 0.1803],\n",
      "         [0.2286, 0.1996, 0.1837, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.55192009]\n",
      "[0.73466716]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9733, 0.8583], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.1766, 0.1890, 0.1803],\n",
      "         [0.2286, 0.2423, 0.1837, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.84243681]\n",
      "[0.96488054]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9014, 0.9884], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.2235, 0.1890, 0.1803],\n",
      "         [0.2286, 0.2423, 0.2357, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.97034059]\n",
      "[0.94393583]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.6555, 0.9703], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.2235, 0.2227, 0.1803],\n",
      "         [0.2286, 0.2423, 0.2357, 0.2467, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.96805405]\n",
      "[0.53001674]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9095, 0.9569], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.2235, 0.2227, 0.2281],\n",
      "         [0.2286, 0.2423, 0.2357, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.71278732]\n",
      "[0.98381221]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9186, 0.9852], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2290, 0.2235, 0.2227, 0.2281],\n",
      "         [0.2803, 0.2423, 0.2357, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.69985945]\n",
      "[0.85794203]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.8810, 0.9608], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2235, 0.2227, 0.2281],\n",
      "         [0.2803, 0.2928, 0.2357, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.82644116]\n",
      "[0.61731617]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.7579, 0.9913], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2628, 0.2227, 0.2281],\n",
      "         [0.2803, 0.2928, 0.2877, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.35464363]\n",
      "[0.8292417]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9789], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2628, 0.2749, 0.2281],\n",
      "         [0.2803, 0.2928, 0.2877, 0.2981, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.57237759]\n",
      "[0.76894515]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9095, 0.9845], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2628, 0.2749, 0.2754],\n",
      "         [0.2803, 0.2928, 0.2877, 0.2981, 0.2864]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.77282944]\n",
      "[0.58204038]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9688, 0.9671], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.3435, 0.2752, 0.2628, 0.2749, 0.2754],\n",
      "         [0.3311, 0.2928, 0.2877, 0.2981, 0.2864]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.93810264]\n",
      "[0.55569326]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m Ti[\u001b[38;5;241m2\u001b[39m,num,i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(emulators))\n\u001b[1;32m     69\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 70\u001b[0m model_dc_learned \u001b[38;5;241m=\u001b[39m GPE\u001b[38;5;241m.\u001b[39mensemble(X_train1,y_train1,mean_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrepancy_cohort\u001b[39m\u001b[38;5;124m\"\u001b[39m,training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,ref_emulator\u001b[38;5;241m=\u001b[39m[emulators2[em]])\n\u001b[1;32m     71\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     72\u001b[0m R2temp,R2std\u001b[38;5;241m=\u001b[39mmodel_dc_learned\u001b[38;5;241m.\u001b[39mR2_sample(X_test,y_test,\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:22\u001b[0m, in \u001b[0;36mensemble.__init__\u001b[0;34m(self, X_train, y_train, mean_func, training_iter, kernel, kernel_params, ref_emulator, a, a_indicator)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m=\u001b[39mkernel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_params\u001b[38;5;241m=\u001b[39mkernel_params\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_ensemble()\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:108\u001b[0m, in \u001b[0;36mensemble.create_ensemble\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, Y)\n\u001b[0;32m--> 108\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print('Iter %d/%d - Loss: %.3f' % (\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#     j + 1, training_iter, loss.item()\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# ))\u001b[39;00m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reps=5\n",
    "nn=[10,15,20,25,30,35,40,45,50,55,60,80,100,120,140]\n",
    "R2=torch.zeros(7,len(nn),2,reps)\n",
    "ISE=torch.zeros(7,len(nn),2,reps)\n",
    "Ti=torch.zeros(7,len(nn),reps)\n",
    "\n",
    "for num, n in enumerate(nn):\n",
    "    for k in range(len(emulators)):\n",
    "        emulators2=emulators.copy()\n",
    "        emulators2.pop(k)\n",
    "        print(len(emulators2))\n",
    "\n",
    "        X_train = train_input[k]\n",
    "        y_train = train_output[k]\n",
    "        X_test = test_input[k]\n",
    "        y_test = test_output[k]\n",
    "        \n",
    "        for i in range(reps):\n",
    "\n",
    "            #b=np.random.choice(range(X_train.shape[0]),n,replace=False)\n",
    "            \n",
    "            X=X_train\n",
    "            y=y_train \n",
    "            X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=n,\n",
    "                random_state=i\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            model_f=GPE.ensemble(X_train1,y_train1,mean_func=\"linear\",training_iter=500)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_f.R2_sample(X_test,y_test,1000)\n",
    "            R2[0,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[0,num,:,i]+=model_f.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[0,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "\n",
    "            em=np.random.randint(len(emulators2))\n",
    "            start = time.time()\n",
    "            model_dc_1 = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=torch.tensor([[1],[1]]))\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_1.R2_sample(X_test,y_test,1000)\n",
    "            R2[1,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[1,num,:,i]+=model_dc_1.ISE(X_test,y_test)/(len(emulators))\n",
    "            print(model_dc_1.R2(X_test,y_test))\n",
    "            print(R2[1])\n",
    "\n",
    "            Ti[1,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            m0 = emulators2[em].predict(X_train1)\n",
    "            a_d=np.zeros((y_train.shape[1],1))\n",
    "            for l in range(y_train.shape[1]):\n",
    "                result = scipy.optimize.minimize(proxy, 1, args=(y_train1,m0,l), method='Nelder-Mead', tol=1e-8)\n",
    "                print(result.x)\n",
    "                a_d[l]=result.x\n",
    "            a_d=torch.tensor(a_d)\n",
    "            model_dc_reg = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_reg.R2_sample(X_test,y_test,1000)\n",
    "            R2[2,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[2,num,:,i]+=model_dc_reg.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[2,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_learned = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]])\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[3,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[3,num,:,i]+=model_dc_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[3,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_all = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_all.R2_sample(X_test,y_test,1000)\n",
    "            R2[4,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[4,num,:,i]+=model_dc_all.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[4,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            a_d=torch.zeros((y_train1.shape[1],len(emulators2)))\n",
    "            for j in range(y_train1.shape[1]):\n",
    "                m0=m0_mat(y_train1,emulators2,X_train1,j)\n",
    "                # fit to an order-3 polynomial data\n",
    "                y_t=(y_train1[:,j]-y_train1.mean(axis=0)[j])/y_train1.std(axis=0)[j]\n",
    "                model = model.fit(m0.detach().numpy(), y_t.detach().numpy())\n",
    "                a_d[j]=torch.tensor(model.named_steps['lasso'].coef_)\n",
    "\n",
    "\n",
    "            model_dc_lasso=GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso.R2_sample(X_test,y_test,1000)\n",
    "            R2[5,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[5,num,:,i]+=model_dc_lasso.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[5,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_lasso_learned=GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d,a_indicator=True)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[6,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[6,num,:,i]+=model_dc_lasso_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[6,num,i]+=(end-start)/(len(emulators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame((R2[:,0].mean(axis=2)))\n",
    "\n",
    "results.index=['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd065459",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame((ISE[:,14].mean(axis=2)))\n",
    "\n",
    "results.index=['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df34a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=0\n",
    "lim=10\n",
    "y_lim=[0.7,1.01]\n",
    "plt.plot(nn[lim:],R2.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.legend(['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2.mean(axis=3)[i,lim:,o]+R2.std(axis=3)[i,lim:,o], R2.mean(axis=3)[i,lim:,o]-R2.std(axis=3)[i,lim:,o],alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4567a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_save = R2.reshape(7,len(nn)*reps*y_train.shape[1])\n",
    "\n",
    "np.savetxt(\"DiscrepR2TrainNVaryDefinitive.csv\", R2_save.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6878161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "707212a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_s=pd.read_csv(\"DiscrepR2TrainNVaryDefinitive.csv\",header=None).values.reshape(7,len(nn),y_train.shape[1],reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "d8abc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "fontS=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "31e45151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=0\n",
    "lim=1\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],R2_s.mean(axis=3)[:,lim:,o].T)\n",
    "plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2_s.mean(axis=3)[i,lim:,o]+R2_s.std(axis=3)[i,lim:,o], R2_s.mean(axis=3)[i,lim:,o]-R2_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepATAT.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "c11db18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=1\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],R2_s.mean(axis=3)[:,lim:,o].T)\n",
    "plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2_s.mean(axis=3)[i,lim:,o]+R2_s.std(axis=3)[i,lim:,o], R2_s.mean(axis=3)[i,lim:,o]-R2_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepVTAT.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "7398961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 2, 5)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_s[:,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "7880e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ee7f5ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 80, 100, 120, 140]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "1c84885b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mdataframe([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'dataframe'"
     ]
    }
   ],
   "source": [
    "pd.dataframe(['&','&','&','&','&','&','&'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "17b8900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "41e1b73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a9b96_row0_col0, #T_a9b96_row0_col1 {\n",
       "  background-color: pink;\n",
       "}\n",
       "#T_a9b96_row5_col1, #T_a9b96_row6_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a9b96\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a9b96_level0_col0\" class=\"col_heading level0 col0\" >A_TAT</th>\n",
       "      <th id=\"T_a9b96_level0_col1\" class=\"col_heading level0 col1\" >V_TAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row0\" class=\"row_heading level0 row0\" >\\$g_1\\$</th>\n",
       "      <td id=\"T_a9b96_row0_col0\" class=\"data row0 col0\" >0.988466</td>\n",
       "      <td id=\"T_a9b96_row0_col1\" class=\"data row0 col1\" >0.977145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row1\" class=\"row_heading level0 row1\" >\\$g_{\\delta}:a=1\\$</th>\n",
       "      <td id=\"T_a9b96_row1_col0\" class=\"data row1 col0\" >0.995786</td>\n",
       "      <td id=\"T_a9b96_row1_col1\" class=\"data row1 col1\" >0.988301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row2\" class=\"row_heading level0 row2\" >\\$g_{\\delta}:a=a_r\\$</th>\n",
       "      <td id=\"T_a9b96_row2_col0\" class=\"data row2 col0\" >0.996202</td>\n",
       "      <td id=\"T_a9b96_row2_col1\" class=\"data row2 col1\" >0.988720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row3\" class=\"row_heading level0 row3\" >\\$g_{\\delta h}:a=a_h\\$</th>\n",
       "      <td id=\"T_a9b96_row3_col0\" class=\"data row3 col0\" >0.996413</td>\n",
       "      <td id=\"T_a9b96_row3_col1\" class=\"data row3 col1\" >0.991275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row4\" class=\"row_heading level0 row4\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row4_col0\" class=\"data row4 col0\" >0.994823</td>\n",
       "      <td id=\"T_a9b96_row4_col1\" class=\"data row4 col1\" >0.993605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row5\" class=\"row_heading level0 row5\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row5_col0\" class=\"data row5 col0\" >0.997331</td>\n",
       "      <td id=\"T_a9b96_row5_col1\" class=\"data row5 col1\" >0.994397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row6\" class=\"row_heading level0 row6\" >\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row6_col0\" class=\"data row6 col0\" >0.997876</td>\n",
       "      <td id=\"T_a9b96_row6_col1\" class=\"data row6 col1\" >0.994373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2e5886e10>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame((R2_s[:,6].mean(axis=2)))\n",
    "\n",
    "results.index=['\\$g_1\\$','\\$g_{\\delta}:a=1\\$','\\$g_{\\delta}:a=a_r\\$','\\$g_{\\delta h}:a=a_h\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$','\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "2a6bf275",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISE_save = ISE.reshape(7,len(nn)*reps*y_train.shape[1])\n",
    "\n",
    "np.savetxt(\"DiscrepISETrainNVaryDefinitive.csv\", ISE_save.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3f12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c5da4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISE_s=pd.read_csv(\"DiscrepISETrainNVaryDefinitive.csv\",header=None).values.reshape(7,len(nn),y_train.shape[1],reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b49a76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "fontS=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "d59c2b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=0\n",
    "lim=0\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],ISE_s.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], ISE_s.mean(axis=3)[i,lim:,o]+ISE_s.std(axis=3)[i,lim:,o], ISE_s.mean(axis=3)[i,lim:,o]-ISE_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepATATISE.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "37bb95f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=1\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],ISE_s.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], ISE_s.mean(axis=3)[i,lim:,o]+ISE_s.std(axis=3)[i,lim:,o], ISE_s.mean(axis=3)[i,lim:,o]-R2_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepVTATISE.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "f440a490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 2, 5)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_s[:,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "c0197b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "91204688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 80, 100, 120, 140]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "31f56fa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mdataframe([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'dataframe'"
     ]
    }
   ],
   "source": [
    "pd.dataframe(['&','&','&','&','&','&','&'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "923428bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "ba11b09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a9b96_row0_col0, #T_a9b96_row0_col1 {\n",
       "  background-color: pink;\n",
       "}\n",
       "#T_a9b96_row5_col1, #T_a9b96_row6_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a9b96\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a9b96_level0_col0\" class=\"col_heading level0 col0\" >A_TAT</th>\n",
       "      <th id=\"T_a9b96_level0_col1\" class=\"col_heading level0 col1\" >V_TAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row0\" class=\"row_heading level0 row0\" >\\$g_1\\$</th>\n",
       "      <td id=\"T_a9b96_row0_col0\" class=\"data row0 col0\" >0.988466</td>\n",
       "      <td id=\"T_a9b96_row0_col1\" class=\"data row0 col1\" >0.977145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row1\" class=\"row_heading level0 row1\" >\\$g_{\\delta}:a=1\\$</th>\n",
       "      <td id=\"T_a9b96_row1_col0\" class=\"data row1 col0\" >0.995786</td>\n",
       "      <td id=\"T_a9b96_row1_col1\" class=\"data row1 col1\" >0.988301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row2\" class=\"row_heading level0 row2\" >\\$g_{\\delta}:a=a_r\\$</th>\n",
       "      <td id=\"T_a9b96_row2_col0\" class=\"data row2 col0\" >0.996202</td>\n",
       "      <td id=\"T_a9b96_row2_col1\" class=\"data row2 col1\" >0.988720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row3\" class=\"row_heading level0 row3\" >\\$g_{\\delta h}:a=a_h\\$</th>\n",
       "      <td id=\"T_a9b96_row3_col0\" class=\"data row3 col0\" >0.996413</td>\n",
       "      <td id=\"T_a9b96_row3_col1\" class=\"data row3 col1\" >0.991275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row4\" class=\"row_heading level0 row4\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row4_col0\" class=\"data row4 col0\" >0.994823</td>\n",
       "      <td id=\"T_a9b96_row4_col1\" class=\"data row4 col1\" >0.993605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row5\" class=\"row_heading level0 row5\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row5_col0\" class=\"data row5 col0\" >0.997331</td>\n",
       "      <td id=\"T_a9b96_row5_col1\" class=\"data row5 col1\" >0.994397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row6\" class=\"row_heading level0 row6\" >\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row6_col0\" class=\"data row6 col0\" >0.997876</td>\n",
       "      <td id=\"T_a9b96_row6_col1\" class=\"data row6 col1\" >0.994373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2e5886e10>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame((R2_s[:,6].mean(axis=2)))\n",
    "\n",
    "results.index=['\\$g_1\\$','\\$g_{\\delta}:a=1\\$','\\$g_{\\delta}:a=a_r\\$','\\$g_{\\delta h}:a=a_h\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$','\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3c31f",
   "metadata": {},
   "source": [
    "# Atrial Stiffness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee892899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43b4ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes=['01','02','03','04','05','06']\n",
    "\n",
    "Ys=[]\n",
    "Xs=[]\n",
    "\n",
    "for i in range(len(meshes)):\n",
    "    val=meshes[i]\n",
    "    \n",
    "    inputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/LA_data/case\"+val+\"/X.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    outputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/LA_data/case\"+val+\"/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "\n",
    "    \n",
    "\n",
    "    Xs.append(torch.tensor(inputData[0:200]))\n",
    "    Ys.append(torch.tensor(outputData[0:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b9b3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "train_input=[]\n",
    "test_input = []\n",
    "train_output=[]\n",
    "test_output = []\n",
    "emulators=[]\n",
    "\n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=Xs[i]\n",
    "    y=Ys[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input.append(X_train)\n",
    "    test_input.append(X_test)\n",
    "    train_output.append(y_train)\n",
    "    test_output.append(y_test)\n",
    "    emulator = GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=500)\n",
    "    emulators.append(emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bb6a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -9.0876,  -1.7839, -13.9763,  -6.8036,  -5.3839, -28.4096,   0.7142],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2980,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3310,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1354,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9003,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-4.7392,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1186,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n",
      "[0.42850708]\n",
      "[0.16977268]\n",
      "[0.18070077]\n",
      "[0.48470408]\n",
      "[0.06471149]\n",
      "[0.30723889]\n",
      "[0.00915806]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.5460,  0.7232, -0.4243,  0.7317,  0.4467, -0.9929,  0.9197],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159,  0.0892,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2980,  0.1193,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3310, -0.0738,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1354,  0.1175,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9003,  0.0718,  0.0000,  0.0000,  0.0000],\n",
      "         [-4.7392, -0.1692,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1186,  0.1523,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n",
      "[0.44011486]\n",
      "[0.35291706]\n",
      "[0.31281794]\n",
      "[0.64680969]\n",
      "[0.07741194]\n",
      "[0.17071068]\n",
      "[0.03316094]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.1040, -0.8396, -0.2666,  0.5166,  0.1814, -3.6879,  0.8796],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159,  0.0892, -0.1893,  0.0000,  0.0000],\n",
      "         [-0.2980,  0.1193, -0.1452,  0.0000,  0.0000],\n",
      "         [-2.3310, -0.0738, -0.0516,  0.0000,  0.0000],\n",
      "         [-1.1354,  0.1175,  0.0817,  0.0000,  0.0000],\n",
      "         [-0.9003,  0.0718,  0.0278,  0.0000,  0.0000],\n",
      "         [-4.7392, -0.1692, -0.6288,  0.0000,  0.0000],\n",
      "         [ 0.1186,  0.1523,  0.1439,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n",
      "[0.64439529]\n",
      "[0.08674326]\n",
      "[0.04759069]\n",
      "[0.66768465]\n",
      "[0.56556691]\n",
      "[0.17467271]\n",
      "[0.21179094]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 2.9886e-02, -9.8268e-01, -7.7263e+00, -8.4969e+01, -2.2269e+00,\n",
      "        -2.5796e+00, -1.3139e-01], dtype=torch.float64,\n",
      "       grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159e+00,  8.9238e-02, -1.8929e-01, -3.1074e-04,  0.0000e+00],\n",
      "         [-2.9798e-01,  1.1934e-01, -1.4525e-01, -1.6743e-01,  0.0000e+00],\n",
      "         [-2.3310e+00, -7.3837e-02, -5.1640e-02, -1.2959e+00,  0.0000e+00],\n",
      "         [-1.1354e+00,  1.1755e-01,  8.1675e-02, -1.4165e+01,  0.0000e+00],\n",
      "         [-9.0030e-01,  7.1777e-02,  2.7786e-02, -3.7606e-01,  0.0000e+00],\n",
      "         [-4.7392e+00, -1.6921e-01, -6.2883e-01, -4.3527e-01,  0.0000e+00],\n",
      "         [ 1.1855e-01,  1.5228e-01,  1.4393e-01, -2.3658e-02,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.02861606]\n",
      "[0.00496223]\n",
      "[0.03694755]\n",
      "[0.71150105]\n",
      "[0.03157613]\n",
      "[0.0565942]\n",
      "[0.12495688]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-0.1265,  0.6150, -0.2116,  0.6184, -0.1561, -0.0845,  0.6877],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159e+00,  8.9238e-02, -1.8929e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-2.9798e-01,  1.1934e-01, -1.4525e-01, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.3310e+00, -7.3837e-02, -5.1640e-02, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.1354e+00,  1.1755e-01,  8.1675e-02, -1.4165e+01,  9.0978e-02],\n",
      "         [-9.0030e-01,  7.1777e-02,  2.7786e-02, -3.7606e-01, -3.3002e-02],\n",
      "         [-4.7392e+00, -1.6921e-01, -6.2883e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 1.1855e-01,  1.5228e-01,  1.4393e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.07271767]\n",
      "[0.04698787]\n",
      "[0.04238831]\n",
      "[0.74447277]\n",
      "[0.06359182]\n",
      "[0.13098816]\n",
      "[0.18652486]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-0.9011, -0.5603, -1.2820,  0.7567,  0.8196, -3.8143,  0.6895],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  8.9238e-02, -1.8929e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-3.9845e-01,  1.1934e-01, -1.4525e-01, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.5616e+00, -7.3837e-02, -5.1640e-02, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.1755e-01,  8.1675e-02, -1.4165e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  7.1777e-02,  2.7786e-02, -3.7606e-01, -3.3002e-02],\n",
      "         [-5.3844e+00, -1.6921e-01, -6.2883e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 2.3120e-01,  1.5228e-01,  1.4393e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.13029052]\n",
      "[0.88796357]\n",
      "[1.04526361]\n",
      "[0.87840017]\n",
      "[0.56479376]\n",
      "[0.71230068]\n",
      "[0.34944377]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.5147, -0.1967,  0.4627,  0.4926,  0.3607, -0.8251,  0.6441],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -1.8929e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -1.4525e-01, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.5616e+00, -3.7197e-03, -5.1640e-02, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.9827e-01,  8.1675e-02, -1.4165e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  1.2487e-01,  2.7786e-02, -3.7606e-01, -3.3002e-02],\n",
      "         [-5.3844e+00, -3.1508e-01, -6.2883e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.4393e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.02008228]\n",
      "[0.92773732]\n",
      "[0.5248596]\n",
      "[0.97486199]\n",
      "[0.75069551]\n",
      "[0.97609093]\n",
      "[0.56423686]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -3.3725, -27.0962, -12.8221,  -3.0624,  -4.5058,  -1.8377,   0.3056],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -7.6336e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -4.6718e+00, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.5616e+00, -3.7197e-03, -2.2151e+00, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.9827e-01, -4.3525e-01, -1.4165e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  1.2487e-01, -7.3310e-01, -3.7606e-01, -3.3002e-02],\n",
      "         [-5.3844e+00, -3.1508e-01, -9.5026e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.9112e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.07645264]\n",
      "[0.82026949]\n",
      "[1.0644201]\n",
      "[0.96888561]\n",
      "[0.48250508]\n",
      "[1.02134132]\n",
      "[0.63585567]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -3.9390,  -7.8289, -45.9384,  -9.2830, -11.6866, -23.0289, -14.8757],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -7.6336e-01, -7.4473e-01, -2.8553e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -4.6718e+00, -1.6129e+00,  8.3507e-02],\n",
      "         [-2.5616e+00, -3.7197e-03, -2.2151e+00, -9.0295e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.9827e-01, -4.3525e-01, -1.5751e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  1.2487e-01, -7.3310e-01, -2.3574e+00, -3.3002e-02],\n",
      "         [-5.3844e+00, -3.1508e-01, -9.5026e-01, -4.3203e+00, -2.7121e-02],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.9112e-01, -2.5165e+00,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.45529429]\n",
      "[1.65206165]\n",
      "[0.40189023]\n",
      "[1.41990188]\n",
      "[0.35982506]\n",
      "[1.27250719]\n",
      "[0.07092059]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-0.0077, -4.5067, -1.4140,  0.6690, -5.5467, -0.7967, -4.5380],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -7.6336e-01, -7.4473e-01, -3.7971e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -4.6718e+00, -1.6129e+00, -6.7470e-01],\n",
      "         [-2.5616e+00, -3.7197e-03, -2.2151e+00, -9.0295e+00, -2.9150e-01],\n",
      "         [-1.0136e+00,  1.9827e-01, -4.3525e-01, -1.5751e+01,  1.9450e-01],\n",
      "         [-7.7255e-01,  1.2487e-01, -7.3310e-01, -2.3574e+00, -9.7383e-01],\n",
      "         [-5.3844e+00, -3.1508e-01, -9.5026e-01, -4.3203e+00, -1.7452e-01],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.9112e-01, -2.5165e+00, -6.4878e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.54948695]\n",
      "[0.1307068]\n",
      "[0.45193608]\n",
      "[1.68455612]\n",
      "[0.571211]\n",
      "[0.52624828]\n",
      "[0.03160185]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.8201, -0.8780,  0.0804,  0.5079, -5.1852, -0.5305,  0.0234],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.9893e+00,  1.6980e-01, -7.6336e-01, -7.4473e-01, -3.7971e-02],\n",
      "         [-5.5799e-01,  8.1723e-02, -4.6718e+00, -1.6129e+00, -6.7470e-01],\n",
      "         [-2.5574e+00, -3.7197e-03, -2.2151e+00, -9.0295e+00, -2.9150e-01],\n",
      "         [-9.3094e-01,  1.9827e-01, -4.3525e-01, -1.5751e+01,  1.9450e-01],\n",
      "         [-1.6622e+00,  1.2487e-01, -7.3310e-01, -2.3574e+00, -9.7383e-01],\n",
      "         [-5.5121e+00, -3.1508e-01, -9.5026e-01, -4.3203e+00, -1.7452e-01],\n",
      "         [ 2.2975e-01,  2.5773e-01,  1.9112e-01, -2.5165e+00, -6.4878e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.2223029]\n",
      "[1.03344418]\n",
      "[1.17388989]\n",
      "[1.47676235]\n",
      "[1.14969611]\n",
      "[0.9811797]\n",
      "[0.44833741]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -0.5592,  -0.6650,  -0.0874,   0.5549,  -0.5536, -48.9892,  -0.4980],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -0.7634,  -0.7447,  -0.0380],\n",
      "         [ -0.5580,  -0.0368,  -4.6718,  -1.6129,  -0.6747],\n",
      "         [ -2.5574,  -0.0292,  -2.2151,  -9.0295,  -0.2915],\n",
      "         [ -0.9309,   0.2890,  -0.4353, -15.7510,   0.1945],\n",
      "         [ -1.6622,   0.0210,  -0.7331,  -2.3574,  -0.9738],\n",
      "         [ -5.5121,  -8.5128,  -0.9503,  -4.3203,  -0.1745],\n",
      "         [  0.2297,   0.1709,   0.1911,  -2.5165,  -0.6488]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.13373025]\n",
      "[1.32248917]\n",
      "[1.11305912]\n",
      "[0.92121302]\n",
      "[1.17055445]\n",
      "[1.04957312]\n",
      "[0.52580177]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.4955, -1.4294,  0.1148,  0.2980, -1.3630, -3.6188, -2.2182],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -1.0452,  -0.7447,  -0.0380],\n",
      "         [ -0.5580,  -0.0368,  -4.9599,  -1.6129,  -0.6747],\n",
      "         [ -2.5574,  -0.0292,  -2.2142,  -9.0295,  -0.2915],\n",
      "         [ -0.9309,   0.2890,  -0.4061, -15.7510,   0.1945],\n",
      "         [ -1.6622,   0.0210,  -0.9823,  -2.3574,  -0.9738],\n",
      "         [ -5.5121,  -8.5128,  -1.5887,  -4.3203,  -0.1745],\n",
      "         [  0.2297,   0.1709,  -0.2005,  -2.5165,  -0.6488]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.9599092]\n",
      "[1.01336825]\n",
      "[0.84972161]\n",
      "[0.96835811]\n",
      "[0.75670898]\n",
      "[0.73550715]\n",
      "[0.27096455]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8759, 0.8812, 0.4617, 0.5173, 0.7389, 0.8296, 0.9020],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -1.0452,  -0.6012,  -0.0380],\n",
      "         [ -0.5580,  -0.0368,  -4.9599,  -1.4674,  -0.6747],\n",
      "         [ -2.5574,  -0.0292,  -2.2142,  -8.9586,  -0.2915],\n",
      "         [ -0.9309,   0.2890,  -0.4061, -15.6657,   0.1945],\n",
      "         [ -1.6622,   0.0210,  -0.9823,  -2.2393,  -0.9738],\n",
      "         [ -5.5121,  -8.5128,  -1.5887,  -4.1893,  -0.1745],\n",
      "         [  0.2297,   0.1709,  -0.2005,  -2.3671,  -0.6488]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.75894311]\n",
      "[1.19966229]\n",
      "[1.06858971]\n",
      "[0.98402007]\n",
      "[0.76041896]\n",
      "[0.65378435]\n",
      "[0.33374144]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.0101, -2.1094, -5.7402, -2.1136, -1.0794,  0.7227,  0.7524],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -1.0452,  -0.6012,  -0.0393],\n",
      "         [ -0.5580,  -0.0368,  -4.9599,  -1.4674,  -1.0300],\n",
      "         [ -2.5574,  -0.0292,  -2.2142,  -8.9586,  -1.2533],\n",
      "         [ -0.9309,   0.2890,  -0.4061, -15.6657,  -0.1989],\n",
      "         [ -1.6622,   0.0210,  -0.9823,  -2.2393,  -1.1578],\n",
      "         [ -5.5121,  -8.5128,  -1.5887,  -4.1893,  -0.0557],\n",
      "         [  0.2297,   0.1709,  -0.2005,  -2.3671,  -0.5255]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.02600945]\n",
      "[-0.00712538]\n",
      "[0.0440953]\n",
      "[1.80280321]\n",
      "[0.00164515]\n",
      "[0.07408324]\n",
      "[0.02372122]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.2507, -3.5920, -1.2771, -1.5240, -0.1049,  0.5904, -0.7743],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9530,   0.0641,  -1.0452,  -0.6012,  -0.0393],\n",
      "         [ -1.1628,  -0.0368,  -4.9599,  -1.4674,  -1.0300],\n",
      "         [ -2.7801,  -0.0292,  -2.2142,  -8.9586,  -1.2533],\n",
      "         [ -1.1951,   0.2890,  -0.4061, -15.6657,  -0.1989],\n",
      "         [ -1.6912,   0.0210,  -0.9823,  -2.2393,  -1.1578],\n",
      "         [ -5.4192,  -8.5128,  -1.5887,  -4.1893,  -0.0557],\n",
      "         [  0.0974,   0.1709,  -0.2005,  -2.3671,  -0.5255]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.12159441]\n",
      "[0.03086853]\n",
      "[0.17833149]\n",
      "[0.587976]\n",
      "[0.06118822]\n",
      "[0.1205446]\n",
      "[0.23965024]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -4.4481, -15.6362,  -5.2597,  -7.6927, -10.4462,  -0.7631,  -2.3826],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9530,  -0.6824,  -1.0452,  -0.6012,  -0.0393],\n",
      "         [ -1.1628,  -2.6534,  -4.9599,  -1.4674,  -1.0300],\n",
      "         [ -2.7801,  -0.9098,  -2.2142,  -8.9586,  -1.2533],\n",
      "         [ -1.1951,  -0.9967,  -0.4061, -15.6657,  -0.1989],\n",
      "         [ -1.6912,  -1.7323,  -0.9823,  -2.2393,  -1.1578],\n",
      "         [ -5.4192,  -8.6466,  -1.5887,  -4.1893,  -0.0557],\n",
      "         [  0.0974,  -0.2318,  -0.2005,  -2.3671,  -0.5255]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.093986]\n",
      "[0.19462666]\n",
      "[0.02443466]\n",
      "[0.1126737]\n",
      "[0.02831467]\n",
      "[0.16848996]\n",
      "[0.06810595]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.1429e+02, -3.3096e+01, -4.1948e+02, -6.2976e+00,  2.4170e-01,\n",
      "        -4.2526e+01, -5.3397e+01], dtype=torch.float64,\n",
      "       grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.9530e+00, -6.8238e-01, -2.0111e+01, -6.0120e-01, -3.9328e-02],\n",
      "         [-1.1628e+00, -2.6534e+00, -1.0484e+01, -1.4674e+00, -1.0300e+00],\n",
      "         [-2.7801e+00, -9.0976e-01, -7.2283e+01, -8.9586e+00, -1.2533e+00],\n",
      "         [-1.1951e+00, -9.9669e-01, -1.4569e+00, -1.5666e+01, -1.9887e-01],\n",
      "         [-1.6912e+00, -1.7323e+00, -9.4971e-01, -2.2393e+00, -1.1578e+00],\n",
      "         [-5.4192e+00, -8.6466e+00, -8.7190e+00, -4.1893e+00, -5.5703e-02],\n",
      "         [ 9.7369e-02, -2.3182e-01, -9.0961e+00, -2.3671e+00, -5.2546e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04617866]\n",
      "[0.18357426]\n",
      "[0.00443144]\n",
      "[0.0333105]\n",
      "[0.05584475]\n",
      "[0.03238173]\n",
      "[0.85556338]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.1938, -0.4693,  0.6190,  0.5562,  0.5081,  0.3070,  0.8376],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.9530e+00, -6.8238e-01, -2.0111e+01, -5.9788e-01, -3.9328e-02],\n",
      "         [-1.1628e+00, -2.6534e+00, -1.0484e+01, -1.5772e+00, -1.0300e+00],\n",
      "         [-2.7801e+00, -9.0976e-01, -7.2283e+01, -8.8736e+00, -1.2533e+00],\n",
      "         [-1.1951e+00, -9.9669e-01, -1.4569e+00, -1.5581e+01, -1.9887e-01],\n",
      "         [-1.6912e+00, -1.7323e+00, -9.4971e-01, -2.1775e+00, -1.1578e+00],\n",
      "         [-5.4192e+00, -8.6466e+00, -8.7190e+00, -4.1631e+00, -5.5703e-02],\n",
      "         [ 9.7369e-02, -2.3182e-01, -9.0961e+00, -2.2306e+00, -5.2546e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04022648]\n",
      "[0.01869117]\n",
      "[0.06738706]\n",
      "[0.35855708]\n",
      "[0.02742423]\n",
      "[0.07474823]\n",
      "[0.1309245]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -2.1043,  -1.4666,  -2.1456, -13.0647,  -4.6588,  -3.6519,  -5.0345],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9530,  -0.6824, -20.1109,  -0.5979,  -0.3950],\n",
      "         [ -1.1628,  -2.6534, -10.4843,  -1.5772,  -1.2840],\n",
      "         [ -2.7801,  -0.9098, -72.2828,  -8.8736,  -1.6200],\n",
      "         [ -1.1951,  -0.9967,  -1.4569, -15.5815,  -2.3964],\n",
      "         [ -1.6912,  -1.7323,  -0.9497,  -2.1775,  -1.9525],\n",
      "         [ -5.4192,  -8.6466,  -8.7190,  -4.1631,  -0.6713],\n",
      "         [  0.0974,  -0.2318,  -9.0961,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.02837794]\n",
      "[-0.01931925]\n",
      "[0.02790187]\n",
      "[0.28639597]\n",
      "[0.00327114]\n",
      "[0.14100475]\n",
      "[0.09565693]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.7856, -0.1464, -0.4788, -0.4424,  0.6392,  0.4986,  0.8500],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -0.6824, -20.1109,  -0.5979,  -0.3950],\n",
      "         [ -1.1877,  -2.6534, -10.4843,  -1.5772,  -1.2840],\n",
      "         [ -2.8644,  -0.9098, -72.2828,  -8.8736,  -1.6200],\n",
      "         [ -1.2723,  -0.9967,  -1.4569, -15.5815,  -2.3964],\n",
      "         [ -1.5888,  -1.7323,  -0.9497,  -2.1775,  -1.9525],\n",
      "         [ -5.3403,  -8.6466,  -8.7190,  -4.1631,  -0.6713],\n",
      "         [  0.2379,  -0.2318,  -9.0961,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.37187161]\n",
      "[0.14164157]\n",
      "[0.11595056]\n",
      "[0.44432908]\n",
      "[0.03590506]\n",
      "[0.44437426]\n",
      "[0.0212642]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-3.5449, -3.9422, -6.4219, -0.1840, -0.7939, -2.9104,  0.2943],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1109,  -0.5979,  -0.3950],\n",
      "         [ -1.1877,  -3.3354, -10.4843,  -1.5772,  -1.2840],\n",
      "         [ -2.8644,  -2.0310, -72.2828,  -8.8736,  -1.6200],\n",
      "         [ -1.2723,  -1.0388,  -1.4569, -15.5815,  -2.3964],\n",
      "         [ -1.5888,  -1.8891,  -0.9497,  -2.1775,  -1.9525],\n",
      "         [ -5.3403,  -9.1803,  -8.7190,  -4.1631,  -0.6713],\n",
      "         [  0.2379,  -0.1890,  -9.0961,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.0870387]\n",
      "[0.09508985]\n",
      "[0.11081348]\n",
      "[0.53238232]\n",
      "[1.89823918]\n",
      "[1.70403277]\n",
      "[0.25674953]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.0134,  0.4340, -3.0033, -1.6794, -0.3272,  0.4908,  0.9357],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1128,  -0.5979,  -0.3950],\n",
      "         [ -1.1877,  -3.3354, -10.4144,  -1.5772,  -1.2840],\n",
      "         [ -2.8644,  -2.0310, -72.7936,  -8.8736,  -1.6200],\n",
      "         [ -1.2723,  -1.0388,  -1.7385, -15.5815,  -2.3964],\n",
      "         [ -1.5888,  -1.8891,  -1.0156,  -2.1775,  -1.9525],\n",
      "         [ -5.3403,  -9.1803,  -8.6396,  -4.1631,  -0.6713],\n",
      "         [  0.2379,  -0.1890,  -8.9415,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.44508698]\n",
      "[0.08639242]\n",
      "[0.22692524]\n",
      "[0.48468619]\n",
      "[0.32815288]\n",
      "[0.21465635]\n",
      "[0.03568206]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -6.7260,   0.3728,  -2.8444,   0.5183,  -6.7835, -80.8901,  -2.4390],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1128,  -1.7299,  -0.3950],\n",
      "         [ -1.1877,  -3.3354, -10.4144,  -1.5197,  -1.2840],\n",
      "         [ -2.8644,  -2.0310, -72.7936,  -9.3572,  -1.6200],\n",
      "         [ -1.2723,  -1.0388,  -1.7385, -15.4982,  -2.3964],\n",
      "         [ -1.5888,  -1.8891,  -1.0156,  -3.3190,  -1.9525],\n",
      "         [ -5.3403,  -9.1803,  -8.6396, -17.6866,  -0.6713],\n",
      "         [  0.2379,  -0.1890,  -8.9415,  -2.6413,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.24969766]\n",
      "[0.05726541]\n",
      "[0.0794225]\n",
      "[0.2864663]\n",
      "[1.93992311]\n",
      "[1.17877238]\n",
      "[0.26881902]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.2933,  0.1265, -3.6262, -2.7459, -5.8919, -0.6997,  0.4026],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1128,  -1.7299,  -0.3805],\n",
      "         [ -1.1877,  -3.3354, -10.4144,  -1.5197,  -1.2777],\n",
      "         [ -2.8644,  -2.0310, -72.7936,  -9.3572,  -2.3357],\n",
      "         [ -1.2723,  -1.0388,  -1.7385, -15.4982,  -2.8747],\n",
      "         [ -1.5888,  -1.8891,  -1.0156,  -3.3190,  -2.9550],\n",
      "         [ -5.3403,  -9.1803,  -8.6396, -17.6866,  -0.8475],\n",
      "         [  0.2379,  -0.1890,  -8.9415,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.51690834]\n",
      "[0.32208738]\n",
      "[0.26289084]\n",
      "[0.5283942]\n",
      "[0.19525998]\n",
      "[0.49129982]\n",
      "[0.1694067]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6805,  0.4406, -0.3171,  0.8583, -9.5466, -1.4022,  0.6088],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.3061, -20.1128,  -1.7299,  -0.3805],\n",
      "         [ -1.1192,  -3.3354, -10.4144,  -1.5197,  -1.2777],\n",
      "         [ -2.9225,  -2.0310, -72.7936,  -9.3572,  -2.3357],\n",
      "         [ -1.1343,  -1.0388,  -1.7385, -15.4982,  -2.8747],\n",
      "         [ -3.1955,  -1.8891,  -1.0156,  -3.3190,  -2.9550],\n",
      "         [ -5.5799,  -9.1803,  -8.6396, -17.6866,  -0.8475],\n",
      "         [  0.3381,  -0.1890,  -8.9415,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.16722734]\n",
      "[1.01227756]\n",
      "[0.70057038]\n",
      "[1.06654775]\n",
      "[0.39109629]\n",
      "[0.60871071]\n",
      "[0.01950765]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.7626,  0.6268, -0.0332,  0.3281, -4.1906, -1.2206,  0.2983],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.1128,  -1.7299,  -0.3805],\n",
      "         [ -1.1192,  -3.2352, -10.4144,  -1.5197,  -1.2777],\n",
      "         [ -2.9225,  -2.0566, -72.7936,  -9.3572,  -2.3357],\n",
      "         [ -1.1343,  -0.9854,  -1.7385, -15.4982,  -2.8747],\n",
      "         [ -3.1955,  -2.6328,  -1.0156,  -3.3190,  -2.9550],\n",
      "         [ -5.5799,  -9.3941,  -8.6396, -17.6866,  -0.8475],\n",
      "         [  0.3381,  -0.1431,  -8.9415,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.89446774]\n",
      "[1.0803753]\n",
      "[0.90863949]\n",
      "[0.78449876]\n",
      "[1.19923744]\n",
      "[1.01769571]\n",
      "[0.82837117]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -5.2869,  -2.6783,  -6.1776,  -1.6678,  -0.3701, -24.8489,  -4.3627],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -1.7299,  -0.3805],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.5197,  -1.2777],\n",
      "         [ -2.9225,  -2.0566, -73.8253,  -9.3572,  -2.3357],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -15.4982,  -2.8747],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -3.3190,  -2.9550],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.6866,  -0.8475],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.80224752]\n",
      "[0.82240349]\n",
      "[0.40737712]\n",
      "[0.61016257]\n",
      "[0.84480228]\n",
      "[0.87678608]\n",
      "[0.49163966]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -8.1717,  -0.6281, -31.2821,  -7.4206,  -5.4186,  -1.7935,  -3.8388],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -0.3805],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.2777],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -2.3357],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -2.8747],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9550],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -0.8475],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.79953251]\n",
      "[0.93108516]\n",
      "[0.33224576]\n",
      "[0.97285615]\n",
      "[0.61338463]\n",
      "[0.79583521]\n",
      "[0.33812692]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-6.7244, -0.9733, -5.6126, -1.5493, -0.0678, -4.6641, -0.8753],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06753412]\n",
      "[-0.00210624]\n",
      "[0.12207116]\n",
      "[1.73398405]\n",
      "[-0.00217742]\n",
      "[0.08100743]\n",
      "[0.09760005]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8407, 0.8562, 0.8567, 0.8610, 0.6903, 0.6076, 0.9493],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.1365,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1396,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1344,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1378,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1128,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0884,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1567,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.48678024]\n",
      "[0.02027401]\n",
      "[0.04018183]\n",
      "[0.8352402]\n",
      "[0.31689832]\n",
      "[0.04516649]\n",
      "[0.11265719]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7549, 0.4645, 0.7203, 0.8640, 0.6839, 0.7537, 0.9336],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.1365,   0.1224,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1396,   0.0748,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1344,   0.1160,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1378,   0.1406,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1128,   0.1114,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0884,   0.1194,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1567,   0.1537,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.02988786]\n",
      "[0.01539893]\n",
      "[0.02362704]\n",
      "[0.14436269]\n",
      "[0.03109449]\n",
      "[0.08020165]\n",
      "[0.09087409]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.3667,  0.4313,  0.2338,  0.5438,  0.5501, -0.2800,  0.9027],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 1.3647e-01,  1.2243e-01,  4.4471e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3964e-01,  7.4815e-02,  4.4581e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3443e-01,  1.1603e-01, -5.1384e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3778e-01,  1.4057e-01,  5.8175e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1284e-01,  1.1141e-01,  7.3503e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 8.8413e-02,  1.1937e-01, -7.4798e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.5669e-01,  1.5369e-01,  1.4673e-01,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.08993642]\n",
      "[0.0593278]\n",
      "[0.07933902]\n",
      "[0.72921224]\n",
      "[0.03813742]\n",
      "[0.14108795]\n",
      "[0.20134647]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8749, 0.7657, 0.8487, 0.9103, 0.7130, 0.7869, 0.9638],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 1.3647e-01,  1.2243e-01,  4.4471e-02,  1.3846e-01,  0.0000e+00],\n",
      "         [ 1.3964e-01,  7.4815e-02,  4.4581e-02,  1.1398e-01,  0.0000e+00],\n",
      "         [ 1.3443e-01,  1.1603e-01, -5.1384e-03,  1.3440e-01,  0.0000e+00],\n",
      "         [ 1.3778e-01,  1.4057e-01,  5.8175e-02,  1.4664e-01,  0.0000e+00],\n",
      "         [ 1.1284e-01,  1.1141e-01,  7.3503e-02,  1.1276e-01,  0.0000e+00],\n",
      "         [ 8.8413e-02,  1.1937e-01, -7.4798e-02,  1.1775e-01,  0.0000e+00],\n",
      "         [ 1.5669e-01,  1.5369e-01,  1.4673e-01,  1.5928e-01,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.48877575]\n",
      "[0.28176903]\n",
      "[0.33036919]\n",
      "[0.64979236]\n",
      "[0.06505375]\n",
      "[0.33640006]\n",
      "[0.02875275]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7660, 0.7687, 0.5146, 0.7317, 0.6976, 0.7185, 0.9459],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 1.3647e-01,  1.2243e-01,  4.4471e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 1.3964e-01,  7.4815e-02,  4.4581e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [ 1.3443e-01,  1.1603e-01, -5.1384e-03,  1.3440e-01,  7.7610e-02],\n",
      "         [ 1.3778e-01,  1.4057e-01,  5.8175e-02,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.1284e-01,  1.1141e-01,  7.3503e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 8.8413e-02,  1.1937e-01, -7.4798e-02,  1.1775e-01,  9.9964e-02],\n",
      "         [ 1.5669e-01,  1.5369e-01,  1.4673e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.06462405]\n",
      "[0.02361037]\n",
      "[0.06712862]\n",
      "[0.81683831]\n",
      "[0.02210141]\n",
      "[0.10578768]\n",
      "[0.13772177]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.8296,  0.9020, -0.7002,  0.8115,  0.5256,  0.6164,  0.9526],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  1.2243e-01,  4.4471e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  7.4815e-02,  4.4581e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  1.1603e-01, -5.1384e-03,  1.3440e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  1.4057e-01,  5.8175e-02,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  1.1141e-01,  7.3503e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.1937e-01, -7.4798e-02,  1.1775e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  1.5369e-01,  1.4673e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.26103902]\n",
      "[0.10827204]\n",
      "[0.58715234]\n",
      "[1.69695888]\n",
      "[0.23418756]\n",
      "[0.95062301]\n",
      "[0.04678736]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7548, 0.4550, 0.7461, 0.6447, 0.7267, 0.5476, 0.8277],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  4.4471e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  4.4581e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -5.1384e-03,  1.3440e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  2.1950e-01,  5.8175e-02,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  7.3503e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -7.4798e-02,  1.1775e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  2.8409e-01,  1.4673e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04337353]\n",
      "[0.02727942]\n",
      "[0.02011746]\n",
      "[0.52408721]\n",
      "[0.00855577]\n",
      "[0.15192741]\n",
      "[0.06487831]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.0781,  0.3257, -0.0357,  0.7626,  0.2667, -0.5539,  0.7664],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  3.3457e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  8.0757e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -4.7600e-02,  1.3440e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  2.1950e-01,  1.6940e-01,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  8.9606e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -2.6397e-01,  1.1775e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  2.8409e-01,  2.6808e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[2.09295063]\n",
      "[1.18465052]\n",
      "[0.65564006]\n",
      "[1.70624804]\n",
      "[0.48156195]\n",
      "[0.94541316]\n",
      "[0.03833585]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.4250,  0.6683,  0.1773,  0.8247, -0.6545,  0.2295,  0.8998],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  3.3457e-02,  1.5817e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  8.0757e-02,  1.8391e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -4.7600e-02,  1.0439e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  2.1950e-01,  1.6940e-01,  2.7009e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  8.9606e-02, -6.9823e-02,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -2.6397e-01,  1.3101e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  2.8409e-01,  2.6808e-01,  2.9374e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.69875276]\n",
      "[1.60643547]\n",
      "[0.67117455]\n",
      "[1.59227076]\n",
      "[0.51694791]\n",
      "[0.78852496]\n",
      "[0.04686707]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6968, 0.7176, 0.6032, 0.7515, 0.6070, 0.5912, 0.9045],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  3.3457e-02,  1.5817e-01,  2.2860e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  8.0757e-02,  1.8391e-01,  2.3139e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -4.7600e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 2.7086e-01,  2.1950e-01,  1.6940e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  8.9606e-02, -6.9823e-02,  1.9659e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -2.6397e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 3.1440e-01,  2.8409e-01,  2.6808e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.83003266]\n",
      "[0.71995571]\n",
      "[0.61962114]\n",
      "[0.94127292]\n",
      "[0.51911389]\n",
      "[0.46526092]\n",
      "[0.24697493]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8382, 0.6012, 0.6842, 0.7191, 0.7742, 0.6989, 0.9555],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  2.3537e-01,  3.3457e-02,  1.5817e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  1.3773e-01,  8.0757e-02,  1.8391e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  2.2478e-01, -4.7600e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  2.1950e-01,  1.6940e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  2.1585e-01,  8.9606e-02, -6.9823e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  1.8685e-01, -2.6397e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  2.8409e-01,  2.6808e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.23707335]\n",
      "[0.10378951]\n",
      "[0.37566919]\n",
      "[1.41906959]\n",
      "[0.13401207]\n",
      "[0.77640194]\n",
      "[0.03246373]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8923, 0.8154, 0.8694, 0.9536, 0.6640, 0.8303, 0.9440],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  3.3457e-02,  1.5817e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  8.0757e-02,  1.8391e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01, -4.7600e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  1.6940e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  8.9606e-02, -6.9823e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -2.6397e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  2.6808e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.57861342]\n",
      "[0.91600027]\n",
      "[0.25470186]\n",
      "[1.01955743]\n",
      "[0.22283571]\n",
      "[0.67522717]\n",
      "[0.0132134]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9437, 0.9495, 0.6459, 0.8545, 0.3448, 0.7684, 0.9483],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  1.8725e-01,  1.5817e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  2.3618e-01,  1.8391e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01,  4.7080e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  3.0460e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  1.1607e-01, -6.9823e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -1.5410e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  4.2270e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.88341227]\n",
      "[0.99833322]\n",
      "[0.73622344]\n",
      "[0.95410813]\n",
      "[1.02881117]\n",
      "[1.0145263]\n",
      "[0.4934236]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8307, 0.1125, 0.6815, 0.8036, 0.3758, 0.7595, 0.9240],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  1.8725e-01,  2.8369e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  2.3618e-01,  1.7121e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01,  4.7080e-02,  2.0329e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  3.0460e-01,  3.8475e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  1.1607e-01, -1.9391e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -1.5410e-01,  2.4041e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  4.2270e-01,  4.4218e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.08075962]\n",
      "[0.04321774]\n",
      "[0.03718721]\n",
      "[1.5052973]\n",
      "[0.02310004]\n",
      "[0.22014848]\n",
      "[0.02568086]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6981, 0.6090, 0.9006, 0.8804, 0.7662, 0.6590, 0.8878],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  1.8725e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  2.3618e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01,  4.7080e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  3.0460e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  1.1607e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -1.5410e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  4.2270e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.82060382]\n",
      "[1.41383176]\n",
      "[0.8076784]\n",
      "[1.53998309]\n",
      "[0.17211368]\n",
      "[0.56394629]\n",
      "[0.02078767]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.1684,  0.6066, -0.7596,  0.5446,  0.6013, -0.7934,  0.9494],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  3.7885e-01,  1.8725e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  2.6410e-01,  2.3618e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [-6.1993e-02,  3.6795e-01,  4.7080e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  3.7623e-01,  3.0460e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  3.1166e-01,  1.1607e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.1650e-01, -1.5410e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  4.3976e-01,  4.2270e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04141875]\n",
      "[0.10675017]\n",
      "[0.01571136]\n",
      "[0.24378251]\n",
      "[0.03251447]\n",
      "[0.02379043]\n",
      "[0.88771597]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.5111, 0.6064, 0.4446, 0.5019, 0.6149, 0.5240, 0.9459],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  1.8725e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  2.3618e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  4.7080e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  3.0460e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.1607e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.5410e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  4.2270e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.05137508]\n",
      "[0.02211621]\n",
      "[0.05135512]\n",
      "[0.33266055]\n",
      "[0.00455116]\n",
      "[0.07750037]\n",
      "[0.13797808]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.2987, 0.8613, 0.4983, 0.7361, 0.5682, 0.2564, 0.9723],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  2.2406e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  3.6306e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  9.6470e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  4.1571e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.9738e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.7137e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  5.8261e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.07082808]\n",
      "[0.20255007]\n",
      "[0.01985003]\n",
      "[0.45526603]\n",
      "[0.03528001]\n",
      "[0.00630587]\n",
      "[0.96372545]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6672, -0.0646,  0.5902,  0.7543,  0.8379,  0.7791,  0.8015],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  2.2406e-01,  3.7162e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  3.6306e-01,  7.6302e-02,  3.2470e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  9.6470e-02,  2.8790e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  4.1571e-01,  5.0056e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.9738e-01,  1.0706e-01,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.7137e-01,  3.5075e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  5.8261e-01,  5.6287e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.12248166]\n",
      "[0.14522436]\n",
      "[0.05182291]\n",
      "[0.25815277]\n",
      "[0.0280159]\n",
      "[0.32991433]\n",
      "[0.05299037]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6561,  0.7694,  0.5410, -0.3339,  0.5072,  0.4655,  0.9534],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  2.2406e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  3.6306e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  9.6470e-02,  2.8790e-01,  3.6956e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  4.1571e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.9738e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.7137e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  5.8261e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.09488247]\n",
      "[0.01692163]\n",
      "[0.08421654]\n",
      "[0.41055438]\n",
      "[0.01690857]\n",
      "[0.20595028]\n",
      "[0.09394705]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7798, 0.8675, 0.5377, 0.6201, 0.6789, 0.7009, 0.9577],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  4.4555e-01,  2.2406e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  3.4504e-01,  3.6306e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-9.4632e-03,  4.3197e-01,  9.6470e-02,  2.8790e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  4.4103e-01,  4.1571e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.0261e-01,  1.9738e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  3.8834e-01, -1.7137e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  5.9563e-01,  5.8261e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.09812788]\n",
      "[0.25023948]\n",
      "[0.01823575]\n",
      "[0.3286931]\n",
      "[0.09067584]\n",
      "[0.10988243]\n",
      "[0.82384686]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8174, 0.7679, 0.0915, 0.7960, 0.6468, 0.5616, 0.8881],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  2.2406e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  3.6306e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-9.4632e-03,  3.9527e-01,  9.6470e-02,  2.8790e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  4.1571e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  1.9738e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -1.7137e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  5.8261e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.09028549]\n",
      "[0.12490922]\n",
      "[0.18185536]\n",
      "[0.56804216]\n",
      "[1.76755795]\n",
      "[1.18830307]\n",
      "[0.33671865]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6760,  0.8462, -1.7127,  0.8133,  0.8059, -0.0206,  0.9167],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  3.2971e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  4.8956e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-9.4632e-03,  3.9527e-01, -3.2967e-01,  2.8790e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  5.4350e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  3.1981e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -2.3961e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  7.3260e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.6926658]\n",
      "[0.03797917]\n",
      "[0.05775997]\n",
      "[0.5446889]\n",
      "[1.20985186]\n",
      "[-0.20198648]\n",
      "[0.16475993]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.3742,  0.4867, -0.3117,  0.5371,  0.7218,  0.5683,  0.8573],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  3.2971e-01,  4.0932e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  4.8956e-01,  1.4858e-01,  4.4281e-01],\n",
      "         [-9.4632e-03,  3.9527e-01, -3.2967e-01,  2.0320e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  5.4350e-01,  5.7459e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  3.1981e-01,  2.1596e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -2.3961e-01,  4.3001e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  7.3260e-01,  7.0037e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.44105107]\n",
      "[0.0960249]\n",
      "[0.24474813]\n",
      "[0.56819853]\n",
      "[0.25665673]\n",
      "[0.35245889]\n",
      "[0.05094847]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6629, 0.7896, 0.0682, 0.7391, 0.7131, 0.4323, 0.8616],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  3.2971e-01,  4.0932e-01,  5.2554e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  4.8956e-01,  1.4858e-01,  5.6294e-01],\n",
      "         [-9.4632e-03,  3.9527e-01, -3.2967e-01,  2.0320e-01,  3.0674e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  5.4350e-01,  5.7459e-01,  3.8597e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  3.1981e-01,  2.1596e-01,  4.7452e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -2.3961e-01,  4.3001e-01,  3.7914e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  7.3260e-01,  7.0037e-01,  7.4204e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.91908208]\n",
      "[0.12446064]\n",
      "[0.16673474]\n",
      "[0.61202213]\n",
      "[0.93795764]\n",
      "[1.11684642]\n",
      "[0.29967482]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6715,  0.3942,  0.6644,  0.7423, -0.2034,  0.1459,  0.9222],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.5716,   0.3297,   0.4093,   0.5255],\n",
      "         [  0.6315,   0.4512,   0.4896,   0.1486,   0.5629],\n",
      "         [  0.0954,   0.3953,  -0.3297,   0.2032,   0.3067],\n",
      "         [  0.6531,   0.5624,   0.5435,   0.5746,   0.3860],\n",
      "         [  0.4433,   0.4978,   0.3198,   0.2160,   0.4745],\n",
      "         [  0.2356,   0.4622,  -0.2396,   0.4300,   0.3791],\n",
      "         [  0.9316,   0.7399,   0.7326,   0.7004,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.23511732]\n",
      "[0.17017639]\n",
      "[0.13213762]\n",
      "[0.51857252]\n",
      "[0.05148525]\n",
      "[0.71146455]\n",
      "[0.23378105]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9372, 0.8511, 0.6386, 0.9472, 0.7328, 0.8174, 0.9275],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.3297,   0.4093,   0.5255],\n",
      "         [  0.6315,   0.5897,   0.4896,   0.1486,   0.5629],\n",
      "         [  0.0954,   0.4934,  -0.3297,   0.2032,   0.3067],\n",
      "         [  0.6531,   0.7175,   0.5435,   0.5746,   0.3860],\n",
      "         [  0.4433,   0.5996,   0.3198,   0.2160,   0.4745],\n",
      "         [  0.2356,   0.5903,  -0.2396,   0.4300,   0.3791],\n",
      "         [  0.9316,   0.8929,   0.7326,   0.7004,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.30210599]\n",
      "[0.15081581]\n",
      "[0.34926467]\n",
      "[1.53539121]\n",
      "[0.22717496]\n",
      "[0.86608779]\n",
      "[0.09347999]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7161, 0.5245, 0.7043, 0.4398, 0.4714, 0.6571, 0.8375],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.4093,   0.5255],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.1486,   0.5629],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.2032,   0.3067],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.5746,   0.3860],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2160,   0.4745],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.4300,   0.3791],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.7004,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09138154]\n",
      "[0.03540537]\n",
      "[0.08780202]\n",
      "[1.20354667]\n",
      "[0.02027068]\n",
      "[0.30542021]\n",
      "[0.13182698]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7606, 0.6734, 0.8773, 0.8804, 0.2730, 0.7063, 0.8900],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.5255],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.5629],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.3067],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.3860],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.4745],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.3791],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.48618612]\n",
      "[1.9342872]\n",
      "[0.62206129]\n",
      "[1.58279976]\n",
      "[0.37614649]\n",
      "[1.09738457]\n",
      "[0.05123414]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8968, 0.8808, 0.8821, 0.9282, 0.8509, 0.7390, 0.9516],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.98745344]\n",
      "[0.78818604]\n",
      "[0.62929433]\n",
      "[0.72426986]\n",
      "[0.83692937]\n",
      "[0.98002925]\n",
      "[0.07136138]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8659, 0.7132, 0.8775, 0.9047, 0.8301, 0.8224, 0.9806],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.1400,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1076,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1423,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1454,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1329,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1208,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1623,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.51240692]\n",
      "[0.24831183]\n",
      "[0.37468364]\n",
      "[0.58364706]\n",
      "[0.10586649]\n",
      "[0.33473775]\n",
      "[0.02583257]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.4312, 0.6101, 0.5821, 0.7286, 0.5345, 0.0747, 0.9264],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0755e-01,  9.0513e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4231e-01,  8.8764e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4543e-01,  1.1111e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3288e-01,  8.4816e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.2082e-01, -9.2914e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.6235e-01,  1.5242e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.27885423]\n",
      "[0.30074886]\n",
      "[0.17229]\n",
      "[0.51758273]\n",
      "[-0.00279943]\n",
      "[0.1460552]\n",
      "[0.02331558]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6766, 0.4777, 0.5617, 0.8664, 0.7949, 0.6991, 0.9361],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  1.0238e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0755e-01,  9.0513e-02,  6.3331e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4231e-01,  8.8764e-02,  8.5693e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4543e-01,  1.1111e-01,  1.3813e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3288e-01,  8.4816e-02,  1.2298e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.2082e-01, -9.2914e-03,  1.0193e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.6235e-01,  1.5242e-01,  1.5237e-01,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.42902482]\n",
      "[0.37873339]\n",
      "[0.21442044]\n",
      "[0.53883205]\n",
      "[0.09856434]\n",
      "[0.27081597]\n",
      "[0.02164655]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8972, 0.9246, 0.8116, 0.8852, 0.8163, 0.5208, 0.9693],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  1.0238e-01,  1.4640e-01,  0.0000e+00],\n",
      "         [ 1.0755e-01,  9.0513e-02,  6.3331e-02,  1.5045e-01,  0.0000e+00],\n",
      "         [ 1.4231e-01,  8.8764e-02,  8.5693e-02,  1.2881e-01,  0.0000e+00],\n",
      "         [ 1.4543e-01,  1.1111e-01,  1.3813e-01,  1.4534e-01,  0.0000e+00],\n",
      "         [ 1.3288e-01,  8.4816e-02,  1.2298e-01,  1.2649e-01,  0.0000e+00],\n",
      "         [ 1.2082e-01, -9.2914e-03,  1.0193e-01,  7.3380e-02,  0.0000e+00],\n",
      "         [ 1.6235e-01,  1.5242e-01,  1.5237e-01,  1.5963e-01,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.77972167]\n",
      "[0.04185706]\n",
      "[0.05504222]\n",
      "[0.85006257]\n",
      "[0.45297707]\n",
      "[0.23292363]\n",
      "[0.17048108]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7586, 0.8113, 0.6494, 0.6613, 0.7570, 0.7395, 0.9407],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  1.0238e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 1.0755e-01,  9.0513e-02,  6.3331e-02,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.4231e-01,  8.8764e-02,  8.5693e-02,  1.2881e-01,  9.0697e-02],\n",
      "         [ 1.4543e-01,  1.1111e-01,  1.3813e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.3288e-01,  8.4816e-02,  1.2298e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.2082e-01, -9.2914e-03,  1.0193e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 1.6235e-01,  1.5242e-01,  1.5237e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.08310563]\n",
      "[0.02224516]\n",
      "[0.11186942]\n",
      "[0.93537436]\n",
      "[0.02683095]\n",
      "[0.10724435]\n",
      "[0.2124614]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8818, 0.8916, 0.6957, 0.8023, 0.7004, 0.7234, 0.9560],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  6.5241e-02,  1.0238e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  9.0513e-02,  6.3331e-02,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  8.8764e-02,  8.5693e-02,  1.2881e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  1.1111e-01,  1.3813e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  8.4816e-02,  1.2298e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01, -9.2914e-03,  1.0193e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 3.1763e-01,  1.5242e-01,  1.5237e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.96829841]\n",
      "[0.67973381]\n",
      "[0.86813464]\n",
      "[1.03659005]\n",
      "[0.79922714]\n",
      "[0.90091577]\n",
      "[0.36533729]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7631, 0.8514, 0.7868, 0.8971, 0.8165, 0.6077, 0.8484],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  1.8651e-01,  1.0238e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  2.2641e-01,  6.3331e-02,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  2.0662e-01,  8.5693e-02,  1.2881e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  2.5579e-01,  1.3813e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  2.1325e-01,  1.2298e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01,  7.7273e-02,  1.0193e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 3.1763e-01,  2.9208e-01,  1.5237e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.99911411]\n",
      "[0.83352295]\n",
      "[0.68580816]\n",
      "[1.03452712]\n",
      "[0.62724668]\n",
      "[0.81040276]\n",
      "[0.45060351]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8535, 0.9623, 0.6337, 0.9282, 0.8549, 0.7184, 0.9483],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  1.8651e-01,  2.3341e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  2.2641e-01,  2.1703e-01,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  2.0662e-01,  1.4587e-01,  1.2881e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  2.5579e-01,  2.8692e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  2.1325e-01,  2.5000e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01,  7.7273e-02,  1.4833e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 3.1763e-01,  2.9208e-01,  3.0503e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.99008054]\n",
      "[0.75955093]\n",
      "[0.90181191]\n",
      "[1.08290073]\n",
      "[0.66908586]\n",
      "[0.87377897]\n",
      "[0.85579921]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6256, 0.5669, 0.5602, 0.7934, 0.6893, 0.7779, 0.8514],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  1.8651e-01,  2.3341e-01,  2.2532e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  2.2641e-01,  2.1703e-01,  2.1723e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  2.0662e-01,  1.4587e-01,  1.7655e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  2.5579e-01,  2.8692e-01,  2.5699e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  2.1325e-01,  2.5000e-01,  2.1505e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01,  7.7273e-02,  1.4833e-01,  1.7990e-01,  1.1181e-01],\n",
      "         [ 3.1763e-01,  2.9208e-01,  3.0503e-01,  2.9089e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.13538554]\n",
      "[0.05541719]\n",
      "[0.08774401]\n",
      "[0.63937453]\n",
      "[0.02304463]\n",
      "[0.25712527]\n",
      "[0.12494203]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7696, 0.8001, 0.7304, 0.8206, 0.6600, 0.6552, 0.8729],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.2636,   0.1865,   0.2334,   0.2253,   0.2177],\n",
      "         [  0.2520,   0.2264,   0.2170,   0.2172,   0.2298],\n",
      "         [  0.1772,   0.2066,   0.1459,   0.1765,   0.1744],\n",
      "         [  0.2710,   0.2558,   0.2869,   0.2570,   0.1867],\n",
      "         [  0.1978,   0.2132,   0.2500,   0.2150,   0.2054],\n",
      "         [  0.1370,   0.0773,   0.1483,   0.1799,   0.1856],\n",
      "         [  0.3176,   0.2921,   0.3050,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.17571526]\n",
      "[0.1534348]\n",
      "[0.0693115]\n",
      "[1.2772532]\n",
      "[0.01043046]\n",
      "[0.35402076]\n",
      "[0.09168679]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8970, 0.8681, 0.8340, 0.8871, 0.7740, 0.8329, 0.9772],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.1865,   0.2334,   0.2253,   0.2177],\n",
      "         [  0.3871,   0.2264,   0.2170,   0.2172,   0.2298],\n",
      "         [  0.3070,   0.2066,   0.1459,   0.1765,   0.1744],\n",
      "         [  0.4114,   0.2558,   0.2869,   0.2570,   0.1867],\n",
      "         [  0.3121,   0.2132,   0.2500,   0.2150,   0.2054],\n",
      "         [  0.2624,   0.0773,   0.1483,   0.1799,   0.1856],\n",
      "         [  0.4756,   0.2921,   0.3050,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.15982503]\n",
      "[0.13531295]\n",
      "[0.05424655]\n",
      "[0.72425222]\n",
      "[0.03033151]\n",
      "[0.47158145]\n",
      "[0.06093718]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9358, 0.9520, 0.8415, 0.9765, 0.8965, 0.8365, 0.9491],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.2334,   0.2253,   0.2177],\n",
      "         [  0.3871,   0.3832,   0.2170,   0.2172,   0.2298],\n",
      "         [  0.3070,   0.3420,   0.1459,   0.1765,   0.1744],\n",
      "         [  0.4114,   0.4168,   0.2869,   0.2570,   0.1867],\n",
      "         [  0.3121,   0.3558,   0.2500,   0.2150,   0.2054],\n",
      "         [  0.2624,   0.2041,   0.1483,   0.1799,   0.1856],\n",
      "         [  0.4756,   0.4486,   0.3050,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.85899958]\n",
      "[1.03022344]\n",
      "[0.87044259]\n",
      "[0.85259463]\n",
      "[0.87653842]\n",
      "[0.66081174]\n",
      "[0.30186609]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9388, 0.8896, 0.8950, 0.9436, 0.7592, 0.7619, 0.9562],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.3854,   0.2253,   0.2177],\n",
      "         [  0.3871,   0.3832,   0.3581,   0.2172,   0.2298],\n",
      "         [  0.3070,   0.3420,   0.2874,   0.1765,   0.1744],\n",
      "         [  0.4114,   0.4168,   0.4361,   0.2570,   0.1867],\n",
      "         [  0.3121,   0.3558,   0.3587,   0.2150,   0.2054],\n",
      "         [  0.2624,   0.2041,   0.2628,   0.1799,   0.1856],\n",
      "         [  0.4756,   0.4486,   0.4633,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.50231991]\n",
      "[0.24500941]\n",
      "[0.22639448]\n",
      "[1.32753334]\n",
      "[0.17693265]\n",
      "[0.53469883]\n",
      "[0.02546377]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9460, 0.8948, 0.9334, 0.9670, 0.8605, 0.7076, 0.9725],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.3854,   0.3807,   0.2177],\n",
      "         [  0.3871,   0.3832,   0.3581,   0.3640,   0.2298],\n",
      "         [  0.3070,   0.3420,   0.2874,   0.3293,   0.1744],\n",
      "         [  0.4114,   0.4168,   0.4361,   0.4156,   0.1867],\n",
      "         [  0.3121,   0.3558,   0.3587,   0.3488,   0.2054],\n",
      "         [  0.2624,   0.2041,   0.2628,   0.2903,   0.1856],\n",
      "         [  0.4756,   0.4486,   0.4633,   0.4507,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.14262296]\n",
      "[1.01019498]\n",
      "[1.08864671]\n",
      "[1.10042053]\n",
      "[0.96874237]\n",
      "[0.93047191]\n",
      "[0.13427213]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9035, 0.8833, 0.8340, 0.8959, 0.6427, 0.8438, 0.9515],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.3854,   0.3807,   0.3661],\n",
      "         [  0.3871,   0.3832,   0.3581,   0.3640,   0.3671],\n",
      "         [  0.3070,   0.3420,   0.2874,   0.3293,   0.3059],\n",
      "         [  0.4114,   0.4168,   0.4361,   0.4156,   0.3315],\n",
      "         [  0.3121,   0.3558,   0.3587,   0.3488,   0.2858],\n",
      "         [  0.2624,   0.2041,   0.2628,   0.2903,   0.3121],\n",
      "         [  0.4756,   0.4486,   0.4633,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.35075494]\n",
      "[0.41606634]\n",
      "[0.5565349]\n",
      "[1.31332552]\n",
      "[0.34563085]\n",
      "[0.6074622]\n",
      "[0.04450679]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7846, 0.4582, 0.6865, 0.8027, 0.7330, 0.7809, 0.9141],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.3402,   0.3854,   0.3807,   0.3661],\n",
      "         [  0.3986,   0.3832,   0.3581,   0.3640,   0.3671],\n",
      "         [  0.3874,   0.3420,   0.2874,   0.3293,   0.3059],\n",
      "         [  0.5159,   0.4168,   0.4361,   0.4156,   0.3315],\n",
      "         [  0.4087,   0.3558,   0.3587,   0.3488,   0.2858],\n",
      "         [  0.3332,   0.2041,   0.2628,   0.2903,   0.3121],\n",
      "         [  0.6017,   0.4486,   0.4633,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.14152322]\n",
      "[0.06740237]\n",
      "[0.1364127]\n",
      "[0.56601328]\n",
      "[0.05883992]\n",
      "[0.19506315]\n",
      "[0.22509392]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8828, 0.7505, 0.7856, 0.7402, 0.7088, 0.7364, 0.9298],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.3854,   0.3807,   0.3661],\n",
      "         [  0.3986,   0.4947,   0.3581,   0.3640,   0.3671],\n",
      "         [  0.3874,   0.4637,   0.2874,   0.3293,   0.3059],\n",
      "         [  0.5159,   0.5298,   0.4361,   0.4156,   0.3315],\n",
      "         [  0.4087,   0.4644,   0.3587,   0.3488,   0.2858],\n",
      "         [  0.3332,   0.3144,   0.2628,   0.2903,   0.3121],\n",
      "         [  0.6017,   0.6014,   0.4633,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.0279155]\n",
      "[0.01489329]\n",
      "[0.04554824]\n",
      "[0.36788654]\n",
      "[0.0337967]\n",
      "[0.08192811]\n",
      "[0.13182323]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8284, 0.8763, 0.6891, 0.7373, 0.8632, 0.7571, 0.9387],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.5093,   0.3807,   0.3661],\n",
      "         [  0.3986,   0.4947,   0.4759,   0.3640,   0.3671],\n",
      "         [  0.3874,   0.4637,   0.3802,   0.3293,   0.3059],\n",
      "         [  0.5159,   0.5298,   0.5389,   0.4156,   0.3315],\n",
      "         [  0.4087,   0.4644,   0.4875,   0.3488,   0.2858],\n",
      "         [  0.3332,   0.3144,   0.3737,   0.2903,   0.3121],\n",
      "         [  0.6017,   0.6014,   0.6141,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06113793]\n",
      "[0.02529342]\n",
      "[0.08884058]\n",
      "[0.6074095]\n",
      "[0.02386796]\n",
      "[0.11954872]\n",
      "[0.16155103]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6010, 0.8621, 0.6451, 0.7577, 0.8380, 0.6135, 0.9496],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.5093,   0.4698,   0.3661],\n",
      "         [  0.3986,   0.4947,   0.4759,   0.5020,   0.3671],\n",
      "         [  0.3874,   0.4637,   0.3802,   0.4071,   0.3059],\n",
      "         [  0.5159,   0.5298,   0.5389,   0.5290,   0.3315],\n",
      "         [  0.4087,   0.4644,   0.4875,   0.4785,   0.2858],\n",
      "         [  0.3332,   0.3144,   0.3737,   0.3789,   0.3121],\n",
      "         [  0.6017,   0.6014,   0.6141,   0.6056,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.08160334]\n",
      "[0.17058998]\n",
      "[0.01659732]\n",
      "[0.28581403]\n",
      "[0.0656862]\n",
      "[0.08260006]\n",
      "[0.89464241]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8139, 0.6923, 0.6506, 0.7958, 0.6478, 0.7022, 0.8843],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.5093,   0.4698,   0.4879],\n",
      "         [  0.3986,   0.4947,   0.4759,   0.5020,   0.4544],\n",
      "         [  0.3874,   0.4637,   0.3802,   0.4071,   0.3650],\n",
      "         [  0.5159,   0.5298,   0.5389,   0.5290,   0.4427],\n",
      "         [  0.4087,   0.4644,   0.4875,   0.4785,   0.3643],\n",
      "         [  0.3332,   0.3144,   0.3737,   0.3789,   0.4093],\n",
      "         [  0.6017,   0.6014,   0.6141,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.13193817]\n",
      "[0.08498847]\n",
      "[0.14848625]\n",
      "[0.58358746]\n",
      "[0.05535254]\n",
      "[0.14810907]\n",
      "[0.27001837]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8899, 0.8646, 0.6525, 0.6838, 0.6684, 0.7922, 0.9455],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.4837,   0.5093,   0.4698,   0.4879],\n",
      "         [  0.5314,   0.4947,   0.4759,   0.5020,   0.4544],\n",
      "         [  0.4564,   0.4637,   0.3802,   0.4071,   0.3650],\n",
      "         [  0.5921,   0.5298,   0.5389,   0.5290,   0.4427],\n",
      "         [  0.4965,   0.4644,   0.4875,   0.4785,   0.3643],\n",
      "         [  0.4566,   0.3144,   0.3737,   0.3789,   0.4093],\n",
      "         [  0.7567,   0.6014,   0.6141,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.59411328]\n",
      "[0.18380329]\n",
      "[0.21927824]\n",
      "[0.63189389]\n",
      "[0.27842823]\n",
      "[0.42977049]\n",
      "[0.11110902]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7495, 0.7657, 0.4465, 0.6535, 0.7659, 0.7676, 0.9555],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.5093,   0.4698,   0.4879],\n",
      "         [  0.5314,   0.5837,   0.4759,   0.5020,   0.4544],\n",
      "         [  0.4564,   0.5093,   0.3802,   0.4071,   0.3650],\n",
      "         [  0.5921,   0.6076,   0.5389,   0.5290,   0.4427],\n",
      "         [  0.4965,   0.5800,   0.4875,   0.4785,   0.3643],\n",
      "         [  0.4566,   0.4230,   0.3737,   0.3789,   0.4093],\n",
      "         [  0.7567,   0.7546,   0.6141,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.07773117]\n",
      "[0.32215365]\n",
      "[0.00910057]\n",
      "[0.33443339]\n",
      "[0.08131734]\n",
      "[0.05286492]\n",
      "[0.88095022]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8875, 0.8644, 0.5530, 0.7559, 0.7293, 0.5365, 0.9534],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.6430,   0.4698,   0.4879],\n",
      "         [  0.5314,   0.5837,   0.6127,   0.5020,   0.4544],\n",
      "         [  0.4564,   0.5093,   0.4194,   0.4071,   0.3650],\n",
      "         [  0.5921,   0.6076,   0.6445,   0.5290,   0.4427],\n",
      "         [  0.4965,   0.5800,   0.5928,   0.4785,   0.3643],\n",
      "         [  0.4566,   0.4230,   0.4404,   0.3789,   0.4093],\n",
      "         [  0.7567,   0.7546,   0.7697,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94458037]\n",
      "[0.10269996]\n",
      "[0.11034412]\n",
      "[0.57557063]\n",
      "[1.53330488]\n",
      "[0.82774625]\n",
      "[0.30612082]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8849, 0.7556, 0.2045, 0.7464, 0.8242, 0.6258, 0.9241],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.6430,   0.6042,   0.4879],\n",
      "         [  0.5314,   0.5837,   0.6127,   0.6179,   0.4544],\n",
      "         [  0.4564,   0.5093,   0.4194,   0.3929,   0.3650],\n",
      "         [  0.5921,   0.6076,   0.6445,   0.6342,   0.4427],\n",
      "         [  0.4965,   0.5800,   0.5928,   0.5974,   0.3643],\n",
      "         [  0.4566,   0.4230,   0.4404,   0.4544,   0.4093],\n",
      "         [  0.7567,   0.7546,   0.7697,   0.7547,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94791438]\n",
      "[0.12064157]\n",
      "[0.10697415]\n",
      "[0.62146225]\n",
      "[1.34537451]\n",
      "[1.16806383]\n",
      "[0.2872286]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7472, 0.8157, 0.0197, 0.7700, 0.8376, 0.1727, 0.9324],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.6430,   0.6042,   0.5939],\n",
      "         [  0.5314,   0.5837,   0.6127,   0.6179,   0.5770],\n",
      "         [  0.4564,   0.5093,   0.4194,   0.3929,   0.3182],\n",
      "         [  0.5921,   0.6076,   0.6445,   0.6342,   0.5502],\n",
      "         [  0.4965,   0.5800,   0.5928,   0.5974,   0.4897],\n",
      "         [  0.4566,   0.4230,   0.4404,   0.4544,   0.3535],\n",
      "         [  0.7567,   0.7546,   0.7697,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.78617008]\n",
      "[0.07334242]\n",
      "[0.08993421]\n",
      "[0.72552967]\n",
      "[1.17685006]\n",
      "[0.51686392]\n",
      "[0.28053973]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8563, 0.8728, 0.8491, 0.9598, 0.7814, 0.5975, 0.9390],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.5851,   0.6430,   0.6042,   0.5939],\n",
      "         [  0.6716,   0.5837,   0.6127,   0.6179,   0.5770],\n",
      "         [  0.5927,   0.5093,   0.4194,   0.3929,   0.3182],\n",
      "         [  0.7462,   0.6076,   0.6445,   0.6342,   0.5502],\n",
      "         [  0.6062,   0.5800,   0.5928,   0.5974,   0.4897],\n",
      "         [  0.5458,   0.4230,   0.4404,   0.4544,   0.3535],\n",
      "         [  0.9115,   0.7546,   0.7697,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.8861914]\n",
      "[0.95326809]\n",
      "[0.76388804]\n",
      "[0.85634263]\n",
      "[0.85487643]\n",
      "[0.98031542]\n",
      "[0.26732501]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8451, 0.8123, 0.8038, 0.9498, 0.8026, 0.7412, 0.9417],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.6430,   0.6042,   0.5939],\n",
      "         [  0.6716,   0.7077,   0.6127,   0.6179,   0.5770],\n",
      "         [  0.5927,   0.6307,   0.4194,   0.3929,   0.3182],\n",
      "         [  0.7462,   0.7643,   0.6445,   0.6342,   0.5502],\n",
      "         [  0.6062,   0.6770,   0.5928,   0.5974,   0.4897],\n",
      "         [  0.5458,   0.5308,   0.4404,   0.4544,   0.3535],\n",
      "         [  0.9115,   0.9076,   0.7697,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.8783399]\n",
      "[0.99140417]\n",
      "[0.83878057]\n",
      "[0.79743333]\n",
      "[0.97712586]\n",
      "[0.84777373]\n",
      "[0.79835]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8919, 0.9105, 0.7759, 0.9650, 0.6966, 0.8110, 0.9378],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.6042,   0.5939],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.6179,   0.5770],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.3929,   0.3182],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.6342,   0.5502],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.5974,   0.4897],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.4544,   0.3535],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.81581567]\n",
      "[0.9735367]\n",
      "[0.7339906]\n",
      "[0.81719201]\n",
      "[0.8130016]\n",
      "[0.81924256]\n",
      "[0.67790926]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9048, 0.8746, 0.8677, 0.9388, 0.6948, 0.7206, 0.9571],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.5939],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.5770],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.3182],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.5502],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.4897],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.3535],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.93190794]\n",
      "[0.82929968]\n",
      "[0.78450425]\n",
      "[0.85229145]\n",
      "[0.85216777]\n",
      "[0.80603878]\n",
      "[0.20191619]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8881, 0.9362, 0.8678, 0.9634, 0.6328, 0.8068, 0.9374],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.7870766]\n",
      "[0.95296795]\n",
      "[0.73023332]\n",
      "[0.82154954]\n",
      "[0.999247]\n",
      "[0.78914169]\n",
      "[0.64860566]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8821, 0.8030, 0.7893, 0.8902, 0.8130, 0.6531, 0.9837],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1226,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1201,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1395,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1218,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0913,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1615,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.4025655]\n",
      "[0.41839952]\n",
      "[0.20842984]\n",
      "[0.47293733]\n",
      "[0.10643423]\n",
      "[0.3094505]\n",
      "[0.0188977]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8459, 0.6204, 0.6580, 0.7786, 0.7405, 0.6394, 0.9817],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1226,   0.0910,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1201,   0.0961,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1395,   0.1245,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1218,   0.1124,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0913,   0.0748,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1615,   0.1607,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.37751738]\n",
      "[0.48833143]\n",
      "[0.18985925]\n",
      "[0.46711179]\n",
      "[0.12264433]\n",
      "[0.19449943]\n",
      "[0.03164604]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7762, 0.6460, 0.7654, 0.8369, 0.7645, 0.6749, 0.9659],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.1262,   0.0000,   0.0000],\n",
      "         [  0.1226,   0.0910,   0.1030,   0.0000,   0.0000],\n",
      "         [  0.1201,   0.0961,   0.1242,   0.0000,   0.0000],\n",
      "         [  0.1395,   0.1245,   0.1351,   0.0000,   0.0000],\n",
      "         [  0.1218,   0.1124,   0.1225,   0.0000,   0.0000],\n",
      "         [  0.0913,   0.0748,   0.1040,   0.0000,   0.0000],\n",
      "         [  0.1615,   0.1607,   0.1596,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.39621596]\n",
      "[0.21981562]\n",
      "[0.14872993]\n",
      "[0.36641334]\n",
      "[0.11514083]\n",
      "[0.19831455]\n",
      "[0.01291161]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8754, 0.6701, 0.8301, 0.7386, 0.6955, 0.8707, 0.9025],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.1262,   0.1375,   0.0000],\n",
      "         [  0.1226,   0.0910,   0.1030,   0.0989,   0.0000],\n",
      "         [  0.1201,   0.0961,   0.1242,   0.1195,   0.0000],\n",
      "         [  0.1395,   0.1245,   0.1351,   0.1056,   0.0000],\n",
      "         [  0.1218,   0.1124,   0.1225,   0.1119,   0.0000],\n",
      "         [  0.0913,   0.0748,   0.1040,   0.1378,   0.0000],\n",
      "         [  0.1615,   0.1607,   0.1596,   0.1471,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.03862543]\n",
      "[0.00611077]\n",
      "[0.08499306]\n",
      "[1.06451755]\n",
      "[0.01702779]\n",
      "[0.07331281]\n",
      "[0.14247851]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8272, 0.8212, 0.8554, 0.9076, 0.8491, 0.6894, 0.9647],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.1262,   0.1375,   0.1295],\n",
      "         [  0.1226,   0.0910,   0.1030,   0.0989,   0.1281],\n",
      "         [  0.1201,   0.0961,   0.1242,   0.1195,   0.1223],\n",
      "         [  0.1395,   0.1245,   0.1351,   0.1056,   0.1409],\n",
      "         [  0.1218,   0.1124,   0.1225,   0.1119,   0.1354],\n",
      "         [  0.0913,   0.0748,   0.1040,   0.1378,   0.1043],\n",
      "         [  0.1615,   0.1607,   0.1596,   0.1471,   0.1589]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.52540168]\n",
      "[0.33536528]\n",
      "[0.21605337]\n",
      "[0.49510689]\n",
      "[0.11825651]\n",
      "[0.37694103]\n",
      "[0.01493503]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7471, 0.8150, 0.5597, 0.8766, 0.7443, 0.4945, 0.9504],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  1.3260e-01,  1.2625e-01,  1.3749e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  9.0950e-02,  1.0301e-01,  9.8855e-02,  1.2813e-01],\n",
      "         [ 1.3508e-01,  9.6077e-02,  1.2418e-01,  1.1954e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  1.2454e-01,  1.3511e-01,  1.0561e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.1243e-01,  1.2249e-01,  1.1186e-01,  1.3537e-01],\n",
      "         [ 5.8697e-02,  7.4835e-02,  1.0402e-01,  1.3777e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  1.6075e-01,  1.5963e-01,  1.4710e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.78797321]\n",
      "[1.58159381]\n",
      "[0.54156923]\n",
      "[1.29190636]\n",
      "[0.27921859]\n",
      "[1.13572488]\n",
      "[0.03734971]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7208, 0.7975, 0.6325, 0.9152, 0.6481, 0.7954, 0.8773],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  1.2625e-01,  1.3749e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  1.0301e-01,  9.8855e-02,  1.2813e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.2418e-01,  1.1954e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  1.3511e-01,  1.0561e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  1.2249e-01,  1.1186e-01,  1.3537e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.0402e-01,  1.3777e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  1.5963e-01,  1.4710e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.06885]\n",
      "[0.0342411]\n",
      "[0.02742624]\n",
      "[0.49028546]\n",
      "[0.01490691]\n",
      "[0.10109079]\n",
      "[0.05404896]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8336, 0.6900, 0.2576, 0.9205, 0.6379, 0.4932, 0.9465],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  2.4994e-01,  1.3749e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  2.0844e-01,  9.8855e-02,  1.2813e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.1928e-01,  1.1954e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  2.8106e-01,  1.0561e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  2.1261e-01,  1.1186e-01,  1.3537e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.4535e-01,  1.3777e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  3.1227e-01,  1.4710e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.47502161]\n",
      "[0.34652985]\n",
      "[0.88062191]\n",
      "[1.72665617]\n",
      "[0.31810187]\n",
      "[1.30042099]\n",
      "[0.0986125]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6729,  0.7127,  0.5581,  0.8255, -0.0858,  0.4825,  0.9104],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  2.4994e-01,  2.0888e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  2.0844e-01,  1.8845e-01,  1.2813e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.1928e-01,  1.5983e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  2.8106e-01,  2.2314e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  2.1261e-01,  5.2315e-02,  1.3537e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.4535e-01,  1.7769e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  3.1227e-01,  2.9160e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.13936189]\n",
      "[0.07423184]\n",
      "[0.07061949]\n",
      "[0.60648384]\n",
      "[0.0345412]\n",
      "[0.3974506]\n",
      "[0.111263]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7899, 0.9357, 0.6255, 0.9289, 0.8528, 0.6551, 0.9258],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  2.4994e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  2.0844e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.1928e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  2.8106e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  2.1261e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.4535e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  3.1227e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.11507387]\n",
      "[0.74907694]\n",
      "[0.89721121]\n",
      "[1.08657089]\n",
      "[0.88807824]\n",
      "[0.8769918]\n",
      "[0.45562104]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9215, 0.9048, 0.8395, 0.8882, 0.6720, 0.7227, 0.9669],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 3.9419e-01,  2.4327e-01,  2.4994e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 3.7376e-01,  2.1621e-01,  2.0844e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 2.6971e-01,  1.7564e-01,  1.1928e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 4.1861e-01,  2.7112e-01,  2.8106e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 3.0349e-01,  1.8706e-01,  2.1261e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 1.6630e-01,  1.9826e-01,  1.4535e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 4.7481e-01,  3.0397e-01,  3.1227e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.27408052]\n",
      "[0.22716031]\n",
      "[0.492585]\n",
      "[1.27860918]\n",
      "[0.25546186]\n",
      "[0.76098909]\n",
      "[0.03847826]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8966, 0.8777, 0.8338, 0.9410, 0.7592, 0.5569, 0.9796],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 3.9419e-01,  3.8885e-01,  2.4994e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 3.7376e-01,  3.5322e-01,  2.0844e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 2.6971e-01,  3.0595e-01,  1.1928e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 4.1861e-01,  4.2161e-01,  2.8106e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 3.0349e-01,  3.0521e-01,  2.1261e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 1.6630e-01,  2.6635e-01,  1.4535e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 4.7481e-01,  4.6455e-01,  3.1227e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.5204221]\n",
      "[0.27528202]\n",
      "[0.33225551]\n",
      "[1.45120215]\n",
      "[0.24109393]\n",
      "[0.89574072]\n",
      "[0.03431782]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9388, 0.9548, 0.9482, 0.9563, 0.8474, 0.8698, 0.9786],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 3.9419e-01,  3.8885e-01,  4.0460e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 3.7376e-01,  3.5322e-01,  3.6379e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 2.6971e-01,  3.0595e-01,  2.7452e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 4.1861e-01,  4.2161e-01,  4.3764e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 3.0349e-01,  3.0521e-01,  3.4598e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 1.6630e-01,  2.6635e-01,  2.7688e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 4.7481e-01,  4.6455e-01,  4.7397e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.03418695]\n",
      "[1.07442789]\n",
      "[1.17692364]\n",
      "[1.1634885]\n",
      "[1.01883643]\n",
      "[0.74738135]\n",
      "[0.20465383]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9408, 0.8868, 0.8796, 0.9344, 0.7678, 0.9228, 0.9739],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.3942,   0.3888,   0.4046,   0.3568,   0.2534],\n",
      "         [  0.3738,   0.3532,   0.3638,   0.3215,   0.2791],\n",
      "         [  0.2697,   0.3060,   0.2745,   0.2985,   0.2015],\n",
      "         [  0.4186,   0.4216,   0.4376,   0.3708,   0.2898],\n",
      "         [  0.3035,   0.3052,   0.3460,   0.1645,   0.2477],\n",
      "         [  0.1663,   0.2664,   0.2769,   0.3190,   0.1976],\n",
      "         [  0.4748,   0.4645,   0.4740,   0.4507,   0.3109]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.10219966]\n",
      "[0.08976463]\n",
      "[0.04933195]\n",
      "[0.54626462]\n",
      "[0.02527084]\n",
      "[0.31891236]\n",
      "[0.03352556]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9277, 0.9086, 0.8626, 0.9608, 0.8255, 0.5757, 0.9123],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.3942,   0.3888,   0.4046,   0.3568,   0.3967],\n",
      "         [  0.3738,   0.3532,   0.3638,   0.3215,   0.4194],\n",
      "         [  0.2697,   0.3060,   0.2745,   0.2985,   0.3328],\n",
      "         [  0.4186,   0.4216,   0.4376,   0.3708,   0.4449],\n",
      "         [  0.3035,   0.3052,   0.3460,   0.1645,   0.3750],\n",
      "         [  0.1663,   0.2664,   0.2769,   0.3190,   0.2565],\n",
      "         [  0.4748,   0.4645,   0.4740,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.75333622]\n",
      "[1.01883183]\n",
      "[0.99882255]\n",
      "[1.56357613]\n",
      "[0.23387719]\n",
      "[0.7879611]\n",
      "[0.02040617]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6574, 0.5335, 0.5840, 0.5033, 0.7611, 0.7750, 0.8505],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.3888,   0.4046,   0.3568,   0.3967],\n",
      "         [  0.4113,   0.3532,   0.3638,   0.3215,   0.4194],\n",
      "         [  0.3366,   0.3060,   0.2745,   0.2985,   0.3328],\n",
      "         [  0.4842,   0.4216,   0.4376,   0.3708,   0.4449],\n",
      "         [  0.3997,   0.3052,   0.3460,   0.1645,   0.3750],\n",
      "         [  0.2694,   0.2664,   0.2769,   0.3190,   0.2565],\n",
      "         [  0.5943,   0.4645,   0.4740,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.1433275]\n",
      "[0.11314362]\n",
      "[0.0706263]\n",
      "[0.34069418]\n",
      "[0.02413885]\n",
      "[0.30660861]\n",
      "[0.11323082]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7612, 0.8426, 0.6719, 0.6566, 0.8428, 0.7043, 0.9269],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.4046,   0.3568,   0.3967],\n",
      "         [  0.4113,   0.4787,   0.3638,   0.3215,   0.4194],\n",
      "         [  0.3366,   0.3939,   0.2745,   0.2985,   0.3328],\n",
      "         [  0.4842,   0.5141,   0.4376,   0.3708,   0.4449],\n",
      "         [  0.3997,   0.4288,   0.3460,   0.1645,   0.3750],\n",
      "         [  0.2694,   0.3666,   0.2769,   0.3190,   0.2565],\n",
      "         [  0.5943,   0.6168,   0.4740,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.07957271]\n",
      "[0.04009429]\n",
      "[0.07313421]\n",
      "[0.22992675]\n",
      "[0.01924314]\n",
      "[0.2087582]\n",
      "[0.03854116]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8437, 0.7370, 0.4112, 0.6966, 0.8271, 0.8030, 0.7944],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.5324,   0.3568,   0.3967],\n",
      "         [  0.4113,   0.4787,   0.4672,   0.3215,   0.4194],\n",
      "         [  0.3366,   0.3939,   0.3141,   0.2985,   0.3328],\n",
      "         [  0.4842,   0.5141,   0.5412,   0.3708,   0.4449],\n",
      "         [  0.3997,   0.4288,   0.4745,   0.1645,   0.3750],\n",
      "         [  0.2694,   0.3666,   0.3874,   0.3190,   0.2565],\n",
      "         [  0.5943,   0.6168,   0.6006,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09019388]\n",
      "[0.03250095]\n",
      "[0.10476985]\n",
      "[0.49920191]\n",
      "[0.03723115]\n",
      "[0.13111947]\n",
      "[0.22958012]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8433, 0.7009, 0.7140, 0.7563, 0.8496, 0.7949, 0.8810],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.5324,   0.4595,   0.3967],\n",
      "         [  0.4113,   0.4787,   0.4672,   0.4196,   0.4194],\n",
      "         [  0.3366,   0.3939,   0.3141,   0.3789,   0.3328],\n",
      "         [  0.4842,   0.5141,   0.5412,   0.4835,   0.4449],\n",
      "         [  0.3997,   0.4288,   0.4745,   0.2785,   0.3750],\n",
      "         [  0.2694,   0.3666,   0.3874,   0.4169,   0.2565],\n",
      "         [  0.5943,   0.6168,   0.6006,   0.5887,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.18254372]\n",
      "[0.11733496]\n",
      "[0.09810731]\n",
      "[0.36946404]\n",
      "[0.02092464]\n",
      "[0.35707803]\n",
      "[0.10645892]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8703, 0.8498, 0.7000, 0.8475, 0.8297, 0.7976, 0.9102],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.5324,   0.4595,   0.5193],\n",
      "         [  0.4113,   0.4787,   0.4672,   0.4196,   0.5441],\n",
      "         [  0.3366,   0.3939,   0.3141,   0.3789,   0.4166],\n",
      "         [  0.4842,   0.5141,   0.5412,   0.4835,   0.5693],\n",
      "         [  0.3997,   0.4288,   0.4745,   0.2785,   0.4883],\n",
      "         [  0.2694,   0.3666,   0.3874,   0.4169,   0.3582],\n",
      "         [  0.5943,   0.6168,   0.6006,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.13196666]\n",
      "[0.08080704]\n",
      "[0.13361424]\n",
      "[0.53108224]\n",
      "[0.04892763]\n",
      "[0.20205287]\n",
      "[0.2474573]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9121, 0.8876, 0.0917, 0.7827, 0.7452, 0.7784, 0.9531],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.4986,   0.5324,   0.4595,   0.5193],\n",
      "         [  0.5392,   0.4787,   0.4672,   0.4196,   0.5441],\n",
      "         [  0.2996,   0.3939,   0.3141,   0.3789,   0.4166],\n",
      "         [  0.5892,   0.5141,   0.5412,   0.4835,   0.5693],\n",
      "         [  0.4983,   0.4288,   0.4745,   0.2785,   0.4883],\n",
      "         [  0.3814,   0.3666,   0.3874,   0.4169,   0.3582],\n",
      "         [  0.7483,   0.6168,   0.6006,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.45102182]\n",
      "[0.20080987]\n",
      "[0.2886548]\n",
      "[0.46862557]\n",
      "[0.0802635]\n",
      "[0.76499848]\n",
      "[0.08789203]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8221, 0.6911, 0.4338, 0.8052, 0.8685, 0.8728, 0.9755],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.5324,   0.4595,   0.5193],\n",
      "         [  0.5392,   0.5884,   0.4672,   0.4196,   0.5441],\n",
      "         [  0.2996,   0.4266,   0.3141,   0.3789,   0.4166],\n",
      "         [  0.5892,   0.6258,   0.5412,   0.4835,   0.5693],\n",
      "         [  0.4983,   0.5657,   0.4745,   0.2785,   0.4883],\n",
      "         [  0.3814,   0.4974,   0.3874,   0.4169,   0.3582],\n",
      "         [  0.7483,   0.7771,   0.6006,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.49606515]\n",
      "[0.08889655]\n",
      "[0.28214582]\n",
      "[0.59155366]\n",
      "[0.21673222]\n",
      "[0.26958858]\n",
      "[0.038617]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6437,  0.8770, -0.6170,  0.5033,  0.7834, -0.0225,  0.8938],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.6247,   0.4595,   0.5193],\n",
      "         [  0.5392,   0.5884,   0.6002,   0.4196,   0.5441],\n",
      "         [  0.2996,   0.4266,   0.1515,   0.3789,   0.4166],\n",
      "         [  0.5892,   0.6258,   0.5989,   0.4835,   0.5693],\n",
      "         [  0.4983,   0.5657,   0.5929,   0.2785,   0.4883],\n",
      "         [  0.3814,   0.4974,   0.3399,   0.4169,   0.3582],\n",
      "         [  0.7483,   0.7771,   0.7452,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.80670248]\n",
      "[0.09909374]\n",
      "[0.07644913]\n",
      "[0.73872699]\n",
      "[1.18274359]\n",
      "[0.5124512]\n",
      "[0.22888795]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8380, 0.8962, 0.6583, 0.8188, 0.8839, 0.7864, 0.9550],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.6247,   0.5864,   0.5193],\n",
      "         [  0.5392,   0.5884,   0.6002,   0.5544,   0.5441],\n",
      "         [  0.2996,   0.4266,   0.1515,   0.4278,   0.4166],\n",
      "         [  0.5892,   0.6258,   0.5989,   0.6010,   0.5693],\n",
      "         [  0.4983,   0.5657,   0.5929,   0.4114,   0.4883],\n",
      "         [  0.3814,   0.4974,   0.3399,   0.5296,   0.3582],\n",
      "         [  0.7483,   0.7771,   0.7452,   0.7372,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.48818046]\n",
      "[0.14379967]\n",
      "[0.2814378]\n",
      "[0.57570997]\n",
      "[0.17296336]\n",
      "[0.79535505]\n",
      "[0.08201899]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8967, 0.8894, 0.3246, 0.7778, 0.8211, 0.8017, 0.9692],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.6247,   0.5864,   0.6609],\n",
      "         [  0.5392,   0.5884,   0.6002,   0.5544,   0.6845],\n",
      "         [  0.2996,   0.4266,   0.1515,   0.4278,   0.4151],\n",
      "         [  0.5892,   0.6258,   0.5989,   0.6010,   0.6813],\n",
      "         [  0.4983,   0.5657,   0.5929,   0.4114,   0.6041],\n",
      "         [  0.3814,   0.4974,   0.3399,   0.5296,   0.4830],\n",
      "         [  0.7483,   0.7771,   0.7452,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.40986762]\n",
      "[0.1208521]\n",
      "[0.36449529]\n",
      "[0.50642732]\n",
      "[0.12903694]\n",
      "[0.59969355]\n",
      "[0.07970008]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9136, 0.8877, 0.8599, 0.9594, 0.8850, 0.6797, 0.9576],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.6252,   0.6247,   0.5864,   0.6609],\n",
      "         [  0.6832,   0.5884,   0.6002,   0.5544,   0.6845],\n",
      "         [  0.4343,   0.4266,   0.1515,   0.4278,   0.4151],\n",
      "         [  0.7432,   0.6258,   0.5989,   0.6010,   0.6813],\n",
      "         [  0.6222,   0.5657,   0.5929,   0.4114,   0.6041],\n",
      "         [  0.4786,   0.4974,   0.3399,   0.5296,   0.4830],\n",
      "         [  0.9055,   0.7771,   0.7452,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94592704]\n",
      "[0.91268345]\n",
      "[0.90836605]\n",
      "[0.87492432]\n",
      "[0.9768584]\n",
      "[1.01386982]\n",
      "[0.27732182]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9211, 0.8084, 0.8836, 0.9404, 0.7674, 0.7024, 0.9636],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.6247,   0.5864,   0.6609],\n",
      "         [  0.6832,   0.7200,   0.6002,   0.5544,   0.6845],\n",
      "         [  0.4343,   0.5658,   0.1515,   0.4278,   0.4151],\n",
      "         [  0.7432,   0.7796,   0.5989,   0.6010,   0.6813],\n",
      "         [  0.6222,   0.6793,   0.5929,   0.4114,   0.6041],\n",
      "         [  0.4786,   0.5967,   0.3399,   0.5296,   0.4830],\n",
      "         [  0.9055,   0.9360,   0.7452,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.89748698]\n",
      "[0.91593859]\n",
      "[0.85573162]\n",
      "[0.86460127]\n",
      "[0.82331666]\n",
      "[0.93160402]\n",
      "[0.24383163]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8800, 0.9101, 0.8664, 0.9538, 0.7922, 0.6765, 0.9406],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.5864,   0.6609],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.5544,   0.6845],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.4278,   0.4151],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.6010,   0.6813],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.4114,   0.6041],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.5296,   0.4830],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.89410571]\n",
      "[0.89814751]\n",
      "[0.78394586]\n",
      "[0.85447891]\n",
      "[0.87399294]\n",
      "[0.90643076]\n",
      "[0.24117444]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9129, 0.9043, 0.8113, 0.9452, 0.8733, 0.7227, 0.9422],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.6609],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.6845],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.4151],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.6813],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.6041],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.4830],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.91895603]\n",
      "[0.85211251]\n",
      "[0.77004353]\n",
      "[0.79465433]\n",
      "[0.77024881]\n",
      "[0.82088678]\n",
      "[0.28755121]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8535, 0.8436, 0.9108, 0.9591, 0.8956, 0.7707, 0.9156],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.8311049]\n",
      "[0.75573502]\n",
      "[0.66964394]\n",
      "[0.88131951]\n",
      "[0.92329692]\n",
      "[0.63215707]\n",
      "[0.19856577]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8855, 0.8176, 0.8288, 0.8050, 0.6423, 0.8823, 0.9229],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1278,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1238,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1245,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1023,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1286,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1506,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06109698]\n",
      "[0.03249756]\n",
      "[0.07210588]\n",
      "[0.71612455]\n",
      "[0.01973256]\n",
      "[0.10045489]\n",
      "[0.16244369]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7940, 0.6968, 0.8296, 0.9011, 0.7206, 0.6797, 0.9681],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1278,   0.1122,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1238,   0.1340,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1245,   0.1386,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1023,   0.1145,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1286,   0.1026,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1506,   0.1602,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.43338537]\n",
      "[0.20248918]\n",
      "[0.19951279]\n",
      "[0.4821078]\n",
      "[0.04624547]\n",
      "[0.33629289]\n",
      "[0.01154209]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9276, 0.9355, 0.8680, 0.8689, 0.8303, 0.7225, 0.9730],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.1482,   0.0000,   0.0000],\n",
      "         [  0.1278,   0.1122,   0.1493,   0.0000,   0.0000],\n",
      "         [  0.1238,   0.1340,   0.1336,   0.0000,   0.0000],\n",
      "         [  0.1245,   0.1386,   0.1343,   0.0000,   0.0000],\n",
      "         [  0.1023,   0.1145,   0.1308,   0.0000,   0.0000],\n",
      "         [  0.1286,   0.1026,   0.1023,   0.0000,   0.0000],\n",
      "         [  0.1506,   0.1602,   0.1611,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.77064]\n",
      "[0.06968173]\n",
      "[0.06717952]\n",
      "[0.75538816]\n",
      "[0.44534484]\n",
      "[0.16845062]\n",
      "[0.19959654]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8648, 0.8771, 0.8692, 0.8907, 0.8513, 0.7876, 0.9735],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.1482,   0.1375,   0.0000],\n",
      "         [  0.1278,   0.1122,   0.1493,   0.1382,   0.0000],\n",
      "         [  0.1238,   0.1340,   0.1336,   0.1292,   0.0000],\n",
      "         [  0.1245,   0.1386,   0.1343,   0.1410,   0.0000],\n",
      "         [  0.1023,   0.1145,   0.1308,   0.1373,   0.0000],\n",
      "         [  0.1286,   0.1026,   0.1023,   0.1193,   0.0000],\n",
      "         [  0.1506,   0.1602,   0.1611,   0.1599,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.53876239]\n",
      "[0.32490396]\n",
      "[0.29096393]\n",
      "[0.52497387]\n",
      "[0.19511542]\n",
      "[0.40010748]\n",
      "[0.02165219]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8742, 0.7305, 0.8052, 0.7648, 0.7940, 0.8244, 0.9455],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.1482,   0.1375,   0.1365],\n",
      "         [  0.1278,   0.1122,   0.1493,   0.1382,   0.1061],\n",
      "         [  0.1238,   0.1340,   0.1336,   0.1292,   0.1129],\n",
      "         [  0.1245,   0.1386,   0.1343,   0.1410,   0.1103],\n",
      "         [  0.1023,   0.1145,   0.1308,   0.1373,   0.1257],\n",
      "         [  0.1286,   0.1026,   0.1023,   0.1193,   0.1259],\n",
      "         [  0.1506,   0.1602,   0.1611,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.07054089]\n",
      "[0.03595638]\n",
      "[0.08798174]\n",
      "[0.91533029]\n",
      "[0.02185151]\n",
      "[0.09396567]\n",
      "[0.15750508]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8037, 0.7728, 0.1496, 0.9090, 0.4923, 0.6627, 0.9439],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1267,   0.1482,   0.1375,   0.1365],\n",
      "         [  0.2383,   0.1122,   0.1493,   0.1382,   0.1061],\n",
      "         [  0.0973,   0.1340,   0.1336,   0.1292,   0.1129],\n",
      "         [  0.2622,   0.1386,   0.1343,   0.1410,   0.1103],\n",
      "         [  0.1494,   0.1145,   0.1308,   0.1373,   0.1257],\n",
      "         [  0.2062,   0.1026,   0.1023,   0.1193,   0.1259],\n",
      "         [  0.3043,   0.1602,   0.1611,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.57845891]\n",
      "[0.30467334]\n",
      "[0.82690642]\n",
      "[1.67363467]\n",
      "[0.41769488]\n",
      "[1.20873051]\n",
      "[0.09954467]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.4785, 0.8069, 0.6953, 0.8843, 0.5938, 0.6016, 0.8683],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.1482,   0.1375,   0.1365],\n",
      "         [  0.2383,   0.2331,   0.1493,   0.1382,   0.1061],\n",
      "         [  0.0973,   0.2213,   0.1336,   0.1292,   0.1129],\n",
      "         [  0.2622,   0.2669,   0.1343,   0.1410,   0.1103],\n",
      "         [  0.1494,   0.1751,   0.1308,   0.1373,   0.1257],\n",
      "         [  0.2062,   0.1891,   0.1023,   0.1193,   0.1259],\n",
      "         [  0.3043,   0.2950,   0.1611,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.0943231]\n",
      "[0.03650244]\n",
      "[0.04944443]\n",
      "[1.15165954]\n",
      "[0.0208738]\n",
      "[0.1067268]\n",
      "[0.08451934]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9217, 0.8838, 0.7838, 0.9449, 0.8898, 0.7927, 0.9630],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.2931,   0.1375,   0.1365],\n",
      "         [  0.2383,   0.2331,   0.2917,   0.1382,   0.1061],\n",
      "         [  0.0973,   0.2213,   0.2471,   0.1292,   0.1129],\n",
      "         [  0.2622,   0.2669,   0.2885,   0.1410,   0.1103],\n",
      "         [  0.1494,   0.1751,   0.2566,   0.1373,   0.1257],\n",
      "         [  0.2062,   0.1891,   0.2072,   0.1193,   0.1259],\n",
      "         [  0.3043,   0.2950,   0.3192,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.91278799]\n",
      "[0.7420375]\n",
      "[0.72661676]\n",
      "[1.05405731]\n",
      "[0.73038282]\n",
      "[0.86796806]\n",
      "[0.36025236]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9130, 0.9132, 0.6214, 0.9190, 0.6309, 0.8741, 0.9564],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.2931,   0.2764,   0.1365],\n",
      "         [  0.2383,   0.2331,   0.2917,   0.2739,   0.1061],\n",
      "         [  0.0973,   0.2213,   0.2471,   0.2043,   0.1129],\n",
      "         [  0.2622,   0.2669,   0.2885,   0.2872,   0.1103],\n",
      "         [  0.1494,   0.1751,   0.2566,   0.1913,   0.1257],\n",
      "         [  0.2062,   0.1891,   0.2072,   0.2335,   0.1259],\n",
      "         [  0.3043,   0.2950,   0.3192,   0.3170,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.51225246]\n",
      "[1.42277892]\n",
      "[0.36483744]\n",
      "[1.39348488]\n",
      "[0.36397643]\n",
      "[0.91276405]\n",
      "[0.02776382]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6476, 0.7283, 0.6222, 0.9455, 0.5774, 0.6756, 0.9303],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.2931,   0.2764,   0.2170],\n",
      "         [  0.2383,   0.2331,   0.2917,   0.2739,   0.2108],\n",
      "         [  0.0973,   0.2213,   0.2471,   0.2043,   0.1724],\n",
      "         [  0.2622,   0.2669,   0.2885,   0.2872,   0.2516],\n",
      "         [  0.1494,   0.1751,   0.2566,   0.1913,   0.1880],\n",
      "         [  0.2062,   0.1891,   0.2072,   0.2335,   0.2030],\n",
      "         [  0.3043,   0.2950,   0.3192,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.11450942]\n",
      "[0.07947797]\n",
      "[0.06017903]\n",
      "[0.65113077]\n",
      "[0.01723155]\n",
      "[0.27060145]\n",
      "[0.0790013]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9321, 0.9617, 0.9561, 0.9701, 0.8813, 0.7059, 0.9747],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.1878,   0.2931,   0.2764,   0.2170],\n",
      "         [  0.3947,   0.2331,   0.2917,   0.2739,   0.2108],\n",
      "         [  0.2534,   0.2213,   0.2471,   0.2043,   0.1724],\n",
      "         [  0.4213,   0.2669,   0.2885,   0.2872,   0.2516],\n",
      "         [  0.2896,   0.1751,   0.2566,   0.1913,   0.1880],\n",
      "         [  0.3080,   0.1891,   0.2072,   0.2335,   0.2030],\n",
      "         [  0.4652,   0.2950,   0.3192,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.05714516]\n",
      "[1.02423697]\n",
      "[1.0989669]\n",
      "[1.09541577]\n",
      "[0.98453904]\n",
      "[0.89476073]\n",
      "[0.27002026]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8618, 0.8806, 0.8387, 0.9127, 0.7860, 0.7471, 0.9628],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.2931,   0.2764,   0.2170],\n",
      "         [  0.3947,   0.3686,   0.2917,   0.2739,   0.2108],\n",
      "         [  0.2534,   0.3401,   0.2471,   0.2043,   0.1724],\n",
      "         [  0.4213,   0.4079,   0.2885,   0.2872,   0.2516],\n",
      "         [  0.2896,   0.2945,   0.2566,   0.1913,   0.1880],\n",
      "         [  0.3080,   0.2918,   0.2072,   0.2335,   0.2030],\n",
      "         [  0.4652,   0.4520,   0.3192,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.59807575]\n",
      "[0.35624244]\n",
      "[0.49243636]\n",
      "[1.49774852]\n",
      "[0.26465588]\n",
      "[1.03364875]\n",
      "[0.04829712]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9635, 0.9356, 0.9254, 0.9050, 0.8506, 0.7500, 0.9738],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.4495,   0.2764,   0.2170],\n",
      "         [  0.3947,   0.3686,   0.4423,   0.2739,   0.2108],\n",
      "         [  0.2534,   0.3401,   0.3928,   0.2043,   0.1724],\n",
      "         [  0.4213,   0.4079,   0.4347,   0.2872,   0.2516],\n",
      "         [  0.2896,   0.2945,   0.3889,   0.1913,   0.1880],\n",
      "         [  0.3080,   0.2918,   0.3054,   0.2335,   0.2030],\n",
      "         [  0.4652,   0.4520,   0.4790,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.76046191]\n",
      "[1.08920332]\n",
      "[0.47813609]\n",
      "[1.43483443]\n",
      "[0.29155655]\n",
      "[0.83558342]\n",
      "[0.01923997]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9322, 0.9020, 0.9179, 0.9221, 0.8237, 0.8816, 0.9723],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.4495,   0.4251,   0.2170],\n",
      "         [  0.3947,   0.3686,   0.4423,   0.4143,   0.2108],\n",
      "         [  0.2534,   0.3401,   0.3928,   0.3443,   0.1724],\n",
      "         [  0.4213,   0.4079,   0.4347,   0.4169,   0.2516],\n",
      "         [  0.2896,   0.2945,   0.3889,   0.3172,   0.1880],\n",
      "         [  0.3080,   0.2918,   0.3054,   0.3700,   0.2030],\n",
      "         [  0.4652,   0.4520,   0.4790,   0.4751,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.14116884]\n",
      "[0.06735008]\n",
      "[0.07715621]\n",
      "[1.20971928]\n",
      "[0.02503762]\n",
      "[0.39716672]\n",
      "[0.04960227]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9231, 0.9594, 0.9430, 0.9554, 0.8807, 0.6535, 0.9687],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.4495,   0.4251,   0.3683],\n",
      "         [  0.3947,   0.3686,   0.4423,   0.4143,   0.3675],\n",
      "         [  0.2534,   0.3401,   0.3928,   0.3443,   0.3254],\n",
      "         [  0.4213,   0.4079,   0.4347,   0.4169,   0.4057],\n",
      "         [  0.2896,   0.2945,   0.3889,   0.3172,   0.3284],\n",
      "         [  0.3080,   0.2918,   0.3054,   0.3700,   0.2962],\n",
      "         [  0.4652,   0.4520,   0.4790,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.08872456]\n",
      "[1.03074754]\n",
      "[1.10560763]\n",
      "[1.13605651]\n",
      "[0.99023606]\n",
      "[0.9451829]\n",
      "[0.2468443]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9058, 0.8791, 0.8254, 0.8439, 0.7407, 0.8363, 0.9590],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.3187,   0.4495,   0.4251,   0.3683],\n",
      "         [  0.5316,   0.3686,   0.4423,   0.4143,   0.3675],\n",
      "         [  0.3751,   0.3401,   0.3928,   0.3443,   0.3254],\n",
      "         [  0.5546,   0.4079,   0.4347,   0.4169,   0.4057],\n",
      "         [  0.3868,   0.2945,   0.3889,   0.3172,   0.3284],\n",
      "         [  0.4345,   0.2918,   0.3054,   0.3700,   0.2962],\n",
      "         [  0.6230,   0.4520,   0.4790,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06766953]\n",
      "[0.02789246]\n",
      "[0.06157395]\n",
      "[0.31369373]\n",
      "[0.0197897]\n",
      "[0.16433146]\n",
      "[0.09844738]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8708, 0.8667, 0.6422, 0.7566, 0.7731, 0.7915, 0.9556],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.4495,   0.4251,   0.3683],\n",
      "         [  0.5316,   0.4955,   0.4423,   0.4143,   0.3675],\n",
      "         [  0.3751,   0.4250,   0.3928,   0.3443,   0.3254],\n",
      "         [  0.5546,   0.5161,   0.4347,   0.4169,   0.4057],\n",
      "         [  0.3868,   0.4071,   0.3889,   0.3172,   0.3284],\n",
      "         [  0.4345,   0.4093,   0.3054,   0.3700,   0.2962],\n",
      "         [  0.6230,   0.6088,   0.4790,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.11295395]\n",
      "[0.07223752]\n",
      "[0.09668045]\n",
      "[0.38457901]\n",
      "[0.01916273]\n",
      "[0.2150111]\n",
      "[0.13864319]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7790, 0.7896, 0.6752, 0.8307, 0.7662, 0.7288, 0.8985],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.5699,   0.4251,   0.3683],\n",
      "         [  0.5316,   0.4955,   0.5600,   0.4143,   0.3675],\n",
      "         [  0.3751,   0.4250,   0.4846,   0.3443,   0.3254],\n",
      "         [  0.5546,   0.5161,   0.5571,   0.4169,   0.4057],\n",
      "         [  0.3868,   0.4071,   0.5039,   0.3172,   0.3284],\n",
      "         [  0.4345,   0.4093,   0.4133,   0.3700,   0.2962],\n",
      "         [  0.6230,   0.6088,   0.6229,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09917383]\n",
      "[0.04415978]\n",
      "[0.11471508]\n",
      "[0.49132754]\n",
      "[0.04528883]\n",
      "[0.16106811]\n",
      "[0.23465642]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8484, 0.8473, 0.7571, 0.7663, 0.8260, 0.7962, 0.9284],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.5699,   0.5469,   0.3683],\n",
      "         [  0.5316,   0.4955,   0.5600,   0.5245,   0.3675],\n",
      "         [  0.3751,   0.4250,   0.4846,   0.4595,   0.3254],\n",
      "         [  0.5546,   0.5161,   0.5571,   0.5247,   0.4057],\n",
      "         [  0.3868,   0.4071,   0.5039,   0.4333,   0.3284],\n",
      "         [  0.4345,   0.4093,   0.4133,   0.4849,   0.2962],\n",
      "         [  0.6230,   0.6088,   0.6229,   0.6221,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09705949]\n",
      "[0.08313929]\n",
      "[0.0505674]\n",
      "[0.26697959]\n",
      "[0.02317965]\n",
      "[0.21895643]\n",
      "[0.09202839]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.4437, 0.6643, 0.5929, 0.5677, 0.8554, 0.8594, 0.9172],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.5699,   0.5469,   0.4151],\n",
      "         [  0.5316,   0.4955,   0.5600,   0.5245,   0.4412],\n",
      "         [  0.3751,   0.4250,   0.4846,   0.4595,   0.3941],\n",
      "         [  0.5546,   0.5161,   0.5571,   0.5247,   0.4841],\n",
      "         [  0.3868,   0.4071,   0.5039,   0.4333,   0.4507],\n",
      "         [  0.4345,   0.4093,   0.4133,   0.4849,   0.4227],\n",
      "         [  0.6230,   0.6088,   0.6229,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.12595742]\n",
      "[0.08858596]\n",
      "[0.07008049]\n",
      "[0.30176959]\n",
      "[0.01816264]\n",
      "[0.24020995]\n",
      "[0.10040068]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8436, 0.9141, 0.3575, 0.8357, 0.8574, 0.7864, 0.9619],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.4509,   0.5699,   0.5469,   0.4151],\n",
      "         [  0.6726,   0.4955,   0.5600,   0.5245,   0.4412],\n",
      "         [  0.3554,   0.4250,   0.4846,   0.4595,   0.3941],\n",
      "         [  0.6777,   0.5161,   0.5571,   0.5247,   0.4841],\n",
      "         [  0.5096,   0.4071,   0.5039,   0.4333,   0.4507],\n",
      "         [  0.5353,   0.4093,   0.4133,   0.4849,   0.4227],\n",
      "         [  0.7768,   0.6088,   0.6229,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.46522427]\n",
      "[0.19874998]\n",
      "[0.43916757]\n",
      "[0.50766556]\n",
      "[0.2337302]\n",
      "[0.7423122]\n",
      "[0.09637242]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8263, 0.8331, 0.3773, 0.8068, 0.7855, 0.5587, 0.9378],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.5699,   0.5469,   0.4151],\n",
      "         [  0.6726,   0.6252,   0.5600,   0.5245,   0.4412],\n",
      "         [  0.3554,   0.4470,   0.4846,   0.4595,   0.3941],\n",
      "         [  0.6777,   0.6325,   0.5571,   0.5247,   0.4841],\n",
      "         [  0.5096,   0.5219,   0.5039,   0.4333,   0.4507],\n",
      "         [  0.5353,   0.4815,   0.4133,   0.4849,   0.4227],\n",
      "         [  0.7768,   0.7612,   0.6229,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94164199]\n",
      "[0.11094469]\n",
      "[0.1330915]\n",
      "[0.730654]\n",
      "[1.31137936]\n",
      "[0.93303862]\n",
      "[0.34458214]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9011, 0.8841, 0.6477, 0.8548, 0.9106, 0.5827, 0.9700],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.7079,   0.5469,   0.4151],\n",
      "         [  0.6726,   0.6252,   0.6952,   0.5245,   0.4412],\n",
      "         [  0.3554,   0.4470,   0.5583,   0.4595,   0.3941],\n",
      "         [  0.6777,   0.6325,   0.6847,   0.5247,   0.4841],\n",
      "         [  0.5096,   0.5219,   0.6406,   0.4333,   0.4507],\n",
      "         [  0.5353,   0.4815,   0.4876,   0.4849,   0.4227],\n",
      "         [  0.7768,   0.7612,   0.7815,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.90670781]\n",
      "[0.12535583]\n",
      "[0.10238077]\n",
      "[0.7645274]\n",
      "[1.32277114]\n",
      "[0.90311065]\n",
      "[0.26918812]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9122, 0.8611, 0.6632, 0.7556, 0.9065, 0.7588, 0.9548],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.7079,   0.6848,   0.4151],\n",
      "         [  0.6726,   0.6252,   0.6952,   0.6560,   0.4412],\n",
      "         [  0.3554,   0.4470,   0.5583,   0.5197,   0.3941],\n",
      "         [  0.6777,   0.6325,   0.6847,   0.6299,   0.4841],\n",
      "         [  0.5096,   0.5219,   0.6406,   0.5743,   0.4507],\n",
      "         [  0.5353,   0.4815,   0.4876,   0.5934,   0.4227],\n",
      "         [  0.7768,   0.7612,   0.7815,   0.7774,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06179815]\n",
      "[0.18134512]\n",
      "[0.01716977]\n",
      "[0.32521496]\n",
      "[0.0584179]\n",
      "[0.06362197]\n",
      "[0.92884933]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.7919,  0.8241, -0.1020,  0.8204,  0.8519,  0.2925,  0.9574],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.7079,   0.6848,   0.5323],\n",
      "         [  0.6726,   0.6252,   0.6952,   0.6560,   0.5692],\n",
      "         [  0.3554,   0.4470,   0.5583,   0.5197,   0.2959],\n",
      "         [  0.6777,   0.6325,   0.6847,   0.6299,   0.5911],\n",
      "         [  0.5096,   0.5219,   0.6406,   0.5743,   0.5757],\n",
      "         [  0.5353,   0.4815,   0.4876,   0.5934,   0.4421],\n",
      "         [  0.7768,   0.7612,   0.7815,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.49957275]\n",
      "[0.17931039]\n",
      "[0.34040875]\n",
      "[0.50374347]\n",
      "[0.1832406]\n",
      "[0.42031754]\n",
      "[0.04481311]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8939, 0.9550, 0.8510, 0.9681, 0.7083, 0.8007, 0.9484],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.5755,   0.7079,   0.6848,   0.5323],\n",
      "         [  0.8285,   0.6252,   0.6952,   0.6560,   0.5692],\n",
      "         [  0.4826,   0.4470,   0.5583,   0.5197,   0.2959],\n",
      "         [  0.8369,   0.6325,   0.6847,   0.6299,   0.5911],\n",
      "         [  0.5904,   0.5219,   0.6406,   0.5743,   0.5757],\n",
      "         [  0.6566,   0.4815,   0.4876,   0.5934,   0.4421],\n",
      "         [  0.9327,   0.7612,   0.7815,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.81740645]\n",
      "[0.93862671]\n",
      "[0.77398058]\n",
      "[0.83613675]\n",
      "[0.85721097]\n",
      "[0.92248914]\n",
      "[0.75284765]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9335, 0.8380, 0.7493, 0.9330, 0.8402, 0.8886, 0.9687],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.7079,   0.6848,   0.5323],\n",
      "         [  0.8285,   0.7598,   0.6952,   0.6560,   0.5692],\n",
      "         [  0.4826,   0.5615,   0.5583,   0.5197,   0.2959],\n",
      "         [  0.8369,   0.7837,   0.6847,   0.6299,   0.5911],\n",
      "         [  0.5904,   0.6486,   0.6406,   0.5743,   0.5757],\n",
      "         [  0.6566,   0.6234,   0.4876,   0.5934,   0.4421],\n",
      "         [  0.9327,   0.9207,   0.7815,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.42263334]\n",
      "[0.19500685]\n",
      "[0.43096873]\n",
      "[1.46442041]\n",
      "[0.23713499]\n",
      "[0.80175201]\n",
      "[0.1418373]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8906, 0.8447, 0.8426, 0.9067, 0.7858, 0.7290, 0.9437],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.6848,   0.5323],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.6560,   0.5692],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.5197,   0.2959],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.6299,   0.5911],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.5743,   0.5757],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.5934,   0.4421],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.45581942]\n",
      "[1.66812749]\n",
      "[0.68534491]\n",
      "[1.58238494]\n",
      "[0.30958706]\n",
      "[1.09153821]\n",
      "[0.04756957]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9060, 0.9369, 0.8868, 0.9657, 0.7350, 0.8217, 0.9528],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.5323],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.5692],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.2959],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.5911],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.5757],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.4421],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.79924622]\n",
      "[0.89633752]\n",
      "[0.78642654]\n",
      "[0.81024923]\n",
      "[0.81014892]\n",
      "[0.78112398]\n",
      "[0.70173311]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8844, 0.9433, 0.9041, 0.9735, 0.7028, 0.8086, 0.9340],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.87303306]\n",
      "[1.00781679]\n",
      "[0.78768069]\n",
      "[0.8269091]\n",
      "[0.99795601]\n",
      "[0.89195585]\n",
      "[0.77137153]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8728, 0.9153, 0.8630, 0.8319, 0.8264, 0.7043, 0.9771],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1437,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1259,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1236,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1279,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0993,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1609,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.50637662]\n",
      "[0.30555928]\n",
      "[0.24734409]\n",
      "[0.49529023]\n",
      "[0.15384236]\n",
      "[0.33767109]\n",
      "[0.01641765]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8723, 0.9115, 0.8815, 0.9195, 0.8450, 0.7514, 0.9730],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1437,   0.1468,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1259,   0.1361,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1236,   0.1430,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1279,   0.1359,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0993,   0.1149,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1609,   0.1607,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.49545859]\n",
      "[0.2689647]\n",
      "[0.30225554]\n",
      "[0.59279098]\n",
      "[0.12215343]\n",
      "[0.24007558]\n",
      "[0.01664211]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9379, 0.8965, 0.7646, 0.9062, 0.8170, 0.7045, 0.9836],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.1488,   0.0000,   0.0000],\n",
      "         [  0.1437,   0.1468,   0.1396,   0.0000,   0.0000],\n",
      "         [  0.1259,   0.1361,   0.1151,   0.0000,   0.0000],\n",
      "         [  0.1236,   0.1430,   0.1415,   0.0000,   0.0000],\n",
      "         [  0.1279,   0.1359,   0.1288,   0.0000,   0.0000],\n",
      "         [  0.0993,   0.1149,   0.1014,   0.0000,   0.0000],\n",
      "         [  0.1609,   0.1607,   0.1616,   0.0000,   0.0000]]])\n",
      "[0.38466799]\n",
      "[0.42059928]\n",
      "[0.28467721]\n",
      "[0.57698693]\n",
      "[0.09971113]\n",
      "[0.19600564]\n",
      "[0.03748663]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8580, 0.6717, 0.7962, 0.9009, 0.8173, 0.8519, 0.9441],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.1488,   0.1388,   0.0000],\n",
      "         [  0.1437,   0.1468,   0.1396,   0.1033,   0.0000],\n",
      "         [  0.1259,   0.1361,   0.1151,   0.1240,   0.0000],\n",
      "         [  0.1236,   0.1430,   0.1415,   0.1413,   0.0000],\n",
      "         [  0.1279,   0.1359,   0.1288,   0.1301,   0.0000],\n",
      "         [  0.0993,   0.1149,   0.1014,   0.1320,   0.0000],\n",
      "         [  0.1609,   0.1607,   0.1616,   0.1551,   0.0000]]])\n",
      "[0.04683058]\n",
      "[0.01455749]\n",
      "[0.07316558]\n",
      "[0.74371817]\n",
      "[0.02326372]\n",
      "[0.08666937]\n",
      "[0.12609362]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8404, 0.7868, 0.8903, 0.8487, 0.8615, 0.7610, 0.9646],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.1488,   0.1388,   0.1323],\n",
      "         [  0.1437,   0.1468,   0.1396,   0.1033,   0.1243],\n",
      "         [  0.1259,   0.1361,   0.1151,   0.1240,   0.1277],\n",
      "         [  0.1236,   0.1430,   0.1415,   0.1413,   0.1268],\n",
      "         [  0.1279,   0.1359,   0.1288,   0.1301,   0.1390],\n",
      "         [  0.0993,   0.1149,   0.1014,   0.1320,   0.1152],\n",
      "         [  0.1609,   0.1607,   0.1616,   0.1551,   0.1571]]])\n",
      "[0.45878588]\n",
      "[0.26513487]\n",
      "[0.32251921]\n",
      "[0.57054374]\n",
      "[0.16828287]\n",
      "[0.28662616]\n",
      "[0.02139747]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8692, 0.9458, 0.8122, 0.9507, 0.8837, 0.8627, 0.9186],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.1406,   0.1488,   0.1388,   0.1323],\n",
      "         [  0.2961,   0.1468,   0.1396,   0.1033,   0.1243],\n",
      "         [  0.2418,   0.1361,   0.1151,   0.1240,   0.1277],\n",
      "         [  0.2782,   0.1430,   0.1415,   0.1413,   0.1268],\n",
      "         [  0.2498,   0.1359,   0.1288,   0.1301,   0.1390],\n",
      "         [  0.2281,   0.1149,   0.1014,   0.1320,   0.1152],\n",
      "         [  0.3099,   0.1607,   0.1616,   0.1551,   0.1571]]])\n",
      "[1.03923018]\n",
      "[0.9028543]\n",
      "[0.74707417]\n",
      "[1.13382905]\n",
      "[0.77296035]\n",
      "[0.80932582]\n",
      "[0.79818587]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8472, 0.8876, 0.6595, 0.9373, 0.6208, 0.8549, 0.9770],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.1488,   0.1388,   0.1323],\n",
      "         [  0.2961,   0.2822,   0.1396,   0.1033,   0.1243],\n",
      "         [  0.2418,   0.2257,   0.1151,   0.1240,   0.1277],\n",
      "         [  0.2782,   0.2918,   0.1415,   0.1413,   0.1268],\n",
      "         [  0.2498,   0.1963,   0.1288,   0.1301,   0.1390],\n",
      "         [  0.2281,   0.2246,   0.1014,   0.1320,   0.1152],\n",
      "         [  0.3099,   0.3195,   0.1616,   0.1551,   0.1571]]])\n",
      "[1.62870839]\n",
      "[1.5875119]\n",
      "[0.3298151]\n",
      "[1.40661871]\n",
      "[0.32525665]\n",
      "[0.84154561]\n",
      "[0.02968318]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7600, 0.8732, 0.7608, 0.9008, 0.6590, 0.7201, 0.9243],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.2532,   0.1388,   0.1323],\n",
      "         [  0.2961,   0.2822,   0.2754,   0.1033,   0.1243],\n",
      "         [  0.2418,   0.2257,   0.2052,   0.1240,   0.1277],\n",
      "         [  0.2782,   0.2918,   0.2747,   0.1413,   0.1268],\n",
      "         [  0.2498,   0.1963,   0.2137,   0.1301,   0.1390],\n",
      "         [  0.2281,   0.2246,   0.1926,   0.1320,   0.1152],\n",
      "         [  0.3099,   0.3195,   0.3062,   0.1551,   0.1571]]])\n",
      "[0.15075074]\n",
      "[0.08817591]\n",
      "[0.06964356]\n",
      "[1.01092688]\n",
      "[0.01894608]\n",
      "[0.35232043]\n",
      "[0.1039875]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7981, 0.9232, 0.5931, 0.9096, 0.6309, 0.6734, 0.9682],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.2532,   0.2627,   0.1323],\n",
      "         [  0.2961,   0.2822,   0.2754,   0.2478,   0.1243],\n",
      "         [  0.2418,   0.2257,   0.2052,   0.1959,   0.1277],\n",
      "         [  0.2782,   0.2918,   0.2747,   0.2853,   0.1268],\n",
      "         [  0.2498,   0.1963,   0.2137,   0.1955,   0.1390],\n",
      "         [  0.2281,   0.2246,   0.1926,   0.2142,   0.1152],\n",
      "         [  0.3099,   0.3195,   0.3062,   0.3131,   0.1571]]])\n",
      "[1.73425348]\n",
      "[1.57018927]\n",
      "[0.4615416]\n",
      "[1.35533556]\n",
      "[0.4581574]\n",
      "[1.18314176]\n",
      "[0.03481445]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9245, 0.9515, 0.8180, 0.9361, 0.8820, 0.8545, 0.9695],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.2532,   0.2627,   0.2784],\n",
      "         [  0.2961,   0.2822,   0.2754,   0.2478,   0.2789],\n",
      "         [  0.2418,   0.2257,   0.2052,   0.1959,   0.2411],\n",
      "         [  0.2782,   0.2918,   0.2747,   0.2853,   0.2774],\n",
      "         [  0.2498,   0.1963,   0.2137,   0.1955,   0.2759],\n",
      "         [  0.2281,   0.2246,   0.1926,   0.2142,   0.2381],\n",
      "         [  0.3099,   0.3195,   0.3062,   0.3131,   0.3153]]])\n",
      "[0.91670077]\n",
      "[0.73888932]\n",
      "[0.79302539]\n",
      "[1.04479031]\n",
      "[0.73228374]\n",
      "[0.85586826]\n",
      "[0.41792453]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8923, 0.9210, 0.9267, 0.9175, 0.8643, 0.7922, 0.9437],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.2702,   0.2532,   0.2627,   0.2784],\n",
      "         [  0.4447,   0.2822,   0.2754,   0.2478,   0.2789],\n",
      "         [  0.3849,   0.2257,   0.2052,   0.1959,   0.2411],\n",
      "         [  0.4215,   0.2918,   0.2747,   0.2853,   0.2774],\n",
      "         [  0.3855,   0.1963,   0.2137,   0.1955,   0.2759],\n",
      "         [  0.3385,   0.2246,   0.1926,   0.2142,   0.2381],\n",
      "         [  0.4645,   0.3195,   0.3062,   0.3131,   0.3153]]])\n",
      "[1.86145487]\n",
      "[1.51861001]\n",
      "[0.84629884]\n",
      "[1.56513154]\n",
      "[0.25156617]\n",
      "[1.07416911]\n",
      "[0.02086842]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9649, 0.9521, 0.9092, 0.9262, 0.7820, 0.8836, 0.9738],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.2532,   0.2627,   0.2784],\n",
      "         [  0.4447,   0.4320,   0.2754,   0.2478,   0.2789],\n",
      "         [  0.3849,   0.3695,   0.2052,   0.1959,   0.2411],\n",
      "         [  0.4215,   0.4380,   0.2747,   0.2853,   0.2774],\n",
      "         [  0.3855,   0.3178,   0.2137,   0.1955,   0.2759],\n",
      "         [  0.3385,   0.3591,   0.1926,   0.2142,   0.2381],\n",
      "         [  0.4645,   0.4789,   0.3062,   0.3131,   0.3153]]])\n",
      "[0.15147468]\n",
      "[0.11778679]\n",
      "[0.05467035]\n",
      "[0.53493621]\n",
      "[0.03015393]\n",
      "[0.52362401]\n",
      "[0.05105766]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9358, 0.9560, 0.8056, 0.9536, 0.7708, 0.8939, 0.9574],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.4053,   0.2627,   0.2784],\n",
      "         [  0.4447,   0.4320,   0.4306,   0.2478,   0.2789],\n",
      "         [  0.3849,   0.3695,   0.3336,   0.1959,   0.2411],\n",
      "         [  0.4215,   0.4380,   0.4294,   0.2853,   0.2774],\n",
      "         [  0.3855,   0.3178,   0.3296,   0.1955,   0.2759],\n",
      "         [  0.3385,   0.3591,   0.3312,   0.2142,   0.2381],\n",
      "         [  0.4645,   0.4789,   0.4636,   0.3131,   0.3153]]])\n",
      "[1.32432995]\n",
      "[0.21870187]\n",
      "[0.41661458]\n",
      "[1.50749058]\n",
      "[0.18137813]\n",
      "[0.63030705]\n",
      "[0.03422566]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9473, 0.9542, 0.9306, 0.9561, 0.8789, 0.8096, 0.9719],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.4053,   0.4191,   0.2784],\n",
      "         [  0.4447,   0.4320,   0.4306,   0.4048,   0.2789],\n",
      "         [  0.3849,   0.3695,   0.3336,   0.3486,   0.2411],\n",
      "         [  0.4215,   0.4380,   0.4294,   0.4426,   0.2774],\n",
      "         [  0.3855,   0.3178,   0.3296,   0.3351,   0.2759],\n",
      "         [  0.3385,   0.3591,   0.3312,   0.3389,   0.2381],\n",
      "         [  0.4645,   0.4789,   0.4636,   0.4740,   0.3153]]])\n",
      "[1.00447246]\n",
      "[1.04848191]\n",
      "[1.03643248]\n",
      "[1.02706106]\n",
      "[0.9797846]\n",
      "[0.86779999]\n",
      "[0.16226901]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9376, 0.9421, 0.9056, 0.9346, 0.8130, 0.9006, 0.9777],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.4053,   0.4191,   0.4279],\n",
      "         [  0.4447,   0.4320,   0.4306,   0.4048,   0.4294],\n",
      "         [  0.3849,   0.3695,   0.3336,   0.3486,   0.3850],\n",
      "         [  0.4215,   0.4380,   0.4294,   0.4426,   0.4199],\n",
      "         [  0.3855,   0.3178,   0.3296,   0.3351,   0.4005],\n",
      "         [  0.3385,   0.3591,   0.3312,   0.3389,   0.3802],\n",
      "         [  0.4645,   0.4789,   0.4636,   0.4740,   0.4743]]])\n",
      "[0.08763258]\n",
      "[0.05735593]\n",
      "[0.04264209]\n",
      "[1.05918316]\n",
      "[0.02122396]\n",
      "[0.28438902]\n",
      "[0.04140599]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8202, 0.8390, 0.8717, 0.8473, 0.8760, 0.7509, 0.9501],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.4235,   0.4053,   0.4191,   0.4279],\n",
      "         [  0.5568,   0.4320,   0.4306,   0.4048,   0.4294],\n",
      "         [  0.5126,   0.3695,   0.3336,   0.3486,   0.3850],\n",
      "         [  0.5456,   0.4380,   0.4294,   0.4426,   0.4199],\n",
      "         [  0.5217,   0.3178,   0.3296,   0.3351,   0.4005],\n",
      "         [  0.4413,   0.3591,   0.3312,   0.3389,   0.3802],\n",
      "         [  0.6164,   0.4789,   0.4636,   0.4740,   0.4743]]])\n",
      "[0.09461352]\n",
      "[0.0442874]\n",
      "[0.09818073]\n",
      "[0.50338873]\n",
      "[0.04037575]\n",
      "[0.15212996]\n",
      "[0.27100685]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8714, 0.9032, 0.8018, 0.8298, 0.8164, 0.6976, 0.9675],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.4053,   0.4191,   0.4279],\n",
      "         [  0.5568,   0.5689,   0.4306,   0.4048,   0.4294],\n",
      "         [  0.5126,   0.4912,   0.3336,   0.3486,   0.3850],\n",
      "         [  0.5456,   0.5670,   0.4294,   0.4426,   0.4199],\n",
      "         [  0.5217,   0.4305,   0.3296,   0.3351,   0.4005],\n",
      "         [  0.4413,   0.4599,   0.3312,   0.3389,   0.3802],\n",
      "         [  0.6164,   0.6357,   0.4636,   0.4740,   0.4743]]])\n",
      "[0.15080419]\n",
      "[0.08909571]\n",
      "[0.09755479]\n",
      "[0.36079898]\n",
      "[0.02271132]\n",
      "[0.35420229]\n",
      "[0.14253775]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8421, 0.7956, 0.7134, 0.7725, 0.7844, 0.5895, 0.9612],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.5333,   0.4191,   0.4279],\n",
      "         [  0.5568,   0.5689,   0.5524,   0.4048,   0.4294],\n",
      "         [  0.5126,   0.4912,   0.4386,   0.3486,   0.3850],\n",
      "         [  0.5456,   0.5670,   0.5420,   0.4426,   0.4199],\n",
      "         [  0.5217,   0.4305,   0.4344,   0.3351,   0.4005],\n",
      "         [  0.4413,   0.4599,   0.4127,   0.3389,   0.3802],\n",
      "         [  0.6164,   0.6357,   0.6197,   0.4740,   0.4743]]])\n",
      "[0.12940099]\n",
      "[0.06675622]\n",
      "[0.09235374]\n",
      "[0.35689947]\n",
      "[0.0253621]\n",
      "[0.30688407]\n",
      "[0.16710192]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6986, 0.8622, 0.7454, 0.8669, 0.8773, 0.7800, 0.9783],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.5333,   0.5214,   0.4279],\n",
      "         [  0.5568,   0.5689,   0.5524,   0.5410,   0.4294],\n",
      "         [  0.5126,   0.4912,   0.4386,   0.4439,   0.3850],\n",
      "         [  0.5456,   0.5670,   0.5420,   0.5721,   0.4199],\n",
      "         [  0.5217,   0.4305,   0.4344,   0.4709,   0.4005],\n",
      "         [  0.4413,   0.4599,   0.4127,   0.4493,   0.3802],\n",
      "         [  0.6164,   0.6357,   0.6197,   0.6337,   0.4743]]])\n",
      "[0.07029603]\n",
      "[0.24464717]\n",
      "[0.0163489]\n",
      "[0.3198633]\n",
      "[0.05215768]\n",
      "[0.08488725]\n",
      "[0.96870946]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8634, 0.8190, 0.8260, 0.8389, 0.7849, 0.8943, 0.9487],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.5333,   0.5214,   0.5524],\n",
      "         [  0.5568,   0.5689,   0.5524,   0.5410,   0.5540],\n",
      "         [  0.5126,   0.4912,   0.4386,   0.4439,   0.4982],\n",
      "         [  0.5456,   0.5670,   0.5420,   0.5721,   0.5305],\n",
      "         [  0.5217,   0.4305,   0.4344,   0.4709,   0.5012],\n",
      "         [  0.4413,   0.4599,   0.4127,   0.4493,   0.5159],\n",
      "         [  0.6164,   0.6357,   0.6197,   0.6337,   0.6264]]])\n",
      "[0.13091599]\n",
      "[0.07334752]\n",
      "[0.07823339]\n",
      "[0.32334844]\n",
      "[0.01952754]\n",
      "[0.26788532]\n",
      "[0.10005937]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8961, 0.8332, 0.5096, 0.7289, 0.8864, 0.8770, 0.9349],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.5579,   0.5333,   0.5214,   0.5524],\n",
      "         [  0.6799,   0.5689,   0.5524,   0.5410,   0.5540],\n",
      "         [  0.5423,   0.4912,   0.4386,   0.4439,   0.4982],\n",
      "         [  0.6478,   0.5670,   0.5420,   0.5721,   0.5305],\n",
      "         [  0.6570,   0.4305,   0.4344,   0.4709,   0.5012],\n",
      "         [  0.5634,   0.4599,   0.4127,   0.4493,   0.5159],\n",
      "         [  0.7651,   0.6357,   0.6197,   0.6337,   0.6264]]])\n",
      "[0.08522691]\n",
      "[0.15789888]\n",
      "[0.02588735]\n",
      "[0.39712844]\n",
      "[0.07439873]\n",
      "[0.07620783]\n",
      "[0.98786029]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7922, 0.8592, 0.4868, 0.8406, 0.8783, 0.6149, 0.9601],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.5333,   0.5214,   0.5524],\n",
      "         [  0.6799,   0.7017,   0.5524,   0.5410,   0.5540],\n",
      "         [  0.5423,   0.5245,   0.4386,   0.4439,   0.4982],\n",
      "         [  0.6478,   0.6911,   0.5420,   0.5721,   0.5305],\n",
      "         [  0.6570,   0.5642,   0.4344,   0.4709,   0.5012],\n",
      "         [  0.5634,   0.5354,   0.4127,   0.4493,   0.5159],\n",
      "         [  0.7651,   0.7884,   0.6197,   0.6337,   0.6264]]])\n",
      "[0.58977323]\n",
      "[0.21970921]\n",
      "[0.27124005]\n",
      "[0.43793328]\n",
      "[0.27141379]\n",
      "[0.57146329]\n",
      "[0.04819385]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9145, 0.8583, 0.6892, 0.8871, 0.8743, 0.8618, 0.9765],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.6758,   0.5214,   0.5524],\n",
      "         [  0.6799,   0.7017,   0.6889,   0.5410,   0.5540],\n",
      "         [  0.5423,   0.5245,   0.5219,   0.4439,   0.4982],\n",
      "         [  0.6478,   0.6911,   0.6755,   0.5721,   0.5305],\n",
      "         [  0.6570,   0.5642,   0.5725,   0.4709,   0.5012],\n",
      "         [  0.5634,   0.5354,   0.5366,   0.4493,   0.5159],\n",
      "         [  0.7651,   0.7884,   0.7794,   0.6337,   0.6264]]])\n",
      "[0.48612046]\n",
      "[0.1472635]\n",
      "[0.23670529]\n",
      "[0.48414001]\n",
      "[0.22888138]\n",
      "[0.44050232]\n",
      "[0.03540915]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8992, 0.8766, 0.5400, 0.9115, 0.8632, 0.7303, 0.9723],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.6758,   0.6622,   0.5524],\n",
      "         [  0.6799,   0.7017,   0.6889,   0.6784,   0.5540],\n",
      "         [  0.5423,   0.5245,   0.5219,   0.5015,   0.4982],\n",
      "         [  0.6478,   0.6911,   0.6755,   0.7137,   0.5305],\n",
      "         [  0.6570,   0.5642,   0.5725,   0.6012,   0.5012],\n",
      "         [  0.5634,   0.5354,   0.5366,   0.5512,   0.5159],\n",
      "         [  0.7651,   0.7884,   0.7794,   0.7917,   0.6264]]])\n",
      "[0.57643014]\n",
      "[0.21156527]\n",
      "[0.23331184]\n",
      "[0.59615867]\n",
      "[0.25297331]\n",
      "[0.46097798]\n",
      "[0.1156767]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8760, 0.8772, 0.0378, 0.6912, 0.8610, 0.7559, 0.9728],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.6758,   0.6622,   0.6846],\n",
      "         [  0.6799,   0.7017,   0.6889,   0.6784,   0.6856],\n",
      "         [  0.5423,   0.5245,   0.5219,   0.5015,   0.4673],\n",
      "         [  0.6478,   0.6911,   0.6755,   0.7137,   0.6237],\n",
      "         [  0.6570,   0.5642,   0.5725,   0.6012,   0.6336],\n",
      "         [  0.5634,   0.5354,   0.5366,   0.5512,   0.6211],\n",
      "         [  0.7651,   0.7884,   0.7794,   0.7917,   0.7833]]])\n",
      "[0.10715771]\n",
      "[0.33159249]\n",
      "[0.0197274]\n",
      "[0.382551]\n",
      "[0.0900454]\n",
      "[0.11822554]\n",
      "[0.92871586]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9567, 0.8587, 0.8067, 0.9281, 0.5900, 0.8749, 0.9704],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.6797,   0.6758,   0.6622,   0.6846],\n",
      "         [  0.8157,   0.7017,   0.6889,   0.6784,   0.6856],\n",
      "         [  0.6619,   0.5245,   0.5219,   0.5015,   0.4673],\n",
      "         [  0.7941,   0.6911,   0.6755,   0.7137,   0.6237],\n",
      "         [  0.7122,   0.5642,   0.5725,   0.6012,   0.6336],\n",
      "         [  0.6945,   0.5354,   0.5366,   0.5512,   0.6211],\n",
      "         [  0.9247,   0.7884,   0.7794,   0.7917,   0.7833]]])\n",
      "[1.41675444]\n",
      "[0.24371413]\n",
      "[0.44155784]\n",
      "[1.45378398]\n",
      "[0.26089476]\n",
      "[0.74008987]\n",
      "[0.14834033]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9067, 0.8935, 0.8809, 0.8907, 0.7977, 0.7969, 0.9539],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.6758,   0.6622,   0.6846],\n",
      "         [  0.8157,   0.8459,   0.6889,   0.6784,   0.6856],\n",
      "         [  0.6619,   0.6625,   0.5219,   0.5015,   0.4673],\n",
      "         [  0.7941,   0.8301,   0.6755,   0.7137,   0.6237],\n",
      "         [  0.7122,   0.6747,   0.5725,   0.6012,   0.6336],\n",
      "         [  0.6945,   0.6588,   0.5366,   0.5512,   0.6211],\n",
      "         [  0.9247,   0.9435,   0.7794,   0.7917,   0.7833]]])\n",
      "[0.10853332]\n",
      "[0.03323938]\n",
      "[0.10343785]\n",
      "[1.12715287]\n",
      "[0.02280728]\n",
      "[0.19340472]\n",
      "[0.14759011]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8891, 0.9157, 0.8950, 0.9405, 0.8916, 0.8023, 0.9625],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.8209,   0.6622,   0.6846],\n",
      "         [  0.8157,   0.8459,   0.8386,   0.6784,   0.6856],\n",
      "         [  0.6619,   0.6625,   0.6668,   0.5015,   0.4673],\n",
      "         [  0.7941,   0.8301,   0.8298,   0.7137,   0.6237],\n",
      "         [  0.7122,   0.6747,   0.6974,   0.6012,   0.6336],\n",
      "         [  0.6945,   0.6588,   0.6621,   0.5512,   0.6211],\n",
      "         [  0.9247,   0.9435,   0.9380,   0.7917,   0.7833]]])\n",
      "[0.92758152]\n",
      "[0.86330696]\n",
      "[0.68388785]\n",
      "[0.89206254]\n",
      "[0.91026449]\n",
      "[0.72562835]\n",
      "[0.19317769]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8721, 0.7618, 0.9061, 0.9153, 0.5831, 0.6935, 0.9306],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.8209,   0.8019,   0.6846],\n",
      "         [  0.8157,   0.8459,   0.8386,   0.7986,   0.6856],\n",
      "         [  0.6619,   0.6625,   0.6668,   0.6450,   0.4673],\n",
      "         [  0.7941,   0.8301,   0.8298,   0.8624,   0.6237],\n",
      "         [  0.7122,   0.6747,   0.6974,   0.6730,   0.6336],\n",
      "         [  0.6945,   0.6588,   0.6621,   0.6533,   0.6211],\n",
      "         [  0.9247,   0.9435,   0.9380,   0.9437,   0.7833]]])\n",
      "[1.79186475]\n",
      "[1.85555563]\n",
      "[0.95505196]\n",
      "[1.41362753]\n",
      "[0.33102971]\n",
      "[1.36017382]\n",
      "[0.05831114]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9207, 0.8904, 0.9067, 0.9356, 0.8385, 0.7727, 0.9440],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.8209,   0.8019,   0.8323],\n",
      "         [  0.8157,   0.8459,   0.8386,   0.7986,   0.8284],\n",
      "         [  0.6619,   0.6625,   0.6668,   0.6450,   0.6114],\n",
      "         [  0.7941,   0.8301,   0.8298,   0.8624,   0.7737],\n",
      "         [  0.7122,   0.6747,   0.6974,   0.6730,   0.7603],\n",
      "         [  0.6945,   0.6588,   0.6621,   0.6533,   0.7391],\n",
      "         [  0.9247,   0.9435,   0.9380,   0.9437,   0.9383]]])\n",
      "[1.53329401]\n",
      "[1.61639087]\n",
      "[0.81647532]\n",
      "[1.36927962]\n",
      "[0.2425125]\n",
      "[0.91654456]\n",
      "[0.04919384]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "reps=5\n",
    "nn=[10,20,30,40,50,60]\n",
    "R2=torch.zeros(7,len(nn),7,reps)\n",
    "ISE=torch.zeros(7,len(nn),7,reps)\n",
    "Ti=torch.zeros(7,len(nn),reps)\n",
    "\n",
    "for num, n in enumerate(nn):\n",
    "    for k in range(len(emulators)):\n",
    "        emulators2=emulators.copy()\n",
    "        emulators2.pop(k)\n",
    "        print(len(emulators2))\n",
    "\n",
    "        X_train = train_input[k]\n",
    "        y_train = train_output[k]\n",
    "        X_test = test_input[k]\n",
    "        y_test = test_output[k]\n",
    "        \n",
    "        for i in range(reps):\n",
    "\n",
    "            b=np.random.choice(range(X_train.shape[0]),n,replace=False)\n",
    "\n",
    "            start = time.time()\n",
    "            model_f=GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"linear\",training_iter=500)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_f.R2_sample(X_test,y_test,1000)\n",
    "            R2[0,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[0,num,:,i]+=model_f.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[0,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "\n",
    "            em=np.random.randint(len(emulators2))\n",
    "            start = time.time()\n",
    "            model_dc_1 = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=torch.tensor([[1],[1],[1],[1],[1],[1],[1]]))\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_1.R2_sample(X_test,y_test,1000)\n",
    "            R2[1,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[1,num,:,i]+=model_dc_1.ISE(X_test,y_test)/(len(emulators))\n",
    "            print(model_dc_1.R2(X_test,y_test))\n",
    "            print(R2[1])\n",
    "\n",
    "            Ti[1,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            m0 = emulators2[em].predict(X_train[b,:])\n",
    "            a_d=np.zeros((y_train.shape[1],1))\n",
    "            for l in range(y_train.shape[1]):\n",
    "                result = scipy.optimize.minimize(proxy, 1, args=(y_train[b,:],m0,l), method='Nelder-Mead', tol=1e-8)\n",
    "                print(result.x)\n",
    "                a_d[l]=result.x\n",
    "            a_d=torch.tensor(a_d)\n",
    "            model_dc_reg = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_reg.R2_sample(X_test,y_test,1000)\n",
    "            R2[2,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[2,num,:,i]+=model_dc_reg.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[2,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_learned = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]])\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[3,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[3,num,:,i]+=model_dc_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[3,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_all = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_all.R2_sample(X_test,y_test,1000)\n",
    "            R2[4,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[4,num,:,i]+=model_dc_all.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[4,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            a_d=torch.zeros((y_train.shape[1],len(emulators2)))\n",
    "            for j in range(y_train.shape[1]):\n",
    "                m0=m0_mat(y_train[b],emulators2,X_train[b],j)\n",
    "                # fit to an order-3 polynomial data\n",
    "                y_t=(y_train[b,j]-y_train.mean(axis=0)[j])/y_train.std(axis=0)[j]\n",
    "                model = model.fit(m0.detach().numpy(), y_t.detach().numpy())\n",
    "                a_d[j]=torch.tensor(model.named_steps['lasso'].coef_)\n",
    "\n",
    "\n",
    "            model_dc_lasso=GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso.R2_sample(X_test,y_test,1000)\n",
    "            R2[5,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[5,num,:,i]+=model_dc_lasso.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[5,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_lasso_learned=GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d,a_indicator=True)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[6,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[6,num,:,i]+=model_dc_lasso_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[6,num,i]+=(end-start)/(len(emulators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdc8cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=2\n",
    "lim=2\n",
    "y_lim=[0.7,1.01]\n",
    "plt.plot(nn[lim:],R2.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.legend(['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2.mean(axis=3)[i,lim:,o]+R2.std(axis=3)[i,lim:,o], R2.mean(axis=3)[i,lim:,o]-R2.std(axis=3)[i,lim:,o],alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d325f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_save = R2.reshape(7,len(nn)*reps*y_train.shape[1])\n",
    "\n",
    "np.savetxt(\"DiscrepR2TrainNVaryDefinitiveAtria.csv\", R2_save.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d731ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
