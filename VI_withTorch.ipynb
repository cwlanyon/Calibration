{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2d8f72-376b-4f7b-b0e0-d1b70bafe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "yobs = 8.0\n",
    "sigma2 = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd90672-e817-4cd8-8c99-611507c7f5b4",
   "metadata": {},
   "source": [
    "First we'll learn how to get the variational posterior for $x | y$ for a single value of y. We have\n",
    "$$y= 4x-x^2/2+N(0,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350d8cd-93ad-464d-aceb-d676326f8bd2",
   "metadata": {},
   "source": [
    "## Variational inference\n",
    "\n",
    "Maximize the ELBO\n",
    "$$ELBO(q) = E[\\log p(y | x)] − KL (q(x)|| p(x)).$$\n",
    "\n",
    "We'll use $q(x) = N(m, s^2)$ as the variational family\n",
    "In this case, the ELBO can be computed exactly, but its a pain to do. So I've approximated the E[log p(y | x)] with a Monte Carlo sum.\n",
    "$\\Phi=(m, \\log s^2)$ are the variational parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7d49-6e46-4465-8d7f-297c6fd1edae",
   "metadata": {},
   "source": [
    "KL (q(x) || p(x)) = E[log q(x)] − E[log p(x)]\n",
    "where expectations are with respect to q.\n",
    "\n",
    "Note, if $p(x) \\propto 1$, then $E\\log p(x)$ does not depend on $\\phi$ so can be ignored\n",
    "If $q(x) =N(m, \\sigma^2)$, then $E[\\log q(x)] = -0.5 \\log(2 \\pi \\sigma^2) - 1/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa68f655-7ebf-4b2f-b77a-cb88572bcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(log_var):   \n",
    "    return(-0.5*np.log(2.0*math.pi)-0.5*log_var-0.5)\n",
    "\n",
    "def f(x):\n",
    "    return(4.*x-0.5*torch.pow(x,2.))\n",
    "# How do we define functions? x needs declaring?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ZtoX(m, log_s2):\n",
    "    #reparameterization trick\n",
    "    return(Z*torch.exp(log_s2/2.)+m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c5ab3e-19a1-4afb-bd58-5a6c8ed37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_dnorm(x, mean, var):\n",
    "    return(-(x-mean).pow(2)/(2.*var)-0.5*np.log(2*math.pi*var))\n",
    "    # checked - could used built-in torch dnorm\n",
    "\n",
    "\n",
    "\n",
    "def Eloglike(m,log_s2):\n",
    "    X= ZtoX(m, log_s2)\n",
    "    \n",
    "    #print(X)\n",
    "    #Rewrite with f\n",
    "    loglikes = log_dnorm(f(X), yobs, sigma2) # observation likelihood\n",
    "    #-(yobs- 4.*X+0.5*X.pow(2)).pow(2)    #f(ZtoX(phi)),2.)\n",
    "    \n",
    "    return(loglikes.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da33ee10-470d-4389-8e8c-723147104603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.7689577341079712 m= 4.89589786529541 s2= 0.10756976157426834\n",
      "199 0.46154969930648804 m= 4.566242694854736 s2= 0.1664574295282364\n",
      "299 0.23953628540039062 m= 4.369981288909912 s2= 0.25673651695251465\n",
      "399 0.041483163833618164 m= 4.22088623046875 s2= 0.38433316349983215\n",
      "499 -0.08927857875823975 m= 4.112062931060791 s2= 0.5364567637443542\n",
      "599 -0.131186842918396 m= 4.0486979484558105 s2= 0.6696256995201111\n",
      "699 -0.20196151733398438 m= 4.015183925628662 s2= 0.7517797350883484\n",
      "799 -0.18288254737854004 m= 3.999823808670044 s2= 0.7941275238990784\n",
      "899 -0.1509265899658203 m= 3.99350643157959 s2= 0.8094796538352966\n",
      "999 -0.11520564556121826 m= 3.9968996047973633 s2= 0.8120530843734741\n",
      "1099 -0.1278231143951416 m= 3.999289035797119 s2= 0.8111606240272522\n",
      "1199 -0.1050577163696289 m= 4.003118515014648 s2= 0.8121963739395142\n",
      "1299 -0.11719095706939697 m= 4.006368160247803 s2= 0.8148861527442932\n",
      "1399 -0.11520218849182129 m= 4.001572132110596 s2= 0.8133485913276672\n",
      "1499 -0.1510089635848999 m= 4.001549243927002 s2= 0.8126535415649414\n",
      "1599 -0.1691046953201294 m= 4.003600597381592 s2= 0.816135585308075\n",
      "1699 -0.17779552936553955 m= 3.9968185424804688 s2= 0.8144673109054565\n",
      "1799 -0.1782667636871338 m= 3.998157262802124 s2= 0.8120572566986084\n",
      "1899 -0.13637864589691162 m= 3.9989943504333496 s2= 0.812299907207489\n",
      "1999 -0.14968359470367432 m= 4.003271102905273 s2= 0.8204460144042969\n",
      "2099 -0.15784668922424316 m= 3.9972479343414307 s2= 0.8209784030914307\n",
      "2199 -0.1826624870300293 m= 3.999082565307617 s2= 0.8194177150726318\n",
      "2299 -0.17462801933288574 m= 4.00311803817749 s2= 0.8178103566169739\n",
      "2399 -0.1439899206161499 m= 4.003312587738037 s2= 0.814035177230835\n",
      "2499 -0.15655040740966797 m= 3.9980309009552 s2= 0.8150253295898438\n",
      "2599 -0.10682821273803711 m= 4.003666400909424 s2= 0.8134885430335999\n",
      "2699 -0.16158950328826904 m= 3.999195098876953 s2= 0.8194049596786499\n",
      "2799 -0.09639990329742432 m= 3.9993810653686523 s2= 0.8151923418045044\n",
      "2899 -0.14092350006103516 m= 3.9994442462921143 s2= 0.817929744720459\n",
      "2999 -0.11489927768707275 m= 4.001426696777344 s2= 0.8153133392333984\n",
      "3099 -0.11987519264221191 m= 4.004286766052246 s2= 0.813713788986206\n",
      "3199 -0.06858432292938232 m= 3.9954404830932617 s2= 0.8152878880500793\n",
      "3299 -0.15034997463226318 m= 4.0009002685546875 s2= 0.8162318468093872\n",
      "3399 -0.1014639139175415 m= 4.002735614776611 s2= 0.8153712153434753\n",
      "3499 -0.10603868961334229 m= 3.9980523586273193 s2= 0.8107100129127502\n",
      "3599 -0.17891788482666016 m= 4.002513885498047 s2= 0.8163289427757263\n",
      "3699 -0.13116967678070068 m= 3.9989359378814697 s2= 0.814733624458313\n",
      "3799 -0.1943575143814087 m= 3.997499465942383 s2= 0.8146112561225891\n",
      "3899 -0.1356726884841919 m= 3.997775077819824 s2= 0.81662517786026\n",
      "3999 -0.19023609161376953 m= 4.003026008605957 s2= 0.8160912394523621\n",
      "4099 -0.16033577919006348 m= 3.997030019760132 s2= 0.8129851818084717\n",
      "4199 -0.14008665084838867 m= 4.002264022827148 s2= 0.8125240206718445\n",
      "4299 -0.10447633266448975 m= 4.001151084899902 s2= 0.8158529996871948\n",
      "4399 -0.18224763870239258 m= 3.999232530593872 s2= 0.818670392036438\n",
      "4499 -0.15543639659881592 m= 3.995567798614502 s2= 0.81789630651474\n",
      "4599 -0.17368388175964355 m= 3.9992778301239014 s2= 0.8151642084121704\n",
      "4699 -0.13485264778137207 m= 4.00807523727417 s2= 0.8191443681716919\n",
      "4799 -0.1721569299697876 m= 4.005934715270996 s2= 0.8190878033638\n",
      "4899 -0.12841343879699707 m= 4.005003452301025 s2= 0.8157981038093567\n",
      "4999 -0.12825775146484375 m= 4.00314998626709 s2= 0.8173757791519165\n",
      "5099 -0.19013714790344238 m= 3.9993298053741455 s2= 0.8151170611381531\n",
      "5199 -0.190621018409729 m= 3.9956021308898926 s2= 0.8136441111564636\n",
      "5299 -0.1077573299407959 m= 3.9956536293029785 s2= 0.8192011117935181\n",
      "5399 -0.13127899169921875 m= 3.996619462966919 s2= 0.8189887404441833\n",
      "5499 -0.13973772525787354 m= 3.9983928203582764 s2= 0.8145753741264343\n",
      "5599 -0.156890869140625 m= 4.0000834465026855 s2= 0.8169002532958984\n",
      "5699 -0.13069140911102295 m= 3.9951000213623047 s2= 0.8171063661575317\n",
      "5799 -0.1839996576309204 m= 3.9967668056488037 s2= 0.8187827467918396\n",
      "5899 -0.12098956108093262 m= 3.9969446659088135 s2= 0.821540117263794\n",
      "5999 -0.165868878364563 m= 3.997647762298584 s2= 0.8186938166618347\n",
      "6099 -0.11727142333984375 m= 3.995760917663574 s2= 0.8168244361877441\n",
      "6199 -0.13168692588806152 m= 3.9997167587280273 s2= 0.8182384371757507\n",
      "6299 -0.12579751014709473 m= 4.0068793296813965 s2= 0.814538300037384\n",
      "6399 -0.1881096363067627 m= 4.001120090484619 s2= 0.8172900676727295\n",
      "6499 -0.15719091892242432 m= 4.008418560028076 s2= 0.8162546157836914\n",
      "6599 -0.12187302112579346 m= 4.002597332000732 s2= 0.8162413239479065\n",
      "6699 -0.1748366355895996 m= 4.0000529289245605 s2= 0.820163905620575\n",
      "6799 -0.13632440567016602 m= 4.000675678253174 s2= 0.8198825716972351\n",
      "6899 -0.18263745307922363 m= 4.003565788269043 s2= 0.8171080350875854\n",
      "6999 -0.14185047149658203 m= 3.99759840965271 s2= 0.8171453475952148\n",
      "7099 -0.10734093189239502 m= 4.002349376678467 s2= 0.815666913986206\n",
      "7199 -0.1594679355621338 m= 4.004005432128906 s2= 0.8141860961914062\n",
      "7299 -0.11435079574584961 m= 4.0006184577941895 s2= 0.8140497207641602\n",
      "7399 -0.1646416187286377 m= 3.9982004165649414 s2= 0.8164071440696716\n",
      "7499 -0.1552262306213379 m= 3.9981279373168945 s2= 0.8188409805297852\n",
      "7599 -0.2008572816848755 m= 3.9966440200805664 s2= 0.8178203105926514\n",
      "7699 -0.12094533443450928 m= 3.9980509281158447 s2= 0.8159781694412231\n",
      "7799 -0.1717240810394287 m= 3.998072385787964 s2= 0.817299485206604\n",
      "7899 -0.15600132942199707 m= 4.004307746887207 s2= 0.8134036064147949\n",
      "7999 -0.11053931713104248 m= 4.004106521606445 s2= 0.8163560032844543\n",
      "8099 -0.14984381198883057 m= 3.9989583492279053 s2= 0.8165348768234253\n",
      "8199 -0.15002918243408203 m= 3.9987592697143555 s2= 0.8155079483985901\n",
      "8299 -0.19069218635559082 m= 4.008595943450928 s2= 0.8162837624549866\n",
      "8399 -0.07499432563781738 m= 3.9958460330963135 s2= 0.8153322339057922\n",
      "8499 -0.15515470504760742 m= 4.00261116027832 s2= 0.8152530789375305\n",
      "8599 -0.13209986686706543 m= 4.004208087921143 s2= 0.8196547627449036\n",
      "8699 -0.1459110975265503 m= 4.000727653503418 s2= 0.8125599026679993\n",
      "8799 -0.13690614700317383 m= 4.002167224884033 s2= 0.8227424025535583\n",
      "8899 -0.16089773178100586 m= 3.9992446899414062 s2= 0.818468451499939\n",
      "8999 -0.13878178596496582 m= 4.0001912117004395 s2= 0.8193792700767517\n",
      "9099 -0.13258743286132812 m= 4.001811504364014 s2= 0.8182324171066284\n",
      "9199 -0.19209551811218262 m= 4.0006585121154785 s2= 0.8173198103904724\n",
      "9299 -0.20028674602508545 m= 3.999723196029663 s2= 0.8135926127433777\n",
      "9399 -0.175217866897583 m= 4.002823829650879 s2= 0.8164435625076294\n",
      "9499 -0.17171430587768555 m= 3.9998581409454346 s2= 0.8175362944602966\n",
      "9599 -0.18354249000549316 m= 3.9975104331970215 s2= 0.8180894255638123\n",
      "9699 -0.10479211807250977 m= 3.999906063079834 s2= 0.8121917843818665\n",
      "9799 -0.10446012020111084 m= 3.9995906352996826 s2= 0.8120498657226562\n",
      "9899 -0.1939018964767456 m= 4.002811431884766 s2= 0.81874680519104\n",
      "9999 -0.16034436225891113 m= 4.000967502593994 s2= 0.8175416588783264\n",
      "Result: p(x|y) = N(4.001859664916992, 0.8177223801612854) \n"
     ]
    }
   ],
   "source": [
    "### initialize the variational parameters\n",
    "m = torch.full((), 10.,dtype=dtype, requires_grad=True, device=device)\n",
    "log_s2 = torch.full((),np.log(15.), requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "# Samples fixed here - but try adding them into the loop\n",
    "nsamples = 1000\n",
    "#Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(10000):\n",
    "    Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "\n",
    "    negELBO = -Eloglike(m,log_s2)+KL(log_s2)\n",
    "    if t % 100 == 99:\n",
    "        print(t, negELBO.item(), 'm=', m.item(), 's2=', log_s2.exp().item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    negELBO.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        m -= learning_rate * m.grad\n",
    "        log_s2 -= learning_rate * log_s2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        m.grad = None\n",
    "        log_s2.grad = None\n",
    "        \n",
    "print(f'Result: p(x|y) = N({m.item()}, {log_s2.exp().item()}) ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec3f63-f945-48ee-b751-760320f70606",
   "metadata": {},
   "source": [
    "Next steps\n",
    "\n",
    "- amortized inference\n",
    "- Can we use GPtorch as function?\n",
    "- write as a class with a fwd and backward method\n",
    "- use torch.nn class to define params?\n",
    "- use random Z at each stage? Should the number of samples increase as we converge?\n",
    "- add prior for x\n",
    "- change f to avoid bimodal posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb1605-f104-45fa-8a63-4971a7356851",
   "metadata": {},
   "source": [
    "## Amortized Variational Inference\n",
    "\n",
    "Let's assume $q(x|y)$ can be modelled as $N(m(y), s2(y))$ where $m(y)$ and $s2(y)$ are both modelled as neural networks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ddc48-71b5-4673-ab5e-35d2328c59d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
