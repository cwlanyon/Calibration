{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f2d8f72-376b-4f7b-b0e0-d1b70bafe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "from matplotlib import pyplot as plt\n",
    "import GPE_ensemble as GPE\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from  torch.distributions import normal\n",
    "\n",
    "yobs = 8.0\n",
    "sigma2 = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd90672-e817-4cd8-8c99-611507c7f5b4",
   "metadata": {},
   "source": [
    "First we'll learn how to get the variational posterior for $x | y$ for a single value of y. We have\n",
    "$$y= 4x-x^2/2+N(0,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350d8cd-93ad-464d-aceb-d676326f8bd2",
   "metadata": {},
   "source": [
    "## Variational inference\n",
    "\n",
    "Maximize the ELBO\n",
    "$$ELBO(q) = E[\\log p(y | x)] − KL (q(x)|| p(x)).$$\n",
    "\n",
    "We'll use $q(x) = N(m, s^2)$ as the variational family\n",
    "In this case, the ELBO can be computed exactly, but its a pain to do. So I've approximated the E[log p(y | x)] with a Monte Carlo sum.\n",
    "$\\Phi=(m, \\log s^2)$ are the variational parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7d49-6e46-4465-8d7f-297c6fd1edae",
   "metadata": {},
   "source": [
    "KL (q(x) || p(x)) = E[log q(x)] − E[log p(x)]\n",
    "where expectations are with respect to q.\n",
    "\n",
    "Note, if $p(x) \\propto 1$, then $E\\log p(x)$ does not depend on $\\phi$ so can be ignored\n",
    "If $q(x) =N(m, \\sigma^2)$, then $E[\\log q(x)] = -0.5 \\log(2 \\pi \\sigma^2) - 1/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525b9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p=3\n",
    "\n",
    "rl = -10\n",
    "ru=20\n",
    "\n",
    "obs_error = 0.0001\n",
    "\n",
    "x=torch.linspace(rl,ru,p)\n",
    "\n",
    "b=0.5\n",
    "\n",
    "def lin(x):\n",
    "    y = b*x\n",
    "    return y\n",
    "\n",
    "y = lin(x) \n",
    "\n",
    "plt.plot(x[:,None],y[:,None],'o')\n",
    "\n",
    "emulator = GPE.ensemble(x[:,None],y[:,None],mean_func=\"constant\",training_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dcf85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4739272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(m,log_s2,x,emulator,y,prior_mean,prior_cov,obs_error):\n",
    "    param=[m,log_s2]\n",
    "    L=torch.zeros((x.shape[0],x.shape[0]))\n",
    "    mu = torch.tensor((param[0:x.shape[0]]))\n",
    "    L=L.diagonal_scatter(torch.exp(torch.tensor(param[x.shape[0]:2*x.shape[0]])),0)\n",
    "    #L[1,0]=param[6]\n",
    "   # L[2,0]=param[7]\n",
    "   # L[2,1] =param[8]\n",
    "    covar = torch.matmul(L,L.T)\n",
    "    z=x*torch.exp(log_s2/2.)+m\n",
    "    \n",
    "    z=z.T\n",
    "    \n",
    "    mc_int = torch.sum(emulator.ensemble_log_likelihood_obs_error(z,y,obs_error)+torch.log(x_prior(z,prior_mean,prior_cov)).squeeze())\n",
    "        #mc_int +=-np.log(np.sum(((emulator.predict(z.iloc[[i]]).detach().numpy()-y.values)**2)))+np.log(x_prior(z.iloc[[i]],prior_mean,prior_cov))\n",
    "        #mc_int += (np.sum(np.log(emulator.ensemble_likelihood(z.iloc[[i]],y)))+np.log(x_prior(z.iloc[[i]],prior_mean,prior_cov)))\n",
    "    \n",
    "    lb = mc_int/x.shape[1] - q_prior(covar)\n",
    "    #print(mc_int/x.shape[1])\n",
    "    #print(-q_prior(covar))\n",
    "    #print(np.mean(z,axis=0))\n",
    "    #print(-lb)\n",
    "    return -lb\n",
    "\n",
    "def x_prior(x,mean,cov):\n",
    "\n",
    "    #var = scipy.stats.multivariate_normal(mean=mean, cov=cov)\n",
    "    #val1 = var.pdf(x)\n",
    "    dist = normal.Normal(loc=mean, scale=torch.sqrt(cov))\n",
    "    val = torch.exp(dist.log_prob(x))\n",
    "    return val\n",
    "\n",
    "def q_prior(covar):\n",
    "    qp = -(covar.shape[0]/2)*(1+torch.log(torch.tensor(2*torch.pi)))-0.5*torch.log(torch.linalg.det(covar))\n",
    "    return qp\n",
    "\n",
    "def f_likelihood(x,y,f,sigma2):\n",
    "    \n",
    "    likelihood_manual=-0.5*((f(x) - y)**2)/(sigma2)- 0.5*torch.log(2*np.pi)-0.5*torch.log(sigma2)\n",
    "    return likelihood_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa68f655-7ebf-4b2f-b77a-cb88572bcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(log_var):   \n",
    "    return(-0.5*np.log(2.0*math.pi)-0.5*log_var-0.5)\n",
    "\n",
    "def f(x):\n",
    "    return(4.*x-0.5*torch.pow(x,2.))\n",
    "# How do we define functions? x needs declaring?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ZtoX(m, log_s2):\n",
    "    #reparameterization trick\n",
    "    return(Z*torch.exp(log_s2/2.)+m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c5ab3e-19a1-4afb-bd58-5a6c8ed37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_dnorm(x, mean, var):\n",
    "    return(-(x-mean).pow(2)/(2.*var)-0.5*np.log(2*math.pi*var))\n",
    "    # checked - could used built-in torch dnorm\n",
    "\n",
    "\n",
    "\n",
    "def Eloglike(m,log_s2):\n",
    "    X= ZtoX(m, log_s2)\n",
    "    \n",
    "    #print(X)\n",
    "    #Rewrite with f\n",
    "    loglikes = log_dnorm(f(X), yobs, sigma2) # observation likelihood\n",
    "    #-(yobs- 4.*X+0.5*X.pow(2)).pow(2)    #f(ZtoX(phi)),2.)\n",
    "    \n",
    "    return(loglikes.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db3cdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean=torch.tensor([0])\n",
    "prior_cov = torch.tensor([5])\n",
    "obs_error = 0.001\n",
    "y_cal = torch.tensor([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120e0a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 4.041954040527344 m= 9.235645294189453 s2= 0.42929309606552124\n",
      "199 4.191799163818359 m= 9.385186195373535 s2= 0.23266252875328064\n",
      "299 4.4540205001831055 m= 9.389925003051758 s2= 0.159476175904274\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t, negELBO\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm=\u001b[39m\u001b[38;5;124m'\u001b[39m, m\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms2=\u001b[39m\u001b[38;5;124m'\u001b[39m, log_s2\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Use autograd to compute the backward pass. This call will compute the\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# gradient of loss with respect to all Tensors with requires_grad=True.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# the gradient of the loss with respect to a, b, c, d respectively.\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m negELBO\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Manually update weights using gradient descent. Wrap in torch.no_grad()\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# because weights have requires_grad=True, but we don't need to track this\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# in autograd.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### initialize the variational parameters\n",
    "m = torch.full((), 1.,dtype=dtype, requires_grad=True, device=device)\n",
    "log_s2 = torch.full((),torch.log(torch.tensor(3.)), requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "# Samples fixed here - but try adding them into the loop\n",
    "nsamples = 1000\n",
    "#Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(10000):\n",
    "    Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "    Z=Z[None,:]\n",
    "    #negELBO = -Eloglike(m,log_s2)+KL(log_s2)\n",
    "    \n",
    "    param = [m,log_s2]\n",
    "    \n",
    "    negELBO = ELBO(m,log_s2,Z,emulator,y_cal[:,None],prior_mean,prior_cov,obs_error)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, negELBO.item(), 'm=', m.item(), 's2=', log_s2.exp().item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    negELBO.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        m -= learning_rate * m.grad\n",
    "        log_s2 -= learning_rate * log_s2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        m.grad = None\n",
    "        log_s2.grad = None\n",
    "        \n",
    "print(f'Result: p(x|y) = N({m.item()}, {log_s2.exp().item()}) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cbd630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4546]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emulator.predict(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9484bd82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L=torch.zeros((Z.shape[0],Z.shape[0]))\n",
    "mu = torch.tensor((param[0:Z.shape[0]]))\n",
    "L=L.diagonal_scatter(torch.exp(torch.tensor(param[Z.shape[0]:2*Z.shape[0]])),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e57dfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6962e-03, -1.3783e-03,  3.9404e-03,  1.8090e-03, -1.1019e-02,\n",
       "         -1.3473e-02,  3.3535e-03, -2.0386e-03,  3.2774e-03,  1.3937e-02,\n",
       "          2.5848e-03, -1.8077e-03, -8.9294e-03, -1.0611e-03, -4.5352e-03,\n",
       "         -2.0727e-03,  2.5578e-03, -3.0620e-03,  7.8247e-03, -8.8110e-03,\n",
       "          9.1323e-03, -1.2269e-03, -6.5461e-03,  3.5732e-03, -8.6877e-03,\n",
       "         -8.5548e-03, -4.2340e-03,  2.3416e-03,  2.2156e-03, -3.1737e-04,\n",
       "         -3.5157e-03,  3.8087e-03, -5.8504e-03,  4.5596e-03,  1.3687e-03,\n",
       "         -2.4046e-03,  2.8981e-03, -2.5650e-03, -2.4175e-03, -3.0101e-03,\n",
       "         -4.2580e-03, -1.4548e-04,  3.9391e-03,  7.2150e-03, -1.9380e-03,\n",
       "         -6.4819e-03,  9.9353e-05,  4.9056e-03,  2.5017e-03, -4.1123e-03,\n",
       "          1.3583e-04,  2.2634e-03,  5.3484e-04,  4.6596e-03, -6.5383e-03,\n",
       "          3.0597e-03, -1.0735e-02,  7.8453e-03, -8.2578e-03, -2.1331e-03,\n",
       "          1.9498e-03,  1.2613e-02, -5.7610e-03,  4.2976e-03,  2.8314e-03,\n",
       "          1.1978e-03,  3.3008e-03,  5.3470e-03,  9.4863e-04,  2.8105e-03,\n",
       "          5.5884e-03, -5.1623e-04,  4.9064e-03,  2.1911e-03,  6.6718e-03,\n",
       "          3.4126e-03,  4.0420e-03, -6.3744e-03, -2.2918e-03, -3.2944e-03,\n",
       "          1.2223e-03, -5.6143e-03, -6.5902e-03, -1.5786e-03, -5.1023e-03,\n",
       "         -2.1041e-03,  7.9148e-06,  7.0744e-03, -1.2785e-04,  8.9468e-04,\n",
       "         -5.1300e-03,  6.2782e-03, -7.4998e-04, -3.6154e-03, -7.8211e-04,\n",
       "          2.5294e-03,  2.5815e-03, -4.0883e-03, -1.5489e-03,  2.3927e-03,\n",
       "         -5.5895e-03, -5.7613e-03, -7.6188e-03, -3.7412e-03,  6.5608e-03,\n",
       "          8.2562e-03, -3.8144e-04, -4.2104e-04,  5.2007e-03,  1.7875e-04,\n",
       "          1.9436e-03, -6.6061e-03,  4.7215e-03, -2.0176e-03, -3.8461e-03,\n",
       "          1.6973e-03,  3.7396e-03,  4.2673e-03, -1.6173e-04,  7.6503e-03,\n",
       "          7.2500e-03,  3.7838e-03, -9.0329e-03, -2.4377e-03, -3.3884e-03,\n",
       "          4.3908e-03, -2.7835e-03,  1.1639e-04, -4.9674e-04,  3.8800e-03,\n",
       "         -9.7194e-03, -2.7542e-03,  1.0963e-03, -5.1365e-03, -7.9260e-03,\n",
       "          4.4667e-03,  2.2463e-03, -1.9184e-04, -4.9092e-03, -5.0911e-03,\n",
       "          4.8070e-03, -3.5790e-03, -2.2188e-03, -6.0804e-03, -1.0021e-02,\n",
       "          1.5510e-02, -8.3221e-03, -3.9125e-03,  1.1124e-02, -1.2958e-03,\n",
       "          2.0655e-04, -3.2980e-03,  3.1056e-03,  9.2869e-03, -1.1054e-03,\n",
       "          3.1208e-03,  2.3212e-03,  4.4093e-03, -7.3334e-03, -4.0522e-04,\n",
       "         -7.6385e-03, -1.8741e-03,  4.9719e-03,  3.5873e-03, -7.6084e-03,\n",
       "         -3.8946e-03, -8.9501e-04,  5.6450e-03, -3.3577e-03, -3.1871e-03,\n",
       "          1.6759e-04, -1.1023e-02,  2.5411e-04,  4.4251e-03,  2.7441e-03,\n",
       "          1.0340e-03,  4.9140e-03,  3.0552e-03, -3.1996e-03,  6.0340e-03,\n",
       "          2.4556e-03, -7.9059e-03, -8.2818e-03,  7.9808e-03,  3.6426e-03,\n",
       "          4.2570e-03, -1.0611e-03,  1.0482e-03,  4.8774e-03,  5.7939e-03,\n",
       "         -1.5080e-03,  1.3636e-03,  7.7282e-03,  7.1083e-04,  4.1025e-03,\n",
       "         -1.5448e-03,  5.7009e-03,  3.0587e-03,  3.0820e-03, -1.3510e-03,\n",
       "         -7.1323e-03, -1.8933e-03, -4.8620e-03,  6.5557e-03,  1.3608e-03,\n",
       "         -2.7778e-03,  4.4675e-03,  6.8049e-04,  4.0920e-03, -4.5552e-03,\n",
       "          4.0760e-03,  3.8471e-03,  9.3391e-03, -5.1641e-03,  5.8634e-03,\n",
       "          9.8227e-04,  6.8889e-03, -1.2190e-03,  3.3620e-03,  6.8414e-03,\n",
       "          3.7333e-03, -3.8330e-03,  6.5976e-03, -3.5163e-03, -6.7447e-03,\n",
       "          4.8392e-03,  3.3134e-04, -5.5167e-04,  4.7747e-03, -2.9234e-03,\n",
       "          1.1636e-03,  5.4553e-03,  4.4308e-03, -8.1591e-03,  1.0149e-03,\n",
       "         -1.8626e-02,  2.4499e-03,  7.2323e-03, -6.9781e-03, -9.3832e-03,\n",
       "          1.2543e-04,  3.3368e-03,  7.7088e-05, -1.6181e-03,  3.7722e-03,\n",
       "         -3.5323e-03,  1.4953e-03,  3.1601e-03,  2.7915e-03,  7.2499e-04,\n",
       "         -3.3794e-03,  6.5337e-03,  9.1174e-04,  4.1940e-03,  4.3083e-03,\n",
       "          5.6776e-04,  2.7431e-03,  1.3541e-03,  2.5585e-03, -3.6831e-04,\n",
       "         -9.2194e-03,  3.8750e-03, -9.9845e-03,  9.5407e-03,  4.1747e-03,\n",
       "          2.9323e-03, -3.7500e-03,  6.0551e-03, -1.6021e-03, -7.2010e-03,\n",
       "         -5.4043e-03,  8.0686e-03, -4.8688e-04, -2.9766e-03,  3.1096e-03,\n",
       "         -4.8551e-03,  2.4500e-03, -2.2212e-03, -6.1423e-03, -1.5833e-03,\n",
       "          2.1143e-04, -1.2308e-02,  8.1935e-03,  1.9552e-02,  3.0612e-03,\n",
       "         -4.0979e-03, -1.6995e-03, -1.7080e-03, -7.3018e-03,  4.2624e-03,\n",
       "          3.7677e-03, -4.0950e-03, -8.6668e-03,  4.0587e-03, -1.2061e-02,\n",
       "          4.1004e-03,  6.1326e-03,  2.8408e-03,  2.2452e-03,  3.3540e-03,\n",
       "         -6.3266e-03,  2.1851e-03, -3.5948e-03, -4.5925e-03,  1.9697e-04,\n",
       "         -6.2648e-04, -1.1347e-03, -4.8760e-03,  5.3911e-03,  8.1794e-03,\n",
       "         -1.7107e-03, -6.0051e-03, -3.3832e-03, -2.3204e-03, -1.9353e-03,\n",
       "          2.2734e-04,  8.4319e-03,  2.3614e-03, -4.6550e-03,  8.2055e-03,\n",
       "          8.0359e-03, -5.7216e-04, -2.1553e-03,  8.9522e-04,  4.4482e-03,\n",
       "          4.8835e-03, -5.7069e-03,  9.5820e-04,  9.1934e-03, -1.5074e-03,\n",
       "          1.5448e-03, -9.1968e-03, -1.2838e-03,  5.0242e-03, -2.9101e-03,\n",
       "         -7.8023e-03, -4.4356e-03, -2.1663e-03,  1.6892e-03, -1.3865e-03,\n",
       "         -1.1814e-03, -4.8111e-03, -3.3119e-04,  5.0524e-03, -1.9191e-03,\n",
       "          3.5271e-03,  5.3577e-03,  5.0415e-03,  1.1411e-02, -2.4691e-03,\n",
       "          8.5933e-04, -1.0728e-02, -4.2700e-03,  1.9266e-03,  7.1502e-03,\n",
       "          5.6167e-03,  5.4836e-03,  3.6032e-03,  4.2375e-03,  8.3235e-03,\n",
       "          2.5158e-03, -6.1228e-03,  4.9348e-03, -2.3715e-03,  9.0495e-03,\n",
       "          1.1119e-03,  5.2305e-03,  1.2627e-03,  9.7007e-03, -8.0328e-03,\n",
       "          7.0309e-04,  3.4257e-03, -6.0086e-03,  1.6241e-03,  7.7718e-03,\n",
       "         -7.2921e-03,  5.7063e-04,  4.8848e-04, -2.5032e-03,  2.6750e-03,\n",
       "          9.1388e-03,  2.8547e-03, -3.6335e-03,  8.6248e-03,  1.6463e-03,\n",
       "         -6.3296e-03,  1.2210e-03, -2.7666e-03,  3.8759e-04, -8.1410e-03,\n",
       "         -6.6557e-04, -1.9307e-03, -5.2606e-03, -2.5410e-03,  1.1216e-04,\n",
       "         -5.1436e-03,  2.1709e-03, -1.1635e-05,  5.8181e-03, -5.3270e-03,\n",
       "          3.9295e-03,  3.9055e-03,  2.2609e-03,  6.1427e-04, -9.2757e-04,\n",
       "          1.6410e-03, -4.5403e-03, -5.4179e-03,  3.3302e-03, -3.6409e-04,\n",
       "          4.4857e-03, -1.8263e-03, -2.4399e-03, -2.2208e-04, -9.8413e-04,\n",
       "          1.6868e-03,  4.1650e-03,  5.9655e-03,  6.3323e-03, -4.7770e-03,\n",
       "         -1.3590e-03, -4.7925e-03, -6.8582e-03,  1.7329e-03,  1.3732e-03,\n",
       "         -2.1379e-03,  8.5164e-03,  3.8892e-03, -8.6936e-03, -3.9965e-03,\n",
       "          4.7590e-03, -4.6255e-03, -1.0227e-03,  1.2400e-02, -2.0315e-04,\n",
       "          4.8948e-03,  5.1399e-03, -1.4630e-04, -2.5901e-04,  1.7767e-03,\n",
       "          2.4516e-03,  4.2433e-03, -1.0004e-02,  3.9474e-03, -8.8692e-04,\n",
       "          3.2086e-03, -3.8942e-03,  1.1365e-02,  4.6426e-03, -1.7074e-03,\n",
       "          2.1681e-05, -5.1239e-03,  6.7725e-03, -6.0108e-03, -4.9657e-03,\n",
       "         -2.3542e-03,  1.3012e-03,  4.7409e-03, -3.1257e-03,  2.2178e-03,\n",
       "         -5.0648e-04, -3.8612e-05,  3.2591e-03, -7.3817e-03,  3.3471e-03,\n",
       "          4.7090e-03, -5.8648e-03,  2.0931e-03, -3.2128e-03,  6.4306e-03,\n",
       "          6.1395e-03, -1.6258e-03, -5.6724e-03, -3.4817e-03,  1.9369e-04,\n",
       "         -2.5601e-03, -1.7413e-03,  2.1777e-03,  7.3628e-03, -7.3797e-03,\n",
       "          5.2853e-03,  3.2322e-03,  1.2855e-04,  2.5212e-03,  7.1585e-03,\n",
       "         -1.8276e-03, -2.4788e-03, -4.2489e-03, -5.0539e-03,  2.5415e-03,\n",
       "         -2.3209e-03,  4.8403e-03, -7.6881e-03, -8.5459e-03, -5.2426e-03,\n",
       "         -6.5000e-03,  5.6570e-03,  1.3879e-04,  1.8719e-03, -6.8752e-03,\n",
       "         -1.1528e-02, -7.2822e-03, -4.1494e-03, -1.6273e-03,  2.5845e-03,\n",
       "          6.9327e-03, -5.1020e-03, -8.4073e-03,  3.2509e-03,  5.1909e-03,\n",
       "          6.1043e-03, -1.7111e-03,  2.8286e-04,  5.3250e-04,  1.9142e-03,\n",
       "          4.3495e-04, -6.3263e-03,  5.8278e-03, -1.1147e-04,  9.6979e-03,\n",
       "          3.5656e-03, -6.1615e-05,  1.1787e-03, -5.1106e-03, -1.5511e-03,\n",
       "          4.7066e-03, -1.2645e-03, -7.3843e-03, -1.0565e-03,  4.4236e-03,\n",
       "          7.2507e-03, -1.0375e-04,  5.5620e-04,  6.1463e-03,  9.9424e-04,\n",
       "         -1.5416e-03, -3.9023e-03,  2.4550e-03,  7.5873e-04,  5.7158e-03,\n",
       "         -6.4245e-05, -6.2895e-04,  5.3068e-03, -1.5130e-02, -8.2104e-04,\n",
       "         -1.8083e-03, -3.7023e-03,  1.9144e-03, -1.2810e-03,  1.0242e-03,\n",
       "          4.7120e-03,  7.2277e-03,  4.6384e-03,  2.3418e-03, -2.8542e-03,\n",
       "         -1.6011e-03,  3.9637e-03, -1.0647e-03,  6.9973e-03,  1.1642e-03,\n",
       "         -6.1481e-03,  6.6599e-03,  4.3331e-03,  1.8135e-02, -5.3594e-03,\n",
       "         -2.7900e-03,  3.2144e-03, -6.0906e-05, -1.0256e-02,  1.3126e-03,\n",
       "         -5.8336e-05,  8.5694e-03,  2.2304e-04,  5.4810e-03, -6.7981e-03,\n",
       "         -3.2008e-03, -4.1366e-03, -5.7060e-03, -7.9941e-03,  5.1078e-04,\n",
       "         -2.6039e-03,  5.0921e-04, -1.9234e-03, -3.0389e-03,  5.1368e-03,\n",
       "          4.0755e-03, -1.3207e-03, -1.3331e-03,  9.6092e-04, -3.1646e-03,\n",
       "          6.6543e-03,  5.3477e-03, -3.7792e-04,  2.3097e-03, -1.7318e-03,\n",
       "          5.7658e-03,  7.9118e-03,  3.7829e-03, -5.7531e-03,  1.0084e-02,\n",
       "         -2.1899e-03, -4.8636e-03,  5.0202e-03,  7.6943e-03, -7.4790e-03,\n",
       "          3.2346e-03,  9.8548e-03, -1.5289e-03, -8.2172e-03, -4.0235e-03,\n",
       "         -8.3172e-03,  3.5494e-04, -2.3522e-03,  4.1592e-04, -1.3069e-02,\n",
       "         -4.8081e-03, -5.8231e-03, -5.6885e-03, -9.8165e-04, -1.5679e-03,\n",
       "          1.6006e-04,  4.5801e-03, -5.2268e-03, -4.4057e-04,  1.9798e-03,\n",
       "          5.4155e-03, -2.0301e-03,  3.0880e-03, -1.1084e-03, -2.6792e-03,\n",
       "         -7.4738e-03, -4.5504e-05,  4.7021e-03, -1.5695e-03, -7.2642e-03,\n",
       "         -1.7421e-03,  6.8058e-03, -1.2103e-02, -1.4106e-03,  1.2387e-03,\n",
       "          7.5579e-03,  6.5949e-03,  1.1810e-03, -3.4508e-03, -3.9069e-03,\n",
       "         -6.4837e-03,  2.8716e-03,  5.7629e-04, -6.4095e-04,  4.4002e-03,\n",
       "          1.1176e-03, -1.1844e-03, -3.4378e-03,  7.1890e-03, -5.7124e-03,\n",
       "          7.7302e-03,  4.7621e-03, -2.1709e-04,  1.0212e-02, -1.2354e-03,\n",
       "         -1.5881e-03, -4.5822e-03, -1.4917e-03, -2.4754e-04,  3.1389e-05,\n",
       "         -3.6648e-03,  1.4592e-03,  5.9024e-03, -2.7975e-04, -1.5846e-05,\n",
       "         -1.0499e-02,  3.6941e-03, -4.2119e-03,  2.2032e-03,  1.9431e-03,\n",
       "          8.0157e-03,  2.9550e-03,  1.7615e-03,  3.6403e-03, -6.0826e-03,\n",
       "          1.6098e-03,  4.3300e-03, -7.4362e-04,  4.7355e-03, -2.3368e-03,\n",
       "         -5.4124e-03,  1.1022e-03, -2.4125e-03, -2.1082e-04, -8.2426e-03,\n",
       "         -4.3458e-04,  5.2007e-04,  2.9900e-03,  5.1493e-03,  3.8515e-03,\n",
       "         -4.0821e-03, -4.5583e-05, -1.8870e-03, -5.3035e-03, -5.6081e-03,\n",
       "          6.4759e-03, -7.8591e-03,  1.9225e-03, -1.1501e-04,  6.7227e-03,\n",
       "          3.5393e-03, -8.2407e-04, -1.3760e-03,  1.4104e-03, -1.1345e-03,\n",
       "         -9.2690e-03,  4.8967e-03, -6.3768e-04,  1.7718e-03, -4.6337e-03,\n",
       "         -1.1945e-02,  4.8156e-03, -1.6513e-03,  3.7250e-03,  4.9802e-03,\n",
       "          1.0551e-02, -2.9828e-03,  2.1010e-03, -9.6070e-03, -5.6633e-04,\n",
       "          1.0481e-02,  6.9206e-04,  1.6619e-03, -5.3435e-03, -4.4937e-03,\n",
       "         -7.9818e-04,  6.0761e-04,  1.1105e-02, -8.0749e-04,  1.2442e-02,\n",
       "         -7.3776e-03, -5.3338e-04,  1.1309e-03, -8.8493e-04,  3.6748e-03,\n",
       "         -7.9752e-03, -1.3964e-03,  6.6847e-03,  5.5432e-03, -1.7673e-03,\n",
       "          6.6773e-03, -1.1686e-03, -3.0189e-03,  1.8080e-03, -4.5904e-03,\n",
       "          7.5301e-03, -5.6874e-03, -5.2581e-03, -3.5047e-03, -3.2517e-03,\n",
       "          1.9030e-03, -1.9936e-03,  1.6608e-03,  1.8505e-05,  1.2376e-02,\n",
       "         -9.4557e-04,  8.4650e-04, -2.9593e-03, -4.0540e-03,  2.5585e-03,\n",
       "         -5.8805e-03, -4.7222e-03, -7.0529e-03, -2.0172e-03, -6.6516e-03,\n",
       "         -6.7039e-04, -4.9040e-03,  2.1929e-03, -6.7404e-04, -8.8748e-04,\n",
       "          5.7230e-03,  1.5943e-03,  5.9933e-03,  4.0483e-03, -3.1020e-04,\n",
       "          6.2351e-04,  8.4889e-03,  8.6116e-03, -4.6035e-03, -8.8767e-03,\n",
       "         -1.3142e-03,  3.3145e-03, -7.0522e-03, -5.1106e-03,  2.1722e-03,\n",
       "         -4.0294e-03, -9.3127e-03,  3.0096e-03, -3.4404e-03, -7.6310e-03,\n",
       "          1.5350e-03,  5.7425e-03,  4.2960e-03,  3.6124e-04, -3.8699e-04,\n",
       "         -2.0051e-03, -1.5310e-04, -1.1527e-03, -5.0901e-03,  6.1890e-03,\n",
       "         -3.9629e-03, -1.0008e-02, -3.7635e-03, -5.9275e-03,  8.1371e-03,\n",
       "         -4.8839e-04, -8.8184e-03,  6.9418e-03,  7.9289e-04,  8.0914e-04,\n",
       "          5.5448e-03, -9.8340e-03, -2.5352e-03, -2.0676e-03,  1.9626e-03,\n",
       "          2.6434e-03,  2.8300e-04, -7.4822e-03,  5.9285e-03, -6.0448e-03,\n",
       "          1.1703e-03,  1.9802e-03, -4.2307e-03,  3.8999e-03, -3.5680e-03,\n",
       "         -6.0038e-04, -6.9780e-04, -1.7530e-03,  3.7388e-03,  6.1230e-03,\n",
       "         -1.8508e-03,  2.3202e-03,  5.6437e-04, -9.4349e-03, -3.0912e-03,\n",
       "         -7.1376e-03,  6.3679e-03,  3.4492e-03,  6.7418e-03,  3.7242e-03,\n",
       "         -3.4491e-04,  3.7679e-03,  1.3717e-03, -9.8090e-03,  7.4708e-03,\n",
       "         -5.9546e-03,  4.1608e-03,  2.4509e-03,  1.4258e-03,  3.4416e-03,\n",
       "          8.8334e-03,  2.2630e-03, -7.8413e-03, -6.3749e-03, -1.3426e-03,\n",
       "         -2.9235e-03,  1.0204e-02, -5.5880e-04, -5.3918e-03,  4.7156e-03,\n",
       "         -1.7967e-04, -5.0530e-03, -3.2512e-03, -1.2211e-03,  1.7062e-03,\n",
       "          3.1254e-03, -2.0217e-03, -5.9607e-03, -3.6223e-03, -3.3054e-03,\n",
       "         -4.4011e-03, -6.0313e-03, -6.7771e-03,  3.7194e-03, -1.9174e-03,\n",
       "          7.4573e-03,  1.8946e-03,  3.0444e-03,  4.6158e-03, -2.3529e-03,\n",
       "         -1.4151e-03,  2.2054e-03,  6.7259e-03, -9.5054e-03, -1.0929e-02,\n",
       "          9.2246e-04,  3.8856e-03, -2.2868e-03,  2.9440e-03,  9.3530e-03,\n",
       "          3.1185e-04, -5.3290e-03,  7.7629e-03, -3.3466e-03,  1.2044e-02,\n",
       "          5.5378e-03,  2.4139e-03,  1.4225e-03,  6.5060e-04,  1.3274e-03,\n",
       "          7.3364e-04,  8.6516e-03,  3.3370e-03, -2.7720e-03, -2.1257e-03,\n",
       "          5.9563e-03, -7.8112e-03,  7.1906e-03, -8.8047e-03, -8.4856e-03,\n",
       "         -2.0265e-03, -1.7512e-03,  3.3064e-03, -3.0856e-03,  1.8705e-03,\n",
       "          5.6936e-03, -3.8055e-03,  1.5100e-03, -1.8875e-04,  3.5297e-03,\n",
       "          4.0670e-03, -5.5285e-03,  3.8756e-03, -3.3582e-03,  1.8222e-02,\n",
       "          6.2339e-03,  4.1522e-04, -1.2192e-02, -2.1338e-04, -1.0085e-02,\n",
       "          1.5579e-03,  8.3705e-04,  7.6019e-03, -5.0865e-03, -2.5099e-04,\n",
       "         -1.8017e-03,  1.0829e-03,  1.4549e-02, -5.4878e-04,  3.4453e-03,\n",
       "          3.0005e-03,  2.5991e-03, -1.0088e-03, -1.4384e-03, -4.8271e-03,\n",
       "          2.1861e-03, -1.3314e-03, -7.5877e-03,  1.0917e-03, -8.3365e-03,\n",
       "          2.3442e-03,  3.2847e-04, -6.2502e-03,  5.8270e-04,  4.0293e-03,\n",
       "         -5.4757e-03, -3.7189e-03, -2.6667e-03,  4.4433e-04,  7.0264e-03,\n",
       "          2.2703e-03,  1.2258e-02, -4.4197e-03, -5.5668e-03,  5.5963e-03,\n",
       "         -1.4646e-03,  6.8669e-03,  4.9801e-03, -3.0463e-03,  2.6708e-03,\n",
       "         -6.8445e-03, -2.0818e-03, -1.5148e-02, -3.0372e-03, -1.3491e-03,\n",
       "         -1.5710e-02, -5.5103e-03, -1.9595e-03, -1.6852e-03, -1.8963e-03,\n",
       "         -1.5140e-02, -1.8780e-04,  4.9055e-03,  1.1672e-03,  1.6377e-03,\n",
       "         -4.6952e-03, -9.1121e-06, -2.0966e-03,  5.6992e-03,  1.4800e-03,\n",
       "         -3.6545e-03,  5.2681e-03,  1.6385e-03,  4.8874e-03,  8.0297e-03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(L,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac761cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_s2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec3f63-f945-48ee-b751-760320f70606",
   "metadata": {},
   "source": [
    "Next steps\n",
    "\n",
    "- amortized inference\n",
    "- Can we use GPtorch as function?\n",
    "- write as a class with a fwd and backward method\n",
    "- use torch.nn class to define params?\n",
    "- use random Z at each stage? Should the number of samples increase as we converge?\n",
    "- add prior for x\n",
    "- change f to avoid bimodal posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb1605-f104-45fa-8a63-4971a7356851",
   "metadata": {},
   "source": [
    "## Amortized Variational Inference\n",
    "\n",
    "Let's assume $q(x|y)$ can be modelled as $N(m(y), s2(y))$ where $m(y)$ and $s2(y)$ are both modelled as neural networks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ddc48-71b5-4673-ab5e-35d2328c59d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
