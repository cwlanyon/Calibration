{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2d8f72-376b-4f7b-b0e0-d1b70bafe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "yobs = 8.0\n",
    "sigma2 = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd90672-e817-4cd8-8c99-611507c7f5b4",
   "metadata": {},
   "source": [
    "First we'll learn how to get the variational posterior for $x | y$ for a single value of y. We have\n",
    "$$y= 4x-x^2/2+N(0,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350d8cd-93ad-464d-aceb-d676326f8bd2",
   "metadata": {},
   "source": [
    "## Variational inference\n",
    "\n",
    "Maximize the ELBO\n",
    "$$ELBO(q) = E[\\log p(y | x)] − KL (q(x)|| p(x)).$$\n",
    "\n",
    "We'll use $q(x) = N(m, s^2)$ as the variational family\n",
    "In this case, the ELBO can be computed exactly, but its a pain to do. So I've approximated the E[log p(y | x)] with a Monte Carlo sum.\n",
    "$\\Phi=(m, \\log s^2)$ are the variational parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7d49-6e46-4465-8d7f-297c6fd1edae",
   "metadata": {},
   "source": [
    "KL (q(x) || p(x)) = E[log q(x)] − E[log p(x)]\n",
    "where expectations are with respect to q.\n",
    "\n",
    "Note, if $p(x) \\propto 1$, then $E\\log p(x)$ does not depend on $\\phi$ so can be ignored\n",
    "If $q(x) =N(m, \\sigma^2)$, then $E[\\log q(x)] = -0.5 \\log(2 \\pi \\sigma^2) - 1/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa68f655-7ebf-4b2f-b77a-cb88572bcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(log_var):   \n",
    "    return(-0.5*np.log(2.0*math.pi)-0.5*log_var-0.5)\n",
    "\n",
    "def f(x):\n",
    "    return(4.*x-0.5*torch.pow(x,2.))\n",
    "# How do we define functions? x needs declaring?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ZtoX(m, log_s2):\n",
    "    #reparameterization trick\n",
    "    return(Z*torch.exp(log_s2/2.)+m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c5ab3e-19a1-4afb-bd58-5a6c8ed37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_dnorm(x, mean, var):\n",
    "    return(-(x-mean).pow(2)/(2.*var)-0.5*np.log(2*math.pi*var))\n",
    "    # checked - could used built-in torch dnorm\n",
    "\n",
    "\n",
    "\n",
    "def Eloglike(m,log_s2):\n",
    "    X= ZtoX(m, log_s2)\n",
    "    \n",
    "    #print(X)\n",
    "    #Rewrite with f\n",
    "    loglikes = log_dnorm(f(X), yobs, sigma2) # observation likelihood\n",
    "    #-(yobs- 4.*X+0.5*X.pow(2)).pow(2)    #f(ZtoX(phi)),2.)\n",
    "    \n",
    "    return(loglikes.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da33ee10-470d-4389-8e8c-723147104603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.9968737363815308 m= 4.919249534606934 s2= 0.06470414251089096\n",
      "199 0.6894204616546631 m= 4.6123948097229 s2= 0.10261012613773346\n",
      "299 0.4494328498840332 m= 4.438750267028809 s2= 0.16267293691635132\n",
      "399 0.23781156539916992 m= 4.3007707595825195 s2= 0.25440698862075806\n",
      "499 0.04173636436462402 m= 4.181687355041504 s2= 0.38362643122673035\n",
      "599 -0.08224809169769287 m= 4.09814453125 s2= 0.5195024609565735\n",
      "699 -0.13930487632751465 m= 4.041082859039307 s2= 0.6588190793991089\n",
      "799 -0.14893770217895508 m= 4.013210296630859 s2= 0.7463732957839966\n",
      "899 -0.11489439010620117 m= 4.0027079582214355 s2= 0.7893606424331665\n",
      "999 -0.11926639080047607 m= 4.000077247619629 s2= 0.8124529123306274\n",
      "1099 -0.1333543062210083 m= 3.99855899810791 s2= 0.8152623176574707\n",
      "1199 -0.1518338918685913 m= 3.9989781379699707 s2= 0.8170526027679443\n",
      "1299 -0.17739498615264893 m= 4.0056538581848145 s2= 0.8173894286155701\n",
      "1399 -0.17237865924835205 m= 3.998837471008301 s2= 0.8135724663734436\n",
      "1499 -0.16475892066955566 m= 3.9988911151885986 s2= 0.819738507270813\n",
      "1599 -0.17615199089050293 m= 3.9962010383605957 s2= 0.8135724663734436\n",
      "1699 -0.17580032348632812 m= 4.000974655151367 s2= 0.8147863149642944\n",
      "1799 -0.14998340606689453 m= 4.0016069412231445 s2= 0.8139273524284363\n",
      "1899 -0.1245959997177124 m= 4.001890182495117 s2= 0.8138514161109924\n",
      "1999 -0.1308974027633667 m= 4.004176139831543 s2= 0.8159288167953491\n",
      "2099 -0.17618513107299805 m= 4.000646114349365 s2= 0.8157004714012146\n",
      "2199 -0.09796249866485596 m= 4.002803802490234 s2= 0.8175274133682251\n",
      "2299 -0.12075376510620117 m= 4.003211498260498 s2= 0.8170691132545471\n",
      "2399 -0.11470234394073486 m= 4.002523899078369 s2= 0.8188767433166504\n",
      "2499 -0.13669323921203613 m= 4.000366687774658 s2= 0.816630482673645\n",
      "2599 -0.1410839557647705 m= 4.002930641174316 s2= 0.8149207234382629\n",
      "2699 -0.17039597034454346 m= 4.002082347869873 s2= 0.8163431882858276\n",
      "2799 -0.11595451831817627 m= 4.00711727142334 s2= 0.8137356042861938\n",
      "2899 -0.1078869104385376 m= 4.002138137817383 s2= 0.8134034872055054\n",
      "2999 -0.11119234561920166 m= 4.008371829986572 s2= 0.8171836733818054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m     13\u001b[0m     Z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((nsamples), dtype\u001b[38;5;241m=\u001b[39mdtype, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 15\u001b[0m     negELBO \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mEloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlog_s2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mKL(log_s2)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m99\u001b[39m:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(t, negELBO\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm=\u001b[39m\u001b[38;5;124m'\u001b[39m, m\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms2=\u001b[39m\u001b[38;5;124m'\u001b[39m, log_s2\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mEloglike\u001b[0;34m(m, log_s2)\u001b[0m\n\u001b[1;32m      8\u001b[0m X\u001b[38;5;241m=\u001b[39m ZtoX(m, log_s2)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#print(X)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Rewrite with f\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m loglikes \u001b[38;5;241m=\u001b[39m log_dnorm(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, yobs, sigma2) \u001b[38;5;66;03m# observation likelihood\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#-(yobs- 4.*X+0.5*X.pow(2)).pow(2)    #f(ZtoX(phi)),2.)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(loglikes\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mKL\u001b[39m(log_var):   \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mlog_var\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;241m4.\u001b[39m\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpow(x,\u001b[38;5;241m2.\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# How do we define functions? x needs declaring?\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### initialize the variational parameters\n",
    "m = torch.full((), 10.,dtype=dtype, requires_grad=True, device=device)\n",
    "log_s2 = torch.full((),np.log(15.), requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "# Samples fixed here - but try adding them into the loop\n",
    "nsamples = 1000\n",
    "#Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(10000):\n",
    "    Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "\n",
    "    negELBO = -Eloglike(m,log_s2)+KL(log_s2)\n",
    "    if t % 100 == 99:\n",
    "        print(t, negELBO.item(), 'm=', m.item(), 's2=', log_s2.exp().item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    negELBO.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        m -= learning_rate * m.grad\n",
    "        log_s2 -= learning_rate * log_s2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        m.grad = None\n",
    "        log_s2.grad = None\n",
    "        \n",
    "print(f'Result: p(x|y) = N({m.item()}, {log_s2.exp().item()}) ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec3f63-f945-48ee-b751-760320f70606",
   "metadata": {},
   "source": [
    "Next steps\n",
    "\n",
    "- amortized inference\n",
    "- Can we use GPtorch as function?\n",
    "- write as a class with a fwd and backward method\n",
    "- use torch.nn class to define params?\n",
    "- use random Z at each stage? Should the number of samples increase as we converge?\n",
    "- add prior for x\n",
    "- change f to avoid bimodal posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb1605-f104-45fa-8a63-4971a7356851",
   "metadata": {},
   "source": [
    "## Amortized Variational Inference\n",
    "\n",
    "Let's assume $q(x|y)$ can be modelled as $N(m(y), s2(y))$ where $m(y)$ and $s2(y)$ are both modelled as neural networks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ddc48-71b5-4673-ab5e-35d2328c59d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
